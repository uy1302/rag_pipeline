,Title,Plot,URL
0,📢 Viblo Talk 2024: Cyber & AI - Lập trình an toàn thời AI | Chính thức mở đơn đăng ký 📢,"🌐 Viblo Talk 2024 đã trở lại với chủ đề Cyber & AI: Lập trình an toàn thời AI. Là sự kiện công nghệ đặc biệt dành cho cộng đồng Viblo, năm nay Viblo Talk đem đến cho các lập trình viên, kỹ sư và chuyên gia bảo mật nội dung về việc kết hợp các phương pháp bảo mật thông tin trong lập trình để nâng cao nhận thức và kỹ năng về an ninh mạng trong kỷ nguyên phát triển mạnh mẽ của AI. Viblo tin chắc rằng, đây là một topic vô cùng nóng hổi, có nhiều khía cạnh và góc nhìn khác nhau để chúng ta cùng tham gia thảo luận.

🔍 Sự kiện này có gì đặc biệt?
Tổng quan về AI và tác động đến Công nghệ thông tin: Hiểu rõ hơn về sự phát triển và ảnh hưởng của AI đến ngành công nghệ và các quy trình phát triển phần mềm.
Chiến lược an ninh thông tin trong kỷ nguyên AI: Khám phá những phương pháp tiên tiến và các chuẩn mực cần thiết để bảo vệ dữ liệu và sản phẩm trước các rủi ro an ninh trong môi trường AI dưới nhiều góc nhìn khác nhau.
Phân tích Case Study và Tình huống giả định: Thực tế hóa kiến thức qua các bài học từ tình huống giả định, đưa ra giải pháp và cùng rút kinh nghiệm với các chuyên gia.
👥 Khách mời đặc biệt:
Host: Anh Nguyễn Anh Tiến - Chuyên gia An ninh mạng từ Sun*
Speaker: Anh Trần Minh Hiếu - Senior Software Engineer tại Acronis Singapore
Speaker: Anh Phạm Văn Toàn - Chuyên gia AI từ Sun*
⏰ Thời gian:
Thứ Bảy, 30/11/2024 từ 15h - 17h
🌍 Hình thức tổ chức:
Online qua Zoom
🎟 Đối tượng tham dự:
Lập trình viên, kỹ sư phần mềm, kỹ sư AI, sinh viên công nghệ thông tin và bất cứ ai quan tâm đến an ninh mạng trong thời đại AI!
📢 Đăng ký ngay hôm nay để không bỏ lỡ cơ hội giao lưu và học hỏi từ các chuyên gia hàng đầu về bảo mật và AI.
👉 Link đăng ký: Viblo Talk 2024 - Cyber & AI
Hãy cùng Viblo lan tỏa kiến thức và nâng cao kỹ năng bảo mật để bảo vệ sản phẩm và dữ liệu trong kỷ nguyên AI!",https://viblo.asia/announcements/viblo-talk-2024-cyber-ai-lap-trinh-an-toan-thoi-ai-chinh-thuc-mo-don-dang-ky-BQyJKqvRVMe
1,Khảo sát Viblo: Nhu cầu phát triển sự nghiệp IT toàn cầu,"Bạn đang làm việc trong ngành CNTT và mong muốn phát triển sự nghiệp ở những công ty công nghệ lớn? Hay bạn đang đi học và có nhu cầu đầu quân cho những công ty lớn với mức lương và đãi ngộ khủng sau khi ra trường?
Vậy thì chắc chắn, những nội dung mà Viblo chuẩn bị đem đến cho cộng đồng Viblo thời gian tới sẽ là những thứ bạn không thể bỏ lỡ!

Chào mừng bạn đến với Khảo sát Viblo: Nhu cầu phát triển sự nghiệp IT toàn cầu! Khảo sát này nhằm tìm hiểu nhu cầu và mong muốn của cộng đồng Viblo về cơ hội định cư và làm việc tại các tập đoàn công nghệ lớn ở trong nước và nước ngoài. Chúng tôi sẽ tìm hiểu về sở thích, kỹ năng hiện tại, và mục tiêu nghề nghiệp, mong muốn phát triển của bạn trong lĩnh vực công nghệ thông tin.
Thông tin thu thập được sẽ giúp Viblo cung cấp nội dung, tài nguyên, và hỗ trợ phù hợp nhất cho hành trình phát triển sự nghiệp của bạn.
Thời gian thực hiện Khảo sát là: 12 phút. Hãy giúp Viblo chia sẻ ý kiến để mang lại những nội dung chất lượng, hỗ trợ nâng tầm cộng đồng IT Việt Nam trên bản đồ công nghệ toàn cầu!
Tham gia khảo sát tại đây
Cảm ơn bạn đã dành thời gian quý báu để cung cấp thông tin cho Viblo thông qua khảo sát này.
Bắt đầu từ tháng 10, Viblo kết hợp cùng EngineerPro sẽ tổ chức chuỗi seminar liên quan đến lĩnh vực Phỏng vấn và Làm việc tại Big Tech. Chúng mình sẽ dựa vào những thông tin bạn cung cấp để xây dựng nội dung chuỗi seminar gần gũi và thiết thực nhất với người tham dự. Hãy đón chờ những thông tin mới nhất về chuỗi seminar tại Fanpage Viblo và Announcement Viblo nhé!
Liên hệ
Người dùng có thể liên hệ với Viblo qua những phương thức sau:
Gửi tin nhắn trực tiếp tới Fanpage Viblo
Gửi email tới contact@viblo.asia
Điện thoại: 0888.712.838 (Ms. Linh) (các tình huống khẩn cấp)",https://viblo.asia/announcements/khao-sat-viblo-nhu-cau-phat-trien-su-nghiep-it-toan-cau-PAoJePaA41j
2,Redis: Khám phá những điều cơ bản,"Redis là một kho lưu trữ key-value hiệu suất cao, mã nguồn mở, hỗ trợ nhiều cấu trúc dữ liệu khác nhau. Bài viết này sẽ giúp bạn hiểu rõ hơn về Redis, từ những khái niệm cơ bản đến các tính năng nâng cao như replication, persistence và sharding.
Redis - Máy chủ từ điển từ xa
REDIS - Remote Dictionary Server (Máy chủ Từ điển Từ xa) là một kho lưu trữ key-value mã nguồn mở, tiên tiến. Nó hỗ trợ nhiều cấu trúc dữ liệu khác nhau như chuỗi, hash, danh sách, tập hợp và tập hợp đã sắp xếp. Redis được biết đến với hiệu suất cao, tính linh hoạt và dễ sử dụng, khiến nó trở thành lựa chọn phổ biến cho các nhà phát triển.
Tại sao chúng ta lại cần Redis?
Tại sao chúng ta cần Redis? Ví dụ, trong các ứng dụng như LinkedIn, Facebook hoặc Instagram, việc tải một lượng lớn dữ liệu (chẳng hạn như ảnh đại diện, tên, người theo dõi và bài đăng) từ cơ sở dữ liệu có thể chậm, đặc biệt nếu người dùng thường xuyên làm mới trang. Redis hỗ trợ bằng cách lưu trữ dữ liệu sau lần tải đầu tiên, vì vậy các yêu cầu tiếp theo sẽ nhanh chóng tìm nạp dữ liệu đã lưu, cải thiện trải nghiệm người dùng.
Vậy rốt cuộc Redis là gì?
Redis là một kho lưu trữ cấu trúc dữ liệu trong bộ nhớ được sử dụng làm cơ sở dữ liệu, bộ nhớ đệm và trình môi giới tin nhắn.
Redis rất tốt cho việc xây dựng các ứng dụng web có hiệu suất cao và khả năng mở rộng. Khi nói ""mọi thứ đều trong bộ nhớ"" với Redis, điều đó có nghĩa là tất cả dữ liệu được lưu trữ trong RAM (Random Access Memory) của máy tính thay vì trên ổ cứng.
Điều này làm cho việc truy cập dữ liệu cực kỳ nhanh chóng vì RAM đọc và ghi nhanh hơn nhiều so với ổ cứng. Redis thường được gọi là máy chủ cấu trúc dữ liệu, vì các khóa có thể chứa chuỗi, hash, danh sách, tập hợp và tập hợp đã sắp xếp. Redis được viết bằng ngôn ngữ C.
Các kiểu dữ liệu trong Redis
Strings (chuỗi): Kiểu đơn giản nhất, lưu trữ văn bản hoặc dữ liệu nhị phân. Hãy coi nó giống như một phần thông tin duy nhất, chẳng hạn như tên hoặc số.
Lists (danh sách): Tập hợp các chuỗi được sắp xếp theo thứ tự. Hãy tưởng tượng một danh sách việc cần làm, nơi bạn có thể thêm, xóa và truy cập các mục theo vị trí của chúng.
Sets (tập hợp): Tập hợp các chuỗi duy nhất không theo thứ tự. Hữu ích để lưu trữ các mục không cho phép trùng lặp, giống như danh sách các thẻ duy nhất.
Sorted Sets (tập hợp đã sắp xếp): Tương tự như tập hợp nhưng có điểm được gán cho mỗi mục, cho phép chúng được sắp xếp. Tuyệt vời cho bảng xếp hạng, nơi bạn cần xếp hạng các mục.
Hashes (bảng băm): Ánh xạ các trường và giá trị, giống như một từ điển. Hoàn hảo để lưu trữ các đối tượng có nhiều thuộc tính, chẳng hạn như hồ sơ người dùng.
Bitmaps (bản đồ bit): Chuỗi có thể được coi là mảng bit, hữu ích để theo dõi các trạng thái nhị phân, như hoạt động của người dùng.
HyperLogLogs: Cấu trúc dữ liệu xác suất để đếm các mục duy nhất với mức sử dụng bộ nhớ tối thiểu. Tiện dụng cho phân tích quy mô lớn.
Streams (luồng): Cấu trúc dữ liệu giống như nhật ký để lưu trữ chuỗi tin nhắn, hữu ích để xử lý dữ liệu theo thời gian thực.
Redis Replication và Pub/Sub
1. Redis Replication
Redis có thể tạo bản sao dữ liệu của nó trên nhiều máy chủ Redis khác, được gọi là ""slave"". Quá trình này được gọi là replication. Máy chủ chính, được gọi là ""master"", gửi các bản cập nhật đến tất cả các slave của nó. Bằng cách này, nếu máy chủ master gặp sự cố, một trong các slave có thể tiếp quản, đảm bảo dữ liệu của bạn luôn sẵn có và an toàn.
2. Pub/Sub
Nó giống như một hệ thống nhắn tin, trong đó người gửi (publisher) gửi tin nhắn trong khi người nhận (subscriber) nhận chúng thông qua kênh. Một client có thể đăng ký bất kỳ số lượng kênh nào.
Bảo mật Redis
Redis có thể được bảo mật để bất kỳ client nào kết nối cần phải xác thực trước khi thực hiện các lệnh. Để bảo mật Redis, bạn cần đặt mật khẩu trong tệp cấu hình. Sau khi đặt mật khẩu, nếu client cố gắng chạy lệnh mà không xác thực, họ sẽ nhận được lỗi (error) NOAUTH Authentication required. Do đó, client phải sử dụng lệnh AUTH để xác thực.
Redis Persistence
Persistence đề cập đến khả năng lưu dữ liệu vào đĩa để có thể khôi phục sau khi khởi động lại hoặc gặp sự cố. Điều này đảm bảo rằng dữ liệu của bạn không bị mất và có thể được khôi phục về trạng thái trước đó. Có 2 phương pháp chính để duy trì tính bền vững: RDB và AOF.
1. RDB (Sao lưu Cơ sở dữ liệu Redis)
Tạo ảnh chụp nhanh của toàn bộ tập dữ liệu theo các khoảng thời gian được định cấu hình. Lưu ảnh chụp nhanh vào đĩa dưới dạng tệp nhị phân. Hiệu quả và nhanh chóng để tải. Tác động tối thiểu đến hiệu suất trong quá trình tạo ảnh chụp nhanh.
Ví dụ: Một trang web thương mại điện tử sử dụng RDB để chụp ảnh nhanh các phiên người dùng và giỏ hàng cứ sau 15 phút để khôi phục nhanh chóng.
Nhược điểm: Nguy cơ mất dữ liệu giữa các ảnh chụp nhanh. Không phù hợp cho các ứng dụng yêu cầu độ bền cao.
2. AOF (Tệp chỉ thêm)
Ghi lại mọi hoạt động ghi được máy chủ nhận. Định kỳ ghi lại tệp AOF để giữ cho nó gọn nhẹ. Độ bền cao hơn với ít rủi ro mất dữ liệu.
Ví dụ: Một hệ thống giao dịch tài chính sử dụng AOF để ghi lại mọi giao dịch ngay lập tức, đảm bảo độ bền cao và giảm thiểu mất dữ liệu.
Nhược điểm: Kích thước tệp lớn hơn so với RDB. Hiệu suất chậm hơn do ghi lại mọi hoạt động. Redis cho phép bạn sử dụng cả RDB và AOF cùng nhau để kết hợp các lợi ích của cả hai phương pháp, mang lại sự cân bằng giữa hiệu suất và an toàn dữ liệu.
Redis Sharding
Redis Sharding là một cách để phân chia dữ liệu của bạn trên nhiều máy chủ Redis để xử lý nhiều dữ liệu và lưu lượng truy cập hiệu quả hơn. Hãy tưởng tượng bạn có một thư viện với hàng nghìn cuốn sách. Nếu tất cả sách được lưu trữ trong một phòng, nó có thể trở nên chật chội và chậm để tìm một cuốn sách.
Để giải quyết vấn đề này, bạn có thể chia sách thành các phòng khác nhau dựa trên thể loại của chúng. Bây giờ, mỗi phòng có ít sách hơn, giúp bạn tìm thấy thứ mình cần nhanh hơn. Trong Redis, sharding hoạt động tương tự. Thay vì lưu trữ tất cả dữ liệu trên một máy chủ, bạn chia nó trên nhiều máy chủ (shard). Mỗi shard xử lý một phần dữ liệu, giúp hệ thống nhanh hơn và có khả năng mở rộng hơn.
Những hạn chế của Redis
Sử dụng bộ nhớ: Redis lưu trữ tất cả dữ liệu trong RAM, điều này có thể tốn kém đối với các tập dữ liệu lớn.
Đơn luồng: Redis chạy trên một luồng duy nhất, điều này có thể hạn chế hiệu suất đối với các tác vụ sử dụng nhiều CPU.
Bảo mật cơ bản: Redis có các tính năng bảo mật tích hợp hạn chế, điều này có thể không đủ cho dữ liệu nhạy cảm.
Không có truy vấn phức tạp: Redis không hỗ trợ các truy vấn phức tạp như cơ sở dữ liệu SQL.
Rủi ro mất dữ liệu: Vì Redis chủ yếu nằm trong bộ nhớ, nên có nguy cơ mất dữ liệu nếu máy chủ gặp sự cố trước khi dữ liệu được lưu vào đĩa.
Cảm ơn các bạn đã theo dõi!",https://viblo.asia/p/redis-kham-pha-nhung-dieu-co-ban-5pPLkyBd4RZ
3,Fuzzing HTTP Services trong Golang: Khám phá lỗi ẩn với Fuzz Testing,"Là một lập trình viên, bạn không thể lường trước được tất cả các đầu vào mà chương trình hoặc hàm của mình có thể nhận được. Mặc dù bạn có thể xác định các trường hợp đặc biệt chính, nhưng bạn vẫn không thể dự đoán chương trình của mình sẽ hoạt động như thế nào trong trường hợp có một số đầu vào bất ngờ kỳ lạ. Nói cách khác, bạn thường chỉ có thể tìm thấy những lỗi mà bạn mong đợi tìm thấy.
Đó là lúc kiểm thử mờ hay fuzzing đến để giải cứu. Và trong hướng dẫn này, bạn sẽ học cách thực hiện kiểm thử mờ trong Go.
Fuzz Testing là gì?
Fuzzing là một kỹ thuật kiểm thử phần mềm tự động liên quan đến việc nhập một lượng lớn dữ liệu ngẫu nhiên hợp lệ, gần như hợp lệ hoặc không hợp lệ vào một chương trình máy tính và quan sát hành vi và đầu ra của nó. Vì vậy, mục tiêu của fuzzing là tiết lộ lỗi, sự cố và lỗ hổng bảo mật trong mã nguồn mà bạn có thể không tìm thấy thông qua các phương pháp kiểm thử truyền thống.
Mã Go có thể hoạt động tốt trừ khi bạn cung cấp một đầu vào nhất định, ví dụ như:
func Equal(a []byte, b []byte) bool {
    for i := range a {
        // can panic with runtime error: index out of range.
        if a[i] != b[i] {
            return false
        }
    }
    return true
}
Hàm mẫu này hoạt động hoàn hảo miễn là độ dài của hai slices bằng nhau. Nhưng nó sẽ hoảng sợ khi slice đầu tiên dài hơn slice thứ hai (lỗi index out of range). Hơn nữa, nó không trả về kết quả chính xác khi slice thứ hai là tập hợp con của slice thứ nhất.
Kỹ thuật fuzzing sẽ dễ dàng phát hiện ra lỗi này bằng cách bắn phá hàm này với nhiều đầu vào khác nhau.
Việc tích hợp fuzzing vào vòng đời phát triển phần mềm (SDLC) của nhóm bạn cũng là một cách thực hành tốt. Ví dụ: Microsoft sử dụng fuzzing như một trong những giai đoạn trong SDLC của mình để tìm ra các lỗi và lỗ hổng tiềm ẩn.
Fuzz Testing trong Go và các bước thực hiện
Có rất nhiều công cụ fuzzing đã có sẵn được một thời gian – chẳng hạn như oss-fuzz – nhưng kể từ Go 1.18, fuzzing đã được thêm vào thư viện chuẩn của Go. Vì vậy, bây giờ nó là một phần của gói kiểm thử thông thường vì nó là một loại kiểm thử. Bạn cũng có thể sử dụng nó cùng với các nguyên thủy kiểm thử khác, điều này rất hay.
Các bước để tạo fuzz test trong Go như sau:
Trong một tệp _test.go, hãy tạo một hàm bắt đầu bằng Fuzz chấp nhận *testing.F
Thêm các hạt giống corpus bằng f.Add() để cho phép fuzzer tạo dữ liệu dựa trên nó.
Gọi fuzz target bằng f.Fuzz() bằng cách chuyển các đối số fuzzing mà hàm mục tiêu của chúng tôi chấp nhận.
Khởi động fuzzer bằng lệnh go test thông thường, nhưng với cờ –fuzz=Fuzz
Lưu ý rằng các đối số fuzzing chỉ có thể thuộc các loại sau:
string, byte, []byte
int, int8, int16, int32/rune, int64
uint, uint8, uint16, uint32, uint64
float32, float64
bool
Một fuzz test đơn giản cho hàm Equal ở trên có thể trông như thế này:
// Fuzz test
func FuzzEqual(f *testing.F) {
  // Seed corpus addition
  f.Add([]byte{'f', 'u', 'z', 'z'}, []byte{'t', 'e', 's', 't'})

  // Fuzz target with fuzzing arguments
  f.Fuzz(func(t *testing.T, a []byte, b []byte) {
    // Call our target function and pass fuzzing arguments
    Equal(a, b)
  })
}
Theo mặc định, các fuzz test chạy mãi mãi, vì vậy bạn cần chỉ định giới hạn thời gian hoặc đợi fuzz test thất bại. Bạn có thể chỉ định test nào để chạy bằng cách sử dụng đối số --fuzz.
go test --fuzz=Fuzz -fuzztime=10s
Nếu có bất kỳ lỗi nào trong quá trình thực thi, đầu ra sẽ trông giống như sau:
go test --fuzz=Fuzz -fuzztime=30s
--- FAIL: FuzzEqual (0.02s)
    --- FAIL: FuzzEqual (0.00s)
        testing.go:1591: panic: runtime error: index out of range
    Failing input written to testdata/fuzz/FuzzEqual/84ed65595ad05a58
    To re-run:
    go test -run=FuzzEqual/84ed65595ad05a58
Lưu ý rằng đầu vào mà fuzz test đã thất bại được ghi vào một tệp trong thư mục testdata và có thể được phát lại bằng cách sử dụng mã định danh đầu vào đó.
go test -run=FuzzEqual/84ed65595ad05a58
Thư mục testdata có thể được kiểm tra vào kho lưu trữ và được sử dụng cho các bài kiểm tra thông thường, bởi vì các fuzz test cũng có thể hoạt động như các bài kiểm tra thông thường khi được thực thi mà không có cờ --fuzz.
Fuzzing HTTP Services: Kiểm thử với httptest
Cũng có thể fuzz test các dịch vụ HTTP bằng cách viết test cho HandlerFunc của bạn và sử dụng gói httptest. Điều này có thể rất hữu ích nếu bạn cần kiểm tra toàn bộ dịch vụ HTTP, không chỉ các hàm bên dưới.
Bây giờ, chúng ta hãy giới thiệu một ví dụ thực tế hơn, chẳng hạn như HTTP Handler chấp nhận một số đầu vào của người dùng trong phần thân yêu cầu và sau đó viết một fuzz test cho nó.
Trình xử lý của chúng tôi chấp nhận yêu cầu JSON với các trường limit và offset để phân trang một số dữ liệu tĩnh được mock. Hãy xác định các loại trước.
type Request struct {
  Limit  int `json:""limit""`
  Offset int `json:""offset""`
}

type Response struct {
  Results    []int `json:""items""`
  PagesCount int   `json:""pagesCount""`
}
Hàm handler của chúng ta sau đó phân tích cú pháp JSON, phân trang slice tĩnh và trả về một JSON mới để phản hồi.
func ProcessRequest(w http.ResponseWriter, r *http.Request) {
 var req Request

  // Decode JSON request
  if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
    http.Error(w, err.Error(), http.StatusBadRequest)
    return
  }

 // Apply offset and limit to some static data
 all := make([]int, 1000)
 start := req.Offset
 end := req.Offset + req.Limit
 res := Response{
   Results:    all[start:end],
   PagesCount: len(all) / req.Limit,
 }

 // Send JSON response
 if err := json.NewEncoder(w).Encode(res); err != nil {
   http.Error(w, err.Error(), http.StatusInternalServerError)
   return
 }

 w.WriteHeader(http.StatusOK)
}
Như bạn có thể đã nhận thấy, hàm này không xử lý các hoạt động slice rất tốt và có thể dễ dàng panic. Ngoài ra, nó có thể panic nếu nó cố gắng chia cho 0. Sẽ rất tốt nếu chúng ta có thể phát hiện ra điều này trong quá trình phát triển hoặc chỉ sử dụng các unit test, nhưng đôi khi không phải mọi thứ đều hiển thị với mắt chúng ta và trình xử lý của chúng ta có thể chuyển đầu vào cho các hàm khác, v.v.
Theo ví dụ FuzzEqual của chúng ta ở trên, hãy triển khai fuzz test cho trình xử lý ProcessRequest. Điều đầu tiên chúng ta cần làm là cung cấp các đầu vào mẫu cho fuzzer. Đây là dữ liệu mà fuzzer sẽ sử dụng và sửa đổi thành các đầu vào mới được thử. Chúng ta có thể tạo một số yêu cầu JSON mẫu và sử dụng f.Add() với loại []byte.
func FuzzProcessRequest(f *testing.F) {
  // Create sample inputs for the fuzzer
  testRequests := []Request{
    {Limit: -10, Offset: -10},
    {Limit: 0, Offset: 0},
    {Limit: 100, Offset: 100},
    {Limit: 200, Offset: 200},
  }

  // Add to the seed corpus
  for _, r := range testRequests {
    if data, err := json.Marshal(r); err == nil {
      f.Add(data)
    }
  }

  // ...
}
Sau đó, chúng ta có thể sử dụng gói httptest để tạo máy chủ HTTP thử nghiệm và gửi yêu cầu đến nó.
Lưu ý: Vì fuzzer của chúng ta có thể tạo các yêu cầu không phải JSON không hợp lệ, nên tốt hơn là chỉ bỏ qua chúng và bỏ qua bằng t.Skip(). Chúng ta cũng có thể bỏ qua các lỗi BadRequest.
func FuzzProcessRequest(f *testing.F) {
  // ...
  // Create a test server
  srv := httptest.NewServer(http.HandlerFunc(ProcessRequest))
  defer srv.Close()

  // Fuzz target with a single []byte argument
  f.Fuzz(func(t *testing.T, data []byte) {
    var req Request
    if err := json.Unmarshal(data, &req); err != nil {
      // Skip invalid JSON requests that may be generated during fuzz
      t.Skip(""invalid json"")
    }

    // Pass data to the server
    resp, err := http.DefaultClient.Post(srv.URL, ""application/json"", bytes.NewBuffer(data))
    if err != nil {
      t.Fatalf(""unable to call server: %v, data: %s"", err, string(data))
    }
    defer resp.Body.Close()

    // Skip BadRequest errors
    if resp.StatusCode == http.StatusBadRequest {
      t.Skip(""invalid json"")
    }

    // Check status code
    if resp.StatusCode != http.StatusOK {
      t.Fatalf(""non-200 status code %d"", resp.StatusCode)
    }
  })
}
Fuzz target của chúng ta có một đối số duy nhất với loại []byte chứa toàn bộ yêu cầu JSON, nhưng bạn có thể thay đổi nó để có nhiều đối số.
Mọi thứ đã sẵn sàng để chạy fuzz test của chúng ta. Khi fuzzing các máy chủ HTTP, bạn có thể cần điều chỉnh số lượng worker song song, nếu không tải có thể làm quá tải máy chủ thử nghiệm. Bạn có thể làm điều đó bằng cách đặt cờ -parallel=1.
go test --fuzz=Fuzz -fuzztime=10s -parallel=1
go test --fuzz=Fuzz -fuzztime=30s
--- FAIL: FuzzProcessRequest (0.02s)
    --- FAIL: FuzzProcessRequest (0.00s)
        runtime error: integer divide by zero
        runtime error: slice bounds out of range
Và như mong đợi, chúng ta sẽ thấy các lỗi trên được phát hiện.
Chúng ta cũng có thể thấy các đầu vào fuzz trong thư mục testdata để xem JSON nào đã góp phần gây ra lỗi này. Dưới đây là nội dung mẫu của tệp:
go test fuzz v1
[]byte(""{""limit"":0,""offset"":0}"")
Để khắc phục sự cố đó, chúng ta có thể giới thiệu xác thực đầu vào và cài đặt mặc định:
if req.Limit <= 0 {
  req.Limit = 1
}
if req.Offset < 0 {
  req.Offset = 0
}
if req.Offset > len(all) {
  start = len(all) - 1
}
if end > len(all) {
  end = len(all)
}
Với thay đổi này, các fuzz test sẽ chạy trong 10 giây và thoát mà không có lỗi.
Kết luận
Viết fuzz test cho các dịch vụ HTTP hoặc bất kỳ phương thức nào khác của bạn là một cách tuyệt vời để phát hiện các lỗi khó tìm. Fuzzers có thể phát hiện các lỗi khó phát hiện chỉ xảy ra đối với một số đầu vào bất ngờ kỳ lạ.
Thật tuyệt vời khi thấy rằng fuzzing là một phần của thư viện kiểm thử tích hợp sẵn của Go, giúp dễ dàng kết hợp với các bài kiểm tra thông thường. Lưu ý: trước Go 1.18, các nhà phát triển đã sử dụng go-fuzz, đây cũng là một công cụ tuyệt vời để fuzzing.",https://viblo.asia/p/fuzzing-http-services-trong-golang-kham-pha-loi-an-voi-fuzz-testing-x7Z4DbE1LnX
4,Ngôi trường thứ 3 của tôi: Viết blog,"Sau một thời gian viết blog, đã có hơn 28,000 lượt xem các bài viết về lập trình backend của mình. Mặc dù đây không phải là con quá số lớn, nhưng mình vẫn rất vui khi thấy bài viết của mình được ai đó ngoài kia đón đọc. Ban đầu, mình viết chủ yếu để tổng hợp kiến thức, chuẩn bị tốt hơn cho tìm kiếm công việc trong tương lai. Nhưng thật bất ngờ, những chia sẻ này lại được mọi người đón nhận tích cực. Nó đã làm mình suy nghĩ sâu hơn về giá trị của việc chia sẻ.
Có lẽ không ít người cảm thấy ngần ngại khi chia sẻ kiến thức, bởi đó là công sức tích lũy qua nhiều năm. Một phần trong chúng ta có thể nghĩ rằng, tại sao phải cho đi khi có thể giữ lại cho riêng mình? Hơn nữa, viết blog thực sự không hề dễ dàng. Mình thường mất 2-3 tuần mới có thể hoàn thành một bài viết (một phần do bận bán mình cho tư bản). Có lẽ đó cũng là lý do khiến nhiều người ngần ngại bắt tay vào viết. Nhưng khi nhìn lại chặng đường đã qua, mình nhận ra rằng việc viết blog mang lại cho mình rất nhiều giá trị:
Thứ nhất là cải thiện kiến thức: Có hẳn một khái niệm được gọi là Hiệu ứng Feynman. lý thuyết này cho rằng khi chia sẻ với người khác, bạn không chỉ học lại mà còn phải tổ chức và tái cấu trúc lại kiến thức của mình. Quá trình này giúp củng cố trí nhớ và làm sâu sắc thêm sự hiểu biết của bản thân.
Thứ nhất là cải thiện kiến thức: Có hẳn một khái niệm được gọi là Hiệu ứng Feynman. lý thuyết này cho rằng khi chia sẻ với người khác, bạn không chỉ học lại mà còn phải tổ chức và tái cấu trúc lại kiến thức của mình. Quá trình này giúp củng cố trí nhớ và làm sâu sắc thêm sự hiểu biết của bản thân.
Feynman
Thứ hai là giúp bạn phát hiện con ""bug"" trong mình: Khi cố gắng giải thích một khái niệm, bạn sẽ nhận thấy những điểm mà bản thân gặp khó khăn trong việc truyền đạt. Và rồi bạn ồ, à, thì ra mình chưa hiểu rõ kiến thức này như mình nghĩ. Điều này giúp bạn nhận ra và lấp đầy những lỗ hổng trong kiến thức còn thiếu.
Thứ hai là giúp bạn phát hiện con ""bug"" trong mình: Khi cố gắng giải thích một khái niệm, bạn sẽ nhận thấy những điểm mà bản thân gặp khó khăn trong việc truyền đạt. Và rồi bạn ồ, à, thì ra mình chưa hiểu rõ kiến thức này như mình nghĩ. Điều này giúp bạn nhận ra và lấp đầy những lỗ hổng trong kiến thức còn thiếu.
ồ
à
thì ra mình chưa hiểu rõ kiến thức này như mình nghĩ
Thứ ba là phát triển kỹ năng mềm: Viết blog là cơ hội để rèn luyện kỹ năng truyền đạt thông tin. Làm việc với máy tính thì khá đơn giản, nhưng khi viết blog, bạn phải bắt đầu từ việc xây dựng cấu trúc bài viết, lựa chọn từ ngữ sao cho dễ hiểu, biến một khái niệm phức tạp thành một thứ đơn giản cho người đọc dễ hình dung, rồi chọn hình ảnh minh họa nữa.
Thứ ba là phát triển kỹ năng mềm: Viết blog là cơ hội để rèn luyện kỹ năng truyền đạt thông tin. Làm việc với máy tính thì khá đơn giản, nhưng khi viết blog, bạn phải bắt đầu từ việc xây dựng cấu trúc bài viết, lựa chọn từ ngữ sao cho dễ hiểu, biến một khái niệm phức tạp thành một thứ đơn giản cho người đọc dễ hình dung, rồi chọn hình ảnh minh họa nữa.
Thứ tư là rèn luyện tính kỷ luật: Mình tự đặt mục tiêu mỗi tháng phải viết được 2 bài, và đó đều là những chủ đề mình thực sự muốn học. Việc này tạo ra thách thức trong việc cân bằng thời gian giữa công việc, viết blog và các mối bận tâm khác, nhưng đồng thời cũng là động lực để mình không ngừng tiến bộ
Thứ tư là rèn luyện tính kỷ luật: Mình tự đặt mục tiêu mỗi tháng phải viết được 2 bài, và đó đều là những chủ đề mình thực sự muốn học. Việc này tạo ra thách thức trong việc cân bằng thời gian giữa công việc, viết blog và các mối bận tâm khác, nhưng đồng thời cũng là động lực để mình không ngừng tiến bộ
Cuối cùng là kết nối với mọi người: Nếu không viết blog, có lẽ mình sẽ không nhận được những kiến thức quý giá từ mọi người như thế. Việc chia sẻ và nhận phản hồi giúp mình nhìn nhận điểm yếu của bản thân, nhận ra ""bầu trời không phải hình tròn"" và từ đó có cơ hội cải thiện để phát triển hơn nữa.
Cuối cùng là kết nối với mọi người: Nếu không viết blog, có lẽ mình sẽ không nhận được những kiến thức quý giá từ mọi người như thế. Việc chia sẻ và nhận phản hồi giúp mình nhìn nhận điểm yếu của bản thân, nhận ra ""bầu trời không phải hình tròn"" và từ đó có cơ hội cải thiện để phát triển hơn nữa.
Viết blog không chỉ là hành trình chia sẻ, mà còn là hành trình khám phá chính mình. Với mình, đó là từng bước nhỏ trong chặng đường phát triển. Và nếu mỗi bài viết của mình có thể giúp ai đó ngoài kia tìm thấy giá trị, thì những nỗ lực này hoàn toàn xứng đáng. Mong rằng con đường này sẽ tiếp tục mở ra nhiều cánh cửa để mình học hỏi, chia sẻ, và phát triển hơn nữa.",https://viblo.asia/p/ngoi-truong-thu-3-cua-toi-viet-blog-EbNVQgWoJvR
5,Xóa nhánh Git an toàn: dọn dẹp kho mã của bạn,"Trong Git, cũng như hầu hết các hệ thống quản lý phiên bản khác, quản lý nhánh đóng vai trò quan trọng trong việc giữ cho kho mã sạch sẽ và dễ bảo trì. Việc dọn dẹp các nhánh không còn cần thiết giúp ngăn ngừa sự lộn xộn và giữ cho quy trình làm việc gọn gàng.
Trong bài viết này, bạn sẽ học cách xóa cả nhánh Git cục bộ và từ xa, đồng thời thực hành với các ví dụ về cách tránh một số cạm bẫy thường gặp. Cho dù là dọn dẹp sau khi một tính năng đã được hợp nhất hay xóa các nhánh lỗi thời, việc biết cách xóa chúng chính xác là một phần quan trọng của việc quản lý dự án.
Hiểu rõ hơn về nhánh Git
Là lập trình viên, chúng ta thường tạo các nhánh liên quan đến tính năng hoặc nhiệm vụ. Một nhánh trong Git về cơ bản là một điểm thời gian trên dự án cho phép chúng ta làm việc trên các sửa đổi mà không làm xáo trộn nhánh chính của kho mã. Nó giống như một sandbox nơi tôi có thể thử nghiệm và thực hiện các điều chỉnh, xem xét rủi ro có thể ảnh hưởng đến tính toàn vẹn của dự án chính.
Điều này trở nên rất quan trọng khi quản lý các nhánh thông qua một dự án lớn có nhiều người đóng góp. Tôi muốn kho lưu trữ của mình được dọn dẹp cho bản thân và đồng nghiệp của tôi. Khi một nhánh đã hoàn thành mục đích của nó, chẳng hạn như chúng ta đã hợp nhất các thay đổi trong nhánh chính, thì chúng ta nên quyết định xem liệu chúng ta có còn cần nó nữa hay không.
Việc dọn dẹp nhánh không sử dụng thường xuyên có thể giảm bớt sự nhầm lẫn và giúp việc điều hướng dễ dàng hơn thông qua kho lưu trữ của chúng ta. Bên cạnh đó, việc giữ cho những thứ này sạch sẽ được thực hiện bằng một lệnh đơn giản, cụ thể là git branch, lệnh này liệt kê tất cả chúng; điều này cho thấy rất rõ ràng về những gì nên giữ lại và những gì có thể bị xóa. Do đó, với tất cả các tiện ích này, chúng ta có thể thực thi một cách phát triển có cấu trúc và tinh gọn.
Lý do nên xóa nhánh Git
Trong quá trình phát triển, chúng ta thường gặp những trường hợp tạo nhánh cho một số tính năng hoặc nhiệm vụ cụ thể. Khi các thay đổi được hợp nhất, rõ ràng, chúng ta sẽ loại bỏ chúng vì chúng sẽ chỉ làm cho kho lưu trữ của chúng ta trở nên lộn xộn.
Thật vậy, việc dọn dẹp các nhánh sau khi mục đích của chúng đã được phục vụ mang lại nhiều lý do: ít nhất là tổ chức tốt hơn, giảm bớt sự nhầm lẫn và điều hướng dễ dàng hơn thông qua kho lưu trữ của chúng ta.
Khi tôi nhìn vào một kho lưu trữ chứa đầy các nhánh cũ, đôi khi bắt đầu khó tìm ra nhánh nào vẫn còn phù hợp trong bối cảnh quy trình làm việc hiện tại của chúng ta. Những sai lầm có thể xảy ra do nhầm lẫn về việc cố gắng hợp nhất một nhánh lỗi thời hoặc làm việc trên một thứ gì đó đã được tích hợp.
Để phù hợp với việc phân nhánh vô ích, mọi thứ giữ cho kho mã được sắp xếp hợp lý và dễ bảo trì hơn thường xuyên. Bên cạnh đó, việc xóa các nhánh này giúp nhóm của chúng ta tập trung vào nhiệm vụ hiện tại, thay vì nghĩ về các nhánh lỗi thời.
Do đó, điều này hoạt động như một cách bản năng để duy trì kiểm soát phiên bản tốt và do đó, cần được cập nhật thường xuyên. Chúng ta nên thực hành xóa những nhánh không phục vụ mục đích hữu ích nào được cung cấp cho dự án của chúng ta vì lợi ích chung.
Cách xóa nhánh cục bộ trong Git
Việc xóa một nhánh cục bộ mà bạn không ở trên tương đối dễ dàng và bạn thực hiện bằng cách chạy lệnh sau: Trước tiên, bạn cần phải thoát khỏi nhánh mà bạn muốn xóa. Bạn chuyển sang bất kỳ nhánh nào khác bằng cách sử dụng lệnh git checkout <branch_name>. Ví dụ: để chuyển sang nhánh 'main', bạn sẽ chạy:
git checkout main
Sau khi không còn cần thiết nữa, bạn sẽ muốn xóa nhánh đó. Sau khi bạn đã chuyển khỏi nhánh đó, bạn có thể xóa nhánh cục bộ. Để xóa một nhánh, Git cung cấp hai biến thể: -d và -D.
Tùy chọn -d cho phép xóa nhánh trong trường hợp nó đã được bao gồm trong một nhánh khác. Ví dụ: để xóa một nhánh có tên 'feature/login' đã được hợp nhất trước đó, bạn sẽ sử dụng:
git branch -d feature/login
Nếu nhánh chưa được hợp nhất và bạn muốn buộc xóa nó, thì bạn sẽ sử dụng tùy chọn -D:
git branch -D feature/login
Hãy cẩn thận với lệnh sử dụng tùy chọn -D vì điều này sẽ buộc nhánh bị xóa vĩnh viễn và bạn sẽ mất dữ liệu nếu có những thay đổi chưa được hợp nhất trên nhánh đó. Luôn đảm bảo bạn xem xét các nhánh của mình trông như thế nào trước khi chạy các lệnh này.
Xóa các nhánh Git từ xa
Đối với việc quản lý các nhánh từ xa, nó hơi khác so với các nhánh cục bộ. Bạn sẽ xóa một nhánh từ xa bằng lệnh này: git push origin --delete <branch_name> .
git push origin --delete <branch_name>
Ngay trước khi xóa một nhánh, điều cần thiết là bạn xem các nhánh hiện có của mình bằng git branch -a, một lệnh hiển thị cả cục bộ và từ xa. Giả sử, ví dụ, tôi muốn xóa một nhánh từ xa - 'feature/login'. Sau khi hoàn tất, điều đầu tiên tôi làm là kiểm tra các nhánh của mình bằng cách sử dụng:
git branch -a
Điều này sẽ liệt kê tất cả các nhánh; do đó, nó sẽ giúp tôi xác nhận rằng 'feature/login' tồn tại từ xa. Sau khi xác nhận, tôi có thể xóa nhánh bằng cách sử dụng:
git push origin --delete feature/login
Việc dọn dẹp các nhánh từ xa rất quan trọng vì nếu không xóa, chúng có thể được phục hồi.
Khắc phục sự cố khi xóa nhánh
Đôi khi các nhánh không bị xóa sạch sẽ và rõ ràng. Chương này đề cập đến một số vấn đề thường gặp, chẳng hạn như cố gắng xóa một nhánh hiện đang được chọn và sử dụng sai cờ -d.
Một vấn đề thường gặp xảy ra khi tôi cố gắng xóa một nhánh mà tôi hiện đang ở trên đó. Git sẽ không cho phép điều này. Để kiểm tra xem tôi đang ở nhánh nào, tôi có thể sử dụng lệnh:
git branch
Lệnh này liệt kê tất cả các nhánh, nhánh hiện tại được đánh dấu bằng dấu hoa thị (*). Đầu tiên, tôi cần chuyển sang một nhánh khác bằng cách sử dụng:
git checkout <another-branch>
Một vấn đề khác là tùy chọn -d không hoạt động vì nó chỉ xóa một nhánh đã được hợp nhất vào nhánh cha của nó. Nếu tôi làm:
git branch -d <branch-name>
Và gặp lỗi, tôi biết nhánh đó chưa được hợp nhất. Trong trường hợp đó, tôi có thể sử dụng tùy chọn xóa buộc với:
git branch -D <branch-name>
Tuy nhiên, đây là một lệnh cần được đưa ra một cách cẩn thận vì nó cũng sẽ xóa các thay đổi chưa được hợp nhất. Vì vậy, việc biết chính xác các nhánh của tôi là gì và chúng đang ở trạng thái nào sẽ giúp tôi tránh mất dữ liệu trong khi quản lý chúng.
Kết luận
Việc xóa các nhánh trong Git khá đơn giản, nhưng bạn cần phải biết cách xử lý cả nhánh cục bộ và nhánh từ xa. Để đạt được điều đó, bạn sẽ sử dụng các bước thích hợp với các lệnh phù hợp để tránh làm rối kho mã của mình, giúp quy trình làm việc của bạn dễ dàng hơn.
Luôn kiểm tra các thay đổi chưa được hợp nhất trước khi buộc xóa một nhánh và đảm bảo không bao giờ xóa một nhánh mà bạn đang ở trên đó. Việc tuân theo các phương pháp hay nhất này sẽ giúp bạn dễ dàng quản lý các nhánh Git của mình.",https://viblo.asia/p/xoa-nhanh-git-an-toan-don-dep-kho-ma-cua-ban-38X4E8QzVN2
6,Đảm bảo khả năng sẵn sàng cao cho ứng dụng trên Kubernetes - High Availability for application on Kubernetes,"Chúng ta đã nghe rất nhiều đến những lợi ích khi sử dụng Kuberntes để chạy ứng dụng dụng microservices, điều này vô tình làm mọi người tin rằng ứng dụng cứ chạy trên Kubernetes là sẽ có độ sẵn sàng cao, khả năng chịu lỗi tốt, scale nhanh,...
Tuy nhiên thực tế thì để đạt được độ sẵn sàng cao (High Availability) cho ứng dụng trên K8s yêu cầu nhiều cấu hình cụ thể phù hợp với từng ứng dụng khác nhau. Trong bài viết ngày hôm nay mình sẽ đề cập đến các cấu hình mà chúng ta cần quan tâm nếu muốn đảm bảo ứng dụng chạy ít lỗi, không có downtime,... gọi chung là tính sẵn sàng cao.

Deployment Strategy là chiến lược triển khai ứng dụng giúp bạn kiểm soát cách thức triển khai và cập nhật ứng dụng trong Kubernetes. Có hai chiến lược phổ biến là Rolling Update (Phương thức mặc định trong Kubernetes) và Recreate.
Rolling Update cho phép triển khai từng phiên bản mới theo cách tuần tự, cập nhật một pod tại một thời điểm. Điều này giúp đảm bảo rằng phiên bản cũ vẫn hoạt động trong khi phiên bản mới được triển khai dần dần và không gây ra downtime.
Recreate, ngược lại, tắt tất cả các pod cũ trước khi triển khai phiên bản mới – phù hợp cho các ứng dụng không cần duy trì tính liên tục. Phương thức này sẽ gây ra downtime trong quá trình đợi phiên bản mới sẵn sàng.
Bạn cũng có thể kết hợp các chiến lược canary hoặc blue-green deployment để kiểm thử phiên bản mới một cách an toàn. Lựa chọn chiến lược triển khai phù hợp giúp bạn duy trì dịch vụ ổn định trong quá trình cập nhật và giảm thiểu rủi ro trong suốt vòng đời của ứng dụng.
Sau đây là ví dụ cho việc cấu hình sử dụng chiến lược Rolling Update cho một deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 3
  strategy:
    type: RollingUpdate  # Deployment strategy config
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: myapp-image:v2
Readiness Probe và Liveness Probe là hai loại healthcheck quan trọng cho các pod trong Kubernetes. 2 healthcheck này giúp đảm bảo dịch vụ của bạn luôn trong trạng thái sẵn sàng và ổn định.
Readiness Probe xác định thời điểm một container có thể nhận yêu cầu mới, tức là nếu một container đang tải dữ liệu hoặc kết nối đến các dịch vụ phụ thuộc thì nó sẽ không được truy cập cho đến khi hoàn toàn sẵn sàng. Điều này ngăn chặn người dùng khỏi việc tiếp cận dịch vụ khi nó chưa sẵn sàng hoạt động, giúp nâng cao trải nghiệm. Nếu người dùng truy cập vào một dịch vụ web khi pod chưa sẵn sàng thì sẽ gặp lỗi HTTP Code 503
Trong khi đó, Liveness Probe kiểm tra xem container có đang hoạt động bình thường hay không. Nếu một container bị lỗi hoặc rơi vào trạng thái không ổn định, Kubernetes sẽ khởi động lại pod để tự động khắc phục. Sự kết hợp của hai cơ chế này giúp nâng cao tính sẵn sàng và độ tin cậy của hệ thống bằng cách đảm bảo chỉ các pod khỏe mạnh mới xử lý yêu cầu. Việc cấu hình thời gian kiểm tra như thế nào cũng rất quan trọng, mình đã đề cập đến Best Practice để cấu hình các Probes trong K8s trong bài viết trước đây.
⚠️ Cấu hình Startup, Readiness, Liveness Probe cho ứng dụng chạy trên Kubernetes
Cấu hình probe cho Deployment
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: myapp-container
    image: myapp-image
    readinessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
Resource Requests và Limits là cài đặt cho phép bạn kiểm soát việc sử dụng tài nguyên của mỗi container để tối ưu hóa hiệu suất và tránh tình trạng quá tải hệ thống.
Requests là mức tài nguyên tối thiểu mà container yêu cầu để chạy ổn định, đảm bảo nó luôn có đủ tài nguyên khi cần.
Limits là mức tối đa tài nguyên mà container được phép sử dụng, giúp tránh tình trạng container chiếm hết tài nguyên của các ứng dụng khác.
Nếu không đặt các giới hạn này, một số container có thể chiếm nhiều tài nguyên hơn dự kiến, gây ra thiếu tài nguyên cho các container khác và ảnh hưởng đến độ ổn định của toàn bộ cluster. Việc cấu hình Requests và Limits hợp lý giúp bạn tối ưu hóa việc phân phối tài nguyên trong Kubernetes và tăng cường tính ổn định của hệ thống. Nếu cấu hình chỉ số quá cao sẽ gây ra lãng phí tài nguyên hệ thống, nếu cấu hình quá thấp thì ứng dụng có thể sẽ bị kill gây ra gián đoạn. Mình cũng đã có viết một bài chỉ các mình chọn resource limit và request cho ứng dụng, bạn có thể tham khảo ở đây nhé
🦾 Kubernetes resource management - Quản trị tài nguyên trong Kubernetes
Sau đây là một cấu hình mẫu resource request và limit cho K8s deployment
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
  - name: resource-demo-container
    image: nginx
    resources:
      requests:
        memory: ""64Mi""
        cpu: ""250m""
      limits:
        memory: ""128Mi""
        cpu: ""500m""
Horizontal Pod Autoscaler (HPA) là một tính năng tự động tăng hoặc giảm số lượng pod của ứng dụng dựa trên các chỉ số như CPU hoặc bộ nhớ. Điều này giúp dịch vụ của bạn có thể tự động mở rộng để đáp ứng nhu cầu tải cao và thu nhỏ lại khi tải giảm, tiết kiệm tài nguyên và chi phí vận hành. HPA dựa vào các chỉ số mà bạn đặt ra, chẳng hạn như sử dụng CPU đạt 70%, HPA sẽ tự động thêm pod để giảm tải cho các pod hiện tại. HPA không chỉ giúp duy trì hiệu suất và tính ổn định của dịch vụ, mà còn là một công cụ hữu ích để quản lý chi phí trong môi trường cloud khi tài nguyên được cấp phát dựa trên nhu cầu thực tế.
Đa số các trường hợp ta có thể scale dựa trên chỉ số CPU, tuy nhiên trong 1 số ứng dụng đặc biệt ta cần scale dựa trên memory hay số lượng request đến chằng hạn thì K8s không có hỗ trợ các chỉ số đó. Lúc này bạn cần cài các công cụ bổ sung để cung cấp các chỉ số (metrics) này, Keda là một trong số đó. Để tìm hiểu thêm về Keda bạn có thể đọc bài viết mình đã viết về công cụ này
🕵️‍♂️ HPA là gì? Auto scaling pod bằng HPA và KEDA
Sau đây là cấu hình HPA ví dụ cho ứng dụng trên K8s
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
PostStart Hook được kích hoạt ngay sau khi container khởi động, giúp bạn thực hiện các hành động cần thiết để thiết lập container hoặc chuẩn bị môi trường làm việc trước khi container chính thức nhận yêu cầu. Chẳng hạn, bạn có thể sử dụng PostStart để tải các file cấu hình cần thiết, khởi động các kết nối cần thiết, hoặc thực hiện các thao tác xác thực, cấu hình bổ sung.
Cấu hình PostStart hay PreStop sử dụng cú pháp exec để chạy các lệnh hoặc httpGet để gọi một API ngay sau khi container khởi động.
PostStart không đóng vai trò lớn trong việc duy trì khả năng sẵn sàng cho hệ thống nhưng cũng là một tính năng rất tiện lợi có thể sử dụng thay để initContainer
initContainer
PreStop Hook là một hook quan trọng để đảm bảo quá trình ngừng hoạt động của container diễn ra mượt mà. Khi container chuẩn bị bị hủy, PreStop cho phép container thực hiện một số công việc dọn dẹp hoặc lưu trữ dữ liệu. Bạn có thể dùng PreStop để đóng các kết nối mạng, lưu dữ liệu đang xử lý, hoặc thông báo cho các dịch vụ khác rằng container sắp ngừng hoạt động. Điều này giúp tránh mất dữ liệu và giảm thiểu ảnh hưởng đến trải nghiệm người dùng.
Trong thực tế, rất nhiều ứng dụng có thời gian tắt khá lâu và lớn hơn 30 giây mặc định. Vậy nên các DevOps Engineer thường cấu hình câu lệnh sleep 30s để kéo dài hơn quá trình shutdown 1 pod, giúp ứng dụng có thể đạt graceful shutdown.
sleep 30s
Dưới đây là cấu hình mẫu khi sử dụng PostStart và PreStop để dừng dịch vụ nginx:
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo-container
    image: nginx
    lifecycle:
      postStart:
        exec:
          command: [""/bin/sh"", ""-c"", ""echo Hello from the postStart handler > /usr/share/message""]
      preStop:
        exec:
          command: [""/bin/sh"",""-c"",""nginx -s quit; while killall -0 nginx; do sleep 1; done""]
Pod Disruption Budget (PDB) là một cơ chế cho phép bạn thiết lập số lượng tối đa các pod có thể ngừng hoạt động tại bất kỳ thời điểm nào để duy trì dịch vụ. Khi bạn thực hiện nâng cấp hệ thống hoặc có một sự cố nào đó, PDB đảm bảo rằng một số pod nhất định vẫn tiếp tục hoạt động để không ảnh hưởng đến trải nghiệm người dùng. PDB đặc biệt hữu ích khi kết hợp với các chiến lược cập nhật như Rolling Update, cho phép bạn bảo trì mà không làm gián đoạn dịch vụ. Pod Disruption Budget mình cho rằng là cấu hình luôn luôn phải có ở trong môi trường Production.
Ví dụ, nếu bạn có 5 pod, bạn có thể thiết lập PDB để chỉ cho phép 1 pod ngừng hoạt động tại một thời điểm. Điều này đảm bảo rằng ngay cả trong quá trình cập nhật hoặc sự cố, dịch vụ vẫn luôn duy trì tính sẵn sàng ở mức tối thiểu nhất định.
Cấu hình mẫu cho Pod Disruption Budget với số lượng pod sẵn sàng ít nhất là 2 :
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp
Pod Anti-Affinity là một thiết lập giúp bạn định hướng vị trí triển khai các pod trong cluster để tránh việc chúng bị gom vào một node hay một zone duy nhất. Điều này rất quan trọng để đảm bảo độ sẵn sàng của dịch vụ, vì nếu các pod của cùng một ứng dụng đều nằm trên một node và node đó gặp sự cố, toàn bộ dịch vụ sẽ bị gián đoạn.
Bằng cách sử dụng Anti-Affinity, bạn có thể chỉ định rằng các pod cần được phân tán trên nhiều node khác nhau, giúp giảm thiểu rủi ro khi có sự cố node. Anti-Affinity thường được áp dụng cho các ứng dụng cần độ sẵn sàng cao và chịu tải lớn. Thiết lập này giúp tăng cường khả năng chịu lỗi và cải thiện độ sẵn sàng tổng thể của hệ thống.
Cấu hình mẫu Pod Anti-Affinity cho một deployment nginx:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: anti-affinity-demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - myapp
            topologyKey: ""kubernetes.io/hostname"" # Yêu cầu các pod không nằm trên cùng 1 node
      containers:
      - name: nginx
        image: nginx
Như vậy thông qua bài viết mình đã đưa đến các bạn tính năng trong Kubernetes giúp ta tăng và đảm bảo tính sẵn sàng cao của ứng dụng là:
Readiness & Liveness Probes
Resource request & limit
Pod Anti-affinity
Pod Disruption Budget
Horizontal Pod Autoscaling
PostStart & PreStop Hook
Deployment Strategy
Ngoài ra còn rất nhiều các yếu tố khác ảnh hưởng đến độ sẵn sàng của ứng dụng như codebase, hạ tầng vật lý,... mà chúng ta cũng cần quan tâm đến. Tuy nhiên về phía DevOps quản trị tầng application thì đảm bảo cấu hình hợp lý các yếu tố trên đã đóng góp 80% trong khả năng sẵn sàng của ứng dụng rồi.
Hy vọng bài viết đã đem lại một vài ý tưởng mới cho bạn trong công việc! Nếu thấy bài viết hay hãy Upvote bài viết ở  góc trên bên trái và Follow mình để hóng thêm các bài viết khác nữa nhé  Have a nice day!
Nếu như bạn đang gặp khúc mắc trong vấn đề chuyên môn hay cần người hỗ trợ về mặt hệ thống, DevOps tools thì mình tự tin có thể hỗ trợ được bạn. Liên hệ với mình để trao đổi thêm nhé https://hoangviet.io.vn/",https://viblo.asia/p/dam-bao-kha-nang-san-sang-cao-cho-ung-dung-tren-kubernetes-high-availability-for-application-on-kubernetes-EbNVQAg1VvR
7,Surfshark vs NordVPN: Expert VPN Service Analysis,"Choosing the right VPN can save you money while keeping your data secure. Let's examine how Surfshark and NordVPN stack up against each other.
Price Breakdown & Value
Surfshark catches attention with its budget-friendly plans, offering the Starter package at €1.99/month with a 24-month commitment. They sweeten the deal by adding 4 bonus months to long-term subscriptions.
NordVPN positions itself in a slightly higher price bracket, with their Basic plan starting at €3.59/month for a 2-year subscription. They include 3 extra months with extended plans. While pricier, NordVPN packs additional features into their service tiers.
Service Tiers & Features
NordVPN's Package Structure:
Basic: Standard VPN features
Plus: Adds blocking tools, password manager, malware shield
Ultimate: Includes 1TB cloud storage, €5K cyber insurance coverage
Surfshark's Offerings:
Starter: Core VPN protection
One: VPN + antivirus bundle
One+: Full security suite with data removal tools
Standout Elements
Surfshark's Edge:
Their Incogni service stands out by actively clearing your personal data from broker databases - a valuable tool for privacy-conscious users. The service hunts down companies trading your information and demands removal, tackling a growing privacy concern head-on.
NordVPN's Strengths:
Advanced Threat Protection Pro system
Cybersecurity insurance up to €5K
Secure cloud storage integration
Coverage for up to 10 devices simultaneously
Monthly Investment Options
**Surfshark Pricing:
**
Monthly: €15.45
Yearly: €2.99/month
Two Years: €1.99/month
NordVPN Rates:
Monthly: €12.99
Yearly: €4.59/month
Two Years: €3.59/month
Both services back their products with a 30-day refund policy.
Essential Protection Features
These VPNs deliver core security elements:
Connection encryption
Breach prevention
Anti-hacking measures
Virus blocking
Data collection prevention
Making Your Choice
Consider Surfshark if you:
Prioritize cost-effectiveness
Want active data broker removal
Need straightforward service options
NordVPN might suit you better if you:
Value included insurance coverage
Require integrated cloud storage
Want advanced threat protection
Seek shopping fraud protection
Each service excels in different areas. Surfshark wins on pure value and privacy focus, while NordVPN shines with its security extras and insurance benefits.
For budget-conscious users seeking solid protection, Surfshark delivers excellent value. If you're after a feature-rich security package and don't mind the higher cost, NordVPN's Ultimate plan offers robust protection plus valuable extras.
Remember: Both providers offer strong core VPN services - your choice should align with your specific needs and budget constraints. Consider which extra features matter most to you before committing to a long-term plan.",https://viblo.asia/p/surfshark-vs-nordvpn-expert-vpn-service-analysis-7ymJXGG64kq
8,#6 Xây dựng giao diện game bằng EzySmasher,"Trong phần trước chúng ta đã đổi background màn login. Trong phần này chúng ta sẽ thực hiện nhiều thay đổi hơn cho con game Wukong của chúng ta nhé.
Thêm tiêu đề vào cho game
Mình nghĩ là cần có một tilte gì đó để khi người chơi vào game có thể biết được game là game gì.
Các bạn truy cập vào màn Login Scene > Canvas > Parent > HorizontalLayout > LeftVerticalLayout > TextObject (1) .
Login Scene
Canvas
Parent
HorizontalLayout
LeftVerticalLayout
TextObject (1)
Dupplicate một Text Object ra và đặt tên nó là title
Text Object
Mục TextMeshPro > Text Input các bạn sửa đoạn Sign in thành <tên game của các bạn> .
TextMeshPro
Text Input
Trong tình huống này mình sẽ để tên game là Wukong go to the west

Các bạn căn chỉnh lại trên mục và ta có thành quả :

Lưu lại và chạy thử :

Thay đổi background màn sảnh chờ và màn chiến đấu
Các bạn truy cập vào màn hình  Lobby Scene , và làm tương tự mình hướng dẫn ở phần trước  :
Lobby Scene
Đối với màn giao chiến, các bạn vào màn Game2DScene, các bạn giữ chuột phải và chọn Open , giao diện sẽ hiển thị ra :
Game2DScene
Open

Tiếp đó mình muốn thay background cho game.  Sau một hồi tìm kiếm trên mạng, thì mình cũng chọn được background tạm ổn, các bạn có thể tìm hoặc tự thiết kế các background tuỳ ý thích của bạn nhé, nhưng nhớ là kích thước nên chia hết cho 4 (mình recommend kích thước 1920 x 1080 )  :

Trong màn Game2DScene. Các bạn lướt xuống và tìm component Floor nhé
Game2DScene
Tại đó các bạn có thể dễ dàng để ý thấy mục Sprite Render
Các bạn sửa mục Sprite giúp mình, chọn background mà các bạn mong muốn (Phần này mình có hướng dẫn cách sửa background ở bài trước rôi, các bạn có thể xem lại), và ta sẽ được kết quả như ý. Các bạn có thể lưu lại và chơi thử để xem :

Sau khi mình chạy thử thì có vẻ như background có vẻ lên rồi. Tuy nhiên kích thước của background có vẻ hơi rộng một chút.
Chúng ta có thể chỉnh kích thước sao cho bản đồ sao cho phù hợp bằng cách kéo mép bản đồ sao cho nó vừa với khung hình. Các bạn xem hình dưới đây mình làm nhé :


Giờ khi chúng ta chạy thử chương trình thì có vẻ mọi thứ đã ổn hơn :
Link tham khảo
ezyplatform
ezySmashers
Lời kết
Trong phần này chúng ta đã thay đổi được map và tiêu đề của game, trong phần sau chúng ta sẽ tập trung vào thay đổi phần nhân vật trong game nhé",https://viblo.asia/p/6-xay-dung-giao-dien-game-bang-ezysmasher-vlZL955WLQK
9,"Các phương pháp thực hành, mẹo và thủ thuật hay nhất của Laravel cho năm 2025","Khi chúng ta tiến gần đến năm 2025, Laravel vẫn tiếp tục là lựa chọn hàng đầu cho phát triển web. Nó cung cấp cho các nhà phát triển công cụ mạnh mẽ và hiệu quả để tạo ra các ứng dụng. Bài viết này sẽ tóm tắt các thực hành tốt nhất, mẹo và thủ thuật chính để nâng cao chất lượng mã và tối ưu hóa quy trình làm việc trong Laravel. Những hiểu biết này sẽ hữu ích cho bất kỳ ai sử dụng Laravel 9, 10 hoặc 11, giúp bạn tận dụng tối đa Laravel và cải thiện kỹ năng phát triển của mình.
Giữ Laravel và Các Gói Luôn Cập Nhật
Giữ cho Laravel luôn được cập nhật có nhiều lợi ích quan trọng:
Cải thiện bảo mật: Các bản phát hành định kỳ thường chứa các bản sửa lỗi bảo mật quan trọng để bảo vệ ứng dụng của bạn.
Cải thiện hiệu suất: Các bản cập nhật thường cải thiện hiệu suất, giảm thời gian tải và làm cho mã hiệu quả hơn.
Tận dụng tính năng mới: Luôn cập nhật để sử dụng các tính năng mới nhất.
Tăng cường khả năng tương thích: Cải thiện khả năng tương thích với các gói chính thức và cộng đồng mới nhất.
Chạy lệnh composer update định kỳ là một cách quan trọng để giữ cho các gói của bạn luôn được cập nhật. Thực hành đơn giản này là một trong những cách tốt nhất để đảm bảo rằng các gói của bạn được cập nhật với các tính năng và bản sửa lỗi bảo mật mới nhất.
composer update
Nếu bạn lo ngại về việc cập nhật, điều đó có thể cho thấy rằng mã của bạn chưa được kiểm tra đầy đủ. Áp dụng các thực hành kiểm tra sẽ giúp bạn đảm bảo rằng các bản cập nhật lớn không làm hỏng mã của bạn, cho phép bạn tận dụng các cải tiến của Laravel.
Nếu bạn không muốn cập nhật bằng tay, hãy xem xét sử dụng một công cụ quản lý môi trường như ServBay. Nó đã tích hợp phiên bản beta của PHP 8.4 và cho phép bạn dễ dàng cập nhật Laravel và nhiều gói khác.
Duy Trì Kiểm Tra Tự Động Mạnh Mẽ Để Ngăn Ngừa Lỗi Quan Trọng
Kiểm tra tự động là một lĩnh vực rộng lớn và thường bị đánh giá thấp, nhưng nó là một trong những thực hành quan trọng nhất để đảm bảo độ tin cậy của dự án.
Các lợi ích chính của việc duy trì một bộ kiểm tra mạnh mẽ bao gồm:
Giảm lỗi: Kiểm tra định kỳ giúp phát hiện sớm các vấn đề và tạo ra một mã nguồn ổn định hơn.
Sự hài lòng của khách hàng: Phần mềm đáng tin cậy giúp người dùng tin tưởng vào sản phẩm.
Sự hài lòng của nhà tuyển dụng: Dự án được kiểm tra đầy đủ phản ánh sự chuyên nghiệp và cam kết với chất lượng.
Sự tự tin của nhà phát triển: Với một bộ kiểm tra toàn diện, bạn sẽ ít lo lắng hơn khi thực hiện thay đổi, và việc quay lại dự án sẽ dễ dàng hơn.
Khi thay đổi mã, chỉ cần chạy php artisan test để xác định các tính năng bị hỏng, sửa lỗi và chạy lại!
php artisan test
Kiểm tra tự động có thể cải thiện đáng kể độ ổn định của dự án và nâng cao sự nghiệp của bạn.
Để bắt đầu, Laracasts cung cấp miễn phí các khóa học về kiểm tra. Bạn có thể tìm hiểu về PHPUnit (tiêu chuẩn ngành) hoặc Pest (một framework kiểm tra hiện đại giúp đơn giản hóa kiểm tra PHP). Khóa học ""Pest From Scratch"" đặc biệt được khuyến nghị.
Tuân Thủ Cấu Trúc Thư Mục Mặc Định
Tại sao lại cần sử dụng framework?
Framework cung cấp một cách tiếp cận cấu trúc cho việc phát triển, giúp tất cả các thành viên trong nhóm duy trì sự nhất quán và làm việc hướng tới một mục tiêu chung. Framework có nhiều tính năng phong phú và phức tạp, cho phép bạn tập trung vào các khía cạnh độc đáo của dự án.
Vì vậy, có phải việc tuân thủ cấu trúc dự án mặc định của Laravel là một thực hành tốt không? Chắc chắn là có! Dưới đây là một số lý do:
Tiện lợi: Các thói quen và cấu trúc của Laravel được tài liệu hóa tốt. Sau vài tuần hoặc vài tháng quay lại dự án, bạn sẽ đánh giá cao sự rõ ràng và thân thuộc của cấu trúc mặc định.
Hợp tác trong nhóm: Khi làm việc với các thành viên trong nhóm, tất cả mọi người đều hiểu các thói quen của Laravel, giúp việc hợp tác trở nên dễ dàng hơn. Bằng cách tận dụng kiến thức chung này, bạn có thể thúc đẩy dự án mà không cần phải tái phát minh cho mọi thứ.
Sử Dụng Yêu Cầu Biểu Mẫu Tùy Chỉnh Để Xử Lý Xác Thực Phức Tạp
Yêu cầu biểu mẫu tùy chỉnh của Laravel là một công cụ mạnh mẽ để xử lý xác thực và có một số lợi ích quan trọng:
Tái sử dụng logic xác thực: Bạn có thể dễ dàng tái sử dụng các quy tắc xác thực trong nhiều bộ điều khiển, giữ cho mã của bạn DRY (Don't Repeat Yourself).
Giảm thiểu độ phình to của bộ điều khiển: Chuyển logic xác thực sang lớp yêu cầu biểu mẫu riêng biệt, giúp bộ điều khiển của bạn sạch sẽ và tập trung vào logic kinh doanh.
Tạo Yêu Cầu Biểu Mẫu Tùy Chỉnh
Để tạo yêu cầu biểu mẫu tùy chỉnh, bạn chỉ cần chạy lệnh Artisan sau:
php artisan make:request StorePostRequest
Sau đó, chỉ cần gán yêu cầu trong bộ điều khiển của bạn:
use App\Http\Requests\StorePostRequest;

class PostController
{
    public function store(StorePostRequest $request)
    {
        $validated = $request->validated();
        Post::create($validated);
        // Logic khác...
    }
}
Tăng Cường Yêu Cầu Biểu Mẫu Tùy Chỉnh
Yêu cầu biểu mẫu tùy chỉnh cũng có thể bao gồm logic bổ sung ngoài xác thực. Ví dụ, bạn có thể ghi đè phương thức prepareForValidation để chuẩn hóa dữ liệu đầu vào:
prepareForValidation
public function prepareForValidation()
{
    $this->merge([
        'slug' => Str::slug($this->title),
    ]);
}
Trong ví dụ này, một slug sẽ tự động được tạo từ tiêu đề trước khi quá trình xác thực bắt đầu.
Quản Lý Ủy Quyền
Yêu cầu biểu mẫu tùy chỉnh cũng có thể xử lý logic ủy quyền. Nếu việc sử dụng chính sách trở nên rườm rà, bạn có thể bao gồm các kiểm tra ủy quyền đơn giản trực tiếp trong yêu cầu biểu mẫu.
Đơn Giản Hóa Mã Nguồn Với Bộ Điều Khiển Hành Động Đơn
Ngay cả khi tuân thủ các thực hành tốt nhất, các bộ điều khiển có thể trở nên khó quản lý. Một giải pháp tuyệt vời là sử dụng bộ điều khiển hành động đơn của Laravel. Những bộ điều khiển này được thiết kế để xử lý một hành động duy nhất thay vì nhiều hành động như bộ điều khiển tài nguyên tiêu chuẩn.
Tạo Bộ Điều Khiển Hành Động Đơn
Để tạo bộ điều khiển hành động đơn, bạn có thể sử dụng lệnh Artisan sau:
php artisan make:controller ShowPostController --invokable
Lệnh này sẽ tạo một bộ điều khiển với một hành động có tên là __invoke. Bạn có thể tìm hiểu thêm về phương thức ma thuật __invoke trong tài liệu của Laravel.
__invoke
__invoke
Định Nghĩa Đường Dẫn
Sử dụng bộ điều khiển hành động đơn giúp đơn giản hóa việc định nghĩa đường dẫn. Bạn có thể sử dụng như sau:
use App\Http\Controllers\ShowPostController;

// Đường dẫn cho bộ điều khiển hành động đơn
Route::get('/posts/{post}', ShowPostController::class);
Thực Hành Tốt Tùy Thuộc
Việc sử dụng bộ điều khiển hành động đơn là một thực hành tốt tùy thuộc vào sở thích cá nhân. Cách tiếp cận này có phù hợp với cấu trúc và khả năng đọc mã của dự án hay không là tùy thuộc vào bạn và nhóm của bạn.
Sử Dụng Middleware Để Tránh Lặp Lại Mã
Trong Laravel, middleware là một công cụ mạnh mẽ để lọc hoặc thay đổi các yêu cầu đến. Bằng cách tận dụng middleware, bạn có thể tránh lặp lại mã và giữ cho bộ điều khiển của mình sạch sẽ và dễ quản lý. Dưới đây là một số ví dụ về việc sử dụng middleware phổ biến:
Kiểm tra quyền truy cập cần thiết: Đảm bảo người dùng có quyền truy cập vào một đường dẫn cụ thể.
Cài đặt ngôn ngữ của người dùng: Kiểm tra ngôn ngữ ưa thích của người dùng và điều chỉnh ngôn ngữ của ứng dụng cho phù hợp.
Laravel cung cấp nhiều middleware tích hợp để xử lý các tác vụ phổ biến như xác thực và giới hạn tốc độ.
Tạo Middleware Tùy Chỉnh
Để tạo middleware tùy chỉnh, bạn có thể sử dụng lệnh Artisan sau:
php artisan make:middleware CheckTokens
Trong middleware, bạn có thể định nghĩa logic để xử lý yêu cầu. Dưới đây là một ví dụ về middleware kiểm tra xem người dùng có đủ token hay không:
public function handle(Request $request, Closure $next): Response
{
    if (! $request->user()->hasEnoughTokens()) {
        abort(403); // Cấm
    }
    return $next($request); // Tiến đến middleware hoặc trình xử lý yêu cầu tiếp theo
}
Gán Middleware Cho Đường Dẫn
Sau khi định nghĩa middleware, bạn có thể gán nó cho bất kỳ số lượng đường dẫn nào. Cách tiếp cận mô-đun này giúp giữ cho mã của bạn sạch sẽ và giảm thiểu sự lặp lại trong bộ điều khiển.
Route::middleware('check.tokens')->group(function () {
    Route::get('/protected-resource', [ResourceController::class, 'index']);
    Route::post('/protected-action', [ResourceController::class, 'store']);
});
Sử Dụng Chính Sách Để Quản Lý Ủy Quyền
Việc tận dụng chính sách trong Laravel để quản lý ủy quyền là rất quan trọng để xây dựng một ứng dụng rõ ràng và dễ bảo trì. Những lợi ích chính của việc sử dụng chính sách bao gồm:
Chia sẻ logic ủy quyền giữa các bộ điều khiển: Tích hợp các quy tắc ủy quyền vào chính sách giúp thúc đẩy sự nhất quán và loại bỏ mã trùng lặp trong ứng dụng.
Giảm độ phức tạp của bộ điều khiển: Di chuyển logic ủy quyền vào chính sách giúp bộ điều khiển tập trung vào các tác vụ chính, giữ cho mã sạch và dễ hiểu.
Đơn giản hóa việc truy cập vào mã ủy quyền: Các chính sách được đặt trong thư mục app/Policies, giúp các nhà phát triển dễ dàng tìm và sửa đổi các quy tắc ủy quyền khi cần.
app/Policies
Hãy xem một kịch bản thực tế về việc sử dụng chính sách để cập nhật bài viết:
// app/Policies/PostPolicy.php
public function update(User $user, Post $post)
{
    return $user->id === $post->user_id; // Kiểm tra xem người dùng có sở hữu bài viết hay không
}

// app/Http/Controllers/PostController.php
public function update(Request $request, Post $post)
{
    $this->authorize('update', $post); // Kiểm tra xem hành động cập nhật có được ủy quyền hay không

    // Nếu được ủy quyền, tiến hành quá trình cập nhật
    // ...
}
Quản Lý Migration Một Cách Hiệu Quả
Migration là cách định nghĩa schema cơ sở dữ liệu bằng mã PHP thuần túy. Bạn có thể coi điều này như một sự thay thế cho việc sử dụng phpMyAdmin. Cách tiếp cận này đặc biệt hữu ích cho các nhóm, vì tất cả các thành viên có thể tái tạo cùng một môi trường phát triển trên máy local và duy trì lịch sử thay đổi bằng Git.
Migration cũng rất hữu ích khi triển khai dự án đến các môi trường khác (như staging hoặc production). Bạn không cần phải yêu cầu xuất cơ sở dữ liệu từ các môi trường khác. Tuy nhiên, một cạm bẫy phổ biến là một số nhà phát triển thực hiện thay đổi trực tiếp vào cơ sở dữ liệu thay vì tạo migration mới. Thực hành này có thể khiến các thành viên khác trong nhóm phải yêu cầu dump cơ sở dữ liệu. Để cải thiện dự án và duy trì sự nhất quán, việc sử dụng migration một cách hiệu quả là rất quan trọng.
Sử Dụng Migration Ẩn Danh Để Tránh Xung Đột (Từ Laravel 8 Trở Đi)
Migration ẩn danh là một giải pháp hiệu quả để tránh xung đột tên lớp. Với tính năng này, bạn có thể tạo nhiều migration với cùng một tên mà không gặp phải vấn đề.
Từ Laravel 9 trở đi, khi bạn chạy lệnh sau, migration ẩn danh sẽ được tạo tự động:
php artisan make:migration UpdatePostsTable
Cấu trúc của các migration này sẽ như sau:
<?php

use Illuminate\Support\Facades\Schema;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Database\Migrations\Migration;

return new class extends Migration {
    // Logic migration sẽ ở đây
};
Điều thú vị là bạn cũng có thể thực hiện migration ẩn danh trong Laravel 8. Để làm điều này, chỉ cần thay thế tên lớp bằng return new class và thêm dấu chấm phẩy ở cuối. Điều này cho phép bạn tận hưởng lợi ích của migration ẩn danh ngay cả trong các phiên bản trước.
return new class
Thực Hiện down() Đúng Cách Để Quay Lại Thay Đổi
down()
Phương thức down() được sử dụng để hoàn tác các thay đổi được thực hiện bởi lệnh php artisan migrate:rollback. Một số nhà phát triển thực hiện điều này trong khi những người khác không, nhưng điều quan trọng là đảm bảo rằng phương thức down() được định nghĩa đúng cách khi bạn sử dụng nó.
down()
php artisan migrate:rollback
down()
Cơ bản là, phương thức down() cần phải đảo ngược các thao tác được thực hiện trong phương thức up(). Dưới đây là một ví dụ cơ bản:
down()
up()
<?php

use Illuminate\Support\Facades\Schema;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Database\Migrations\Migration;

return new class extends Migration {
    public function up()
    {
        Schema::table('posts', function (Blueprint $table) {
            // Thay đổi cột từ boolean sang datetime
            $table->datetime('is_published')->nullable()->change();
        });
    }

    public function down()
    {
        Schema::table('posts', function (Blueprint $table) {
            // Đưa cột về trạng thái ban đầu khi quay lại
            $table->boolean('is_published')->default(false)->change();
        });
    }
};
Việc sử dụng phương thức down() là rất quan trọng để đảm bảo rằng bạn có thể quay lại các thay đổi một cách dễ dàng và an toàn.
down()
Tận Dụng Các Tính Năng Của Eloquent
Eloquent ORM của Laravel là một công cụ mạnh mẽ để đơn giản hóa các thao tác cơ sở dữ liệu. Bằng cách tận dụng các tính năng của Eloquent, bạn có thể nâng cao khả năng đọc và bảo trì mã.
1. Eager Loading
Sử dụng eager loading giúp bạn tránh vấn đề N+1 và truy xuất dữ liệu liên quan một cách hiệu quả. Ví dụ, để lấy bài viết và bình luận của chúng, bạn có thể viết như sau:
$posts = Post::with('comments')->get();
Điều này sẽ đảm bảo rằng tất cả các bài viết và bình luận liên quan được lấy trong một truy vấn, cải thiện hiệu suất.
2. Chế Độ Nghiêm Ngặt
Việc sử dụng chế độ nghiêm ngặt của Eloquent giúp bạn ngăn chặn việc chèn các cột không mong muốn. Điều này giúp duy trì tính toàn vẹn của dữ liệu.
protected $guarded = []; // Cho phép tất cả các cột

protected $fillable = ['title', 'content']; // Chỉ cho phép một số cột cụ thể
3. Accessors và Mutators
Eloquent cho phép bạn sử dụng accessors và mutators để dễ dàng thao tác với các thuộc tính của mô hình. Ví dụ, bạn có thể định dạng ngày tháng lấy từ cơ sở dữ liệu hoặc chuyển đổi chúng khi lưu.
// Accessor
public function getCreatedAtAttribute($value)
{
    return Carbon::parse($value)->format('Y-m-d');
}

// Mutator
public function setTitleAttribute($value)
{
    $this->attributes['title'] = strtolower($value);
}
Sử Dụng Queue Để Xử Lý Các Tác Vụ Chạy Lâu
Hệ thống queue của Laravel cho phép bạn xử lý các tác vụ chạy lâu trong nền, cải thiện khả năng phản hồi của ứng dụng. Điều này cho phép người dùng tiếp tục các thao tác khác mà không phải chờ đợi quá trình hoàn tất.
Cài Đặt Queue
Để sử dụng queue, trước tiên bạn cần cấu hình driver queue. Bạn có thể định cấu hình trong config/queue.php và chọn driver mặc định. Mặc định, các driver như database, Redis, Beanstalkd có sẵn.
config/queue.php
'driver' => env('QUEUE_DRIVER', 'database'),
Tạo Job
Để tạo một job, bạn có thể sử dụng lệnh Artisan:
php artisan make:job ProcessOrder
Điều này sẽ tạo ra một lớp job trong thư mục app/Jobs. Bạn có thể định nghĩa logic xử lý trong phương thức handle():
app/Jobs
handle()
namespace App\Jobs;

use App\Models\Order;

class ProcessOrder
{
    protected $order;

    public function __construct(Order $order)
    {
        $this->order = $order;
    }

    public function handle()
    {
        // Logic xử lý đơn hàng
        // Gửi email, cập nhật trạng thái, v.v.
    }
}
Đưa Job Vào Queue
Để đưa job vào queue, bạn có thể sử dụng phương thức dispatch():
dispatch()
ProcessOrder::dispatch($order);
Khi job được đưa vào queue, nó sẽ được xử lý bởi worker queue. Bạn có thể chạy worker bằng lệnh:
php artisan queue:work
Quản Lý Queue
Laravel cung cấp một số lệnh hữu ích để quản lý queue:
Xem các job trong queue: Bạn có thể kiểm tra các job đang chờ xử lý trong cơ sở dữ liệu (nếu bạn sử dụng driver database).
Giám sát queue: Sử dụng Horizon (nếu bạn đang sử dụng Redis) để giám sát và quản lý queue một cách trực quan.
Làm Mới Cơ Sở Dữ Liệu Trước Khi Kiểm Tra
Một thực hành tốt là làm mới cơ sở dữ liệu trước khi chạy các bài kiểm tra. Điều này đảm bảo rằng mỗi lần kiểm tra đều bắt đầu từ một trạng thái sạch sẽ, giúp phát hiện lỗi dễ dàng hơn và đảm bảo tính nhất quán.
Bạn có thể thực hiện việc này bằng cách sử dụng phương thức refreshDatabase trong lớp kiểm tra của bạn:
refreshDatabase
use Illuminate\Foundation\Testing\RefreshDatabase;

class PostTest extends TestCase
{
    use RefreshDatabase;

    public function test_example()
    {
        // Tạo dữ liệu và thực hiện kiểm tra
    }
}
Sử dụng RefreshDatabase sẽ tự động chạy các migration và làm mới cơ sở dữ liệu trước mỗi bài kiểm tra, giúp bạn tránh được các vấn đề liên quan đến dữ liệu tồn tại từ các bài kiểm tra trước đó.
RefreshDatabase
Quản Lý Thông Tin Nhạy Cảm
Bảo mật thông tin nhạy cảm là một phần quan trọng trong phát triển ứng dụng. Laravel cung cấp nhiều phương pháp để quản lý thông tin nhạy cảm, bao gồm:
Biến môi trường: Sử dụng tệp .env để lưu trữ thông tin nhạy cảm như mật khẩu cơ sở dữ liệu hoặc khóa API. Đảm bảo không đưa tệp này vào kho mã nguồn.
.env
Mã hóa: Sử dụng các phương thức mã hóa của Laravel để mã hóa thông tin nhạy cảm trước khi lưu vào cơ sở dữ liệu.
Ví dụ, bạn có thể mã hóa một trường trong mô hình:
use Illuminate\Contracts\Encryption\EncryptException;
use Illuminate\Support\Facades\Crypt;

class User extends Model
{
    protected $casts = [
        'secret' => 'encrypted',
    ];
}
Sử Dụng Laravel Passport hoặc Sanctum
Đối với xác thực API, bạn có thể sử dụng Laravel Passport hoặc Sanctum để quản lý xác thực token một cách an toàn. Điều này giúp bảo mật API của bạn và đảm bảo rằng chỉ những người dùng đã xác thực mới có thể truy cập vào các tài nguyên nhạy cảm.
Kết Luận
Việc áp dụng những thực hành tốt nhất trong Laravel không chỉ giúp bạn phát triển ứng dụng an toàn và hiệu quả hơn mà còn cải thiện trải nghiệm làm việc của bạn và đội ngũ phát triển. Từ việc sử dụng migration đến quản lý thông tin nhạy cảm, mỗi thực hành đều góp phần vào sự thành công của dự án.
Hy vọng rằng những mẹo này sẽ giúp bạn tối ưu hóa quy trình phát triển và tạo ra những ứng dụng Laravel mạnh mẽ và đáng tin cậy trong năm 2025 và xa hơn nữa!",https://viblo.asia/p/cac-phuong-phap-thuc-hanh-meo-va-thu-thuat-hay-nhat-cua-laravel-cho-nam-2025-E1XVOkbPVMz
10,Rate limiter with Redis and Golang,"
Rate limiter is a mechanism that controls the number of requests or tasks performed within a certain period. It helps prevent excessive resource usage or denial of service (DDoS) attacks. When the number of requests exceeds a set limit, subsequent requests may be denied or delayed until the limit is reset. This ensures that the system operates stably and fairly for all users.
Some algorithm applies to implement Rate limiter
Leaky Bucket
Fixed Window Counter
Sliding Window Log
Sliding Window Counter
Now, there are a lot of services that support and provide for you to config Rate limiter on your website as soon as
Today, I will guide you through the implementation Rate limiter with Redis, Golang, and Fixed Window Counter algorithm:
Concept: The Fixed Window Counter algorithm counts the number of requests in a fixed period, called a ""window"". For example, you can define a window of 1 minute.
Requests: Every time a request arrives, the system checks the number of requests made in the current window. The new request will be rejected if the number of requests exceeds the specified limit during that period. The window is reset every fixed period.
Now, let's implement with Golang
Init the Redis connection
func initRedis(redisUrl string) (*redis.Client, error) {
    opts, err := redis.ParseURL(redisUrl)
    if err != nil {
        log.Fatal(""failed to connect redis:"", err)
        return nil, nil
    }

    opts.PoolSize = 30
    opts.ReadTimeout = 5 * time.Second
    opts.WriteTimeout = 5 * time.Second
    opts.Username = """"

    redisClient := redis.NewClient(opts)

    cmd := redisClient.Ping(context.Background())
    if cmd.Err() != nil {
        log.Fatal(""failed to ping redis: "", cmd.Err())
        return nil, nil
    }

    return redisClient, nil
}
The function gets IP from the request. I will limit the number of requests in a minute following client's IP with 60 requests / 1 minute
func getIPFromRequest(r *http.Request) string {
    ips := r.Header.Get(""X-Forwarded-For"")
    ipList := strings.Split(ips, "","")
    for _, ip := range ipList {
        if ip = strings.TrimSpace(ip); ip != """" && ip != ""::1"" && ip != ""127.0.0.1"" {
            return ip
        }
    }

    ip := r.Header.Get(""X-Real-IP"")
    if ip != """" {
        return ip
    }

    ip, _, err := net.SplitHostPort(r.RemoteAddr)
    if err != nil {
        return r.RemoteAddr
    }
    return ip
}
Middleware to check rate limit. I will write a Lua script with Redis. I will increase one unit when the request is accessed and check if the counter is greater than the maximum number of requests in a minute then return ""not pass"" or else ""pass"". If the request passes the middleware then It forwards it to the service to process and return status code 200 else it returns status code 429 for too many requests.
func (h *HandlerAPI) RateLimiter(next http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        w.Header().Set(""Access-Control-Allow-Origin"", ""*"")
        w.Header().Set(""Access-Control-Allow-Methods"", ""GET, POST, PUT, DELETE, OPTIONS"")
        w.Header().Set(""Access-Control-Allow-Headers"", ""Content-Type, Origin, Authorization"")
        w.Header().Set(""Access-Control-Allow-Credentials"", ""true"")

        // Handle preflight requests
        if r.Method == ""OPTIONS"" {
            w.WriteHeader(http.StatusOK)
            return
        }

        // rate limit
        ip := getIPFromRequest(r)
        script := `
        local currentCount = tonumber(redis.call('GET', KEYS[1]) or '0')
        if currentCount == 0 then
            redis.call('SET', KEYS[1], 0, 'EX', ARGV[1])
        end

        redis.call('SET', KEYS[1], currentCount + 1, 'KEEPTTL')

        if currentCount > tonumber(ARGV[2]) then
            return ""not pass""
        else
            return ""pass""
        end`

        // Running Lua Script
        resultStr, err := h.RedisClient.Eval(context.Background(), script, []string{ip}, 60, MaxRequestOneMinute).Result()
        if err != nil {
            logrus.Warnf(""Running Lua Script is failed with err: %v"", err)
            return
        }

        if resultStr == ""not pass"" {
            w.WriteHeader(http.StatusTooManyRequests)
            return
        }

        next.ServeHTTP(w, r)
    })
}
Test handler will return ""Hello Viet Nam"" with status code 200
func (h *HandlerAPI) testHandler(w http.ResponseWriter, r *http.Request) {
    if r.Method != http.MethodGet {
        http.Error(w, ""Method not allowed"", http.StatusMethodNotAllowed)
        return
    }

    w.WriteHeader(http.StatusOK)
    _, err := fmt.Fprintf(w, ""Hello Viet Nam"")
    if err != nil {
        return
    }
}
The main goroutine
func main() {
    redisClient, err := initRedis(""redis://default:@localhost:6379"")
    if err != nil {
        panic(""failed to init redis"")
    }

    handler := HandlerAPI{
        RedisClient: redisClient,
    }

    mux := http.NewServeMux()
    mux.Handle(""/test"", handler.RateLimiter(http.HandlerFunc(handler.testHandler)))

    // Start the server
    log.Fatal(http.ListenAndServe("":3000"", mux))
}
I will write the function to test the Rate limiter. I test to call 100 current request.
func TestHandlerAPI_testHandler(t *testing.T) {
    var wg sync.WaitGroup
    for i := 0; i < 100; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            callAPI()
        }()
    }

    wg.Wait()
    fmt.Println(""finish"")
}

func callAPI() {
    url := ""http://localhost:3000/test""
    method := ""GET""

    client := &http.Client{}
    req, err := http.NewRequest(method, url, nil)

    if err != nil {
        fmt.Println(err)
        return
    }
    res, err := client.Do(req)
    if err != nil {
        fmt.Println(err)
        return
    }
    defer func(Body io.ReadCloser) {
        err := Body.Close()
        if err != nil {
            return
        }
    }(res.Body)

    fmt.Println(res.StatusCode)
}

Source code: https://github.com/nguyenvantuan2391996/example-code/tree/master/rate-limiter
My article: https://tuannguyenhust.hashnode.dev/rate-limiter-with-redis-and-golang",https://viblo.asia/p/rate-limiter-with-redis-and-golang-WR5JR6qnJGv
11,Top 5 công cụ MLOPS nguồn mở giúp nâng cao năng suất làm việc,"MLOps, hay Machine Learning Operations (Vận hành học máy), là một tập hợp các phương pháp hay quy trình nhằm hợp lý hóa quy trình phát triển, triển khai và duy trì các mô hình học máy (ML).
Nếu như bạn muốn nâng cao hiệu suất cho các dự án Machine Learning của mình? Hãy cùng khám phá top 5 công cụ MLOps nguồn mở mạnh mẽ, giúp bạn đơn giản hóa quy trình phát triển, triển khai và vận hành mô hình học máy một cách hiệu quả.
1. KitOps - Giải pháp đóng gói mô hình AI/ML tiêu chuẩn
KitOps là một dự án nguồn mở tiên tiến được thiết kế để tăng cường sự cộng tác giữa các nhà khoa học dữ liệu, nhà phát triển ứng dụng và các kỹ sư SRE (Site Reliability Engineer - Kỹ sư độ tin cậy hệ thống) làm việc trong lĩnh vực tích hợp hoặc quản lý các mô hình AI/ML tự lưu trữ.
Tại sao nên sử dụng KitOps?
Hiện tại, không có hệ thống đóng gói tiêu chuẩn và có phiên bản cho các dự án AI/ML.
Để giải quyết vấn đề này, KitOps đã được xây dựng. Mục tiêu của KitOps là trở thành một thư viện các gói có phiên bản cho dự án AI của bạn, được lưu trữ trong một registry (nơi lưu trữ các gói phần mềm) doanh nghiệp mà bạn đã sử dụng.
Dưới đây là lý do tại sao ModelKit của Kit là giải pháp tốt hơn:
Kết hợp các mô hình, tập dữ liệu, mã và tất cả ngữ cảnh mà các nhóm cần để tích hợp, kiểm tra hoặc triển khai:
Mã huấn luyện
Mã huấn luyện
Mã mô hình
Mã mô hình
Mô hình tuần tự
Mô hình tuần tự
Huấn luyện, xác thực và các tập dữ liệu khác
Huấn luyện, xác thực và các tập dữ liệu khác
Siêu dữ liệu
Siêu dữ liệu
Cho phép các nhóm sử dụng lại các registry vùng chứa hiện có của họ bằng cách đóng gói mọi thứ dưới dạng một tạo tác tuân thủ OCI (Open Container Initiative).
Hỗ trợ giải nén chỉ một phần của gói mô hình vào máy cục bộ của bạn (tiết kiệm thời gian và dung lượng).
Loại bỏ rủi ro giả mạo bằng cách sử dụng gói bất biến.
Giảm thiểu rủi ro bằng cách bao gồm nguồn gốc của mô hình và tập dữ liệu.
Sử dụng kit pack để đóng gói sổ ghi chép Jupyter, mô hình tuần tự và tập dữ liệu của bạn (dựa trên Kitfile).
Sau đó, kit push nó vào bất kỳ registry tuân thủ OCI nào, ngay cả một registry riêng tư.
Hầu hết mọi người sẽ không cần mọi thứ, vì vậy chỉ cần kit unpack từ registry từ xa để lấy mô hình, chỉ các tập dữ liệu hoặc chỉ sổ ghi chép. Hoặc, nếu bạn cần mọi thứ thì kit pull sẽ lấy mọi thứ.
Bên trong KitOps có gì?
ModelKit: Cốt lõi của KitOps là ModelKit, một định dạng đóng gói tuân thủ OCI cho phép chia sẻ liền mạch tất cả các tạo tác cần thiết liên quan đến vòng đời của mô hình AI/ML.
Kitfile: Bổ sung cho ModelKit là Kitfile, một tệp cấu hình dựa trên YAML giúp đơn giản hóa việc chia sẻ cấu hình mô hình, tập dữ liệu và mã.
Kit CLI: Kết nối mọi thứ lại với nhau là Giao diện dòng lệnh (CLI) của Kit. Kit CLI là một công cụ mạnh mẽ cho phép người dùng tạo, quản lý, chạy và triển khai ModelKit bằng cách sử dụng Kitfiles.
Bạn có thể tham khảo KitOps tại liên kết: https://kitops.ml/
2. Kubeflow - Nền tảng học máy và MLOps trên Kubernetes
Kubeflow là một nền tảng nguồn mở dành cho học máy và MLOps trên Kubernetes được Google giới thiệu. Các giai đoạn khác nhau trong vòng đời học máy điển hình được thể hiện bằng các thành phần phần mềm khác nhau trong Kubeflow, bao gồm phát triển mô hình, huấn luyện mô hình, phục vụ mô hình và học máy tự động.
Một số ưu điểm của Kubeflow:
Kubeflow tương thích với các dịch vụ đám mây (AWS, GCP, Azure) và các dịch vụ tự lưu trữ.
Nó cho phép các kỹ sư học máy tích hợp tất cả các loại framework AI để huấn luyện, tinh chỉnh, lên lịch và triển khai các mô hình.
Nó cung cấp một bảng điều khiển tập trung để giám sát và quản lý các pipeline, chỉnh sửa mã bằng Jupyter Notebook, theo dõi thử nghiệm, registry mô hình và lưu trữ tạo tác.
Bạn có thể tham khảo Kubeflow tại liên kết: https://www.kubeflow.org/
3. MLflow - Quản lý vòng đời dự án học máy toàn diện
MLflow là một nền tảng mã nguồn mở, được xây dựng có mục đích để hỗ trợ các học viên và nhóm học máy trong việc xử lý sự phức tạp của quy trình học máy. MLflow tập trung vào toàn bộ vòng đời cho các dự án học máy, đảm bảo rằng mỗi giai đoạn đều có thể quản lý, theo dõi và tái tạo được.
Nó thường được sử dụng để theo dõi và ghi nhật ký thử nghiệm. Tuy nhiên, theo thời gian, nó đã trở thành một công cụ MLOps đầu cuối cho tất cả các loại mô hình học máy, bao gồm cả LLM (Mô hình ngôn ngữ lớn).
Bạn có thể quản lý toàn bộ hệ sinh thái học máy bằng cách sử dụng CLI, Python, R, Java và REST API.
MLFlow có 6 thành phần cốt lõi:
Theo dõi (Tracking): phiên bản và lưu trữ các tham số, mã, số liệu và tệp đầu ra. Nó cũng đi kèm với hình ảnh trực quan về số liệu và tham số tương tác.
Dự án (Projects): đóng gói mã nguồn khoa học dữ liệu để tái sử dụng và tái tạo.
Mô hình (Models): lưu trữ các mô hình và siêu dữ liệu học máy ở định dạng tiêu chuẩn có thể được sử dụng sau này bởi các công cụ downstream (các công cụ xử lý dữ liệu sau khi được xử lý bởi mô hình). Nó cũng cung cấp các tùy chọn phục vụ và triển khai mô hình.
Cơ quan đăng ký mô hình (Model Registry): kho lưu trữ mô hình tập trung để quản lý vòng đời của Mô hình MLflow. Nó cung cấp các phiên bản, dòng dõi mô hình, bí danh mô hình, gắn thẻ mô hình và chú thích.
Công thức (Pipelines): các pipeline học máy cho phép bạn nhanh chóng đào tạo các mô hình chất lượng cao và triển khai chúng lên môi trường production.
LLM: cung cấp hỗ trợ cho việc đánh giá, kỹ thuật nhắc nhở (prompt engineering), theo dõi và triển khai LLM.
Bạn có thể tham khảo MLflow tại liên kết: https://mlflow.org/
4. Metaflow - Tăng năng suất cho nhà khoa học dữ liệu
MetaFlow là một thư viện Python thân thiện với con người, giúp việc phát triển, triển khai và vận hành nhiều loại ứng dụng sử dụng nhiều dữ liệu trở nên đơn giản, đặc biệt là những ứng dụng liên quan đến khoa học dữ liệu, ML và AI. Metaflow ban đầu được phát triển tại Netflix để tăng năng suất cho các nhà khoa học dữ liệu làm việc trên nhiều dự án khác nhau, từ thống kê cổ điển đến học sâu hiện đại.
Metaflow ban đầu được phát triển tại Netflix để tăng năng suất cho các nhà khoa học dữ liệu. Giờ đây, nó đã được tạo thành mã nguồn mở, vì vậy mọi người đều có thể hưởng lợi từ nó.
Nó cung cấp một API (giao diện lập trình ứng dụng) thống nhất để quản lý dữ liệu, quản lý phiên bản, điều phối, đào tạo và triển khai chế độ và tính toán. Nó tương thích với các nhà cung cấp đám mây và framework học máy chính.
Bạn có thể tham khảo Metaflow tại liên kết: https://metaflow.org/
5. MLRun - Framework điều phối AI mã nguồn mở
MLRun là một framework điều phối AI nguồn mở để quản lý các ứng dụng ML và AI tạo sinh trong suốt vòng đời của chúng. Nó tự động hóa việc chuẩn bị dữ liệu, điều chỉnh mô hình, tùy chỉnh, xác thực và tối ưu hóa các mô hình ML, LLM và các ứng dụng AI trực tiếp trên các tài nguyên đàn hồi. MLRun cho phép triển khai nhanh chóng các đường ống phục vụ và ứng dụng theo thời gian thực có thể mở rộng trong khi cung cấp khả năng quan sát tích hợp và các tùy chọn triển khai linh hoạt, hỗ trợ các môi trường đa đám mây, lai và tại chỗ.
Các chức năng cốt lõi bên trong MLRun:
Quản lý dự án : một trung tâm tập trung quản lý nhiều tài sản của dự án như dữ liệu, chức năng, công việc, quy trình làm việc, bí mật, v.v.
Dữ liệu và hiện vật : kết nối nhiều nguồn dữ liệu khác nhau, quản lý siêu dữ liệu, lập danh mục và tạo phiên bản cho hiện vật.
Cửa hàng tính năng : lưu trữ, chuẩn bị, lập danh mục và cung cấp các tính năng mô hình để đào tạo và triển khai.
Chạy hàng loạt và quy trình công việc : chạy một hoặc nhiều chức năng và thu thập, theo dõi và so sánh tất cả kết quả và hiện vật của chúng.
Đường ống phục vụ thời gian thực : triển khai nhanh chóng dữ liệu có thể mở rộng và đường ống học máy.
Giám sát thời gian thực : giám sát dữ liệu, mô hình, tài nguyên và thành phần sản xuất.
Bạn có thể tham khảo MLRun tại liên kết: https://www.mlrun.org/
Mỗi dự án đều có một số điểm tương đồng và một số điểm khác biệt. Và mỗi sản phẩm đều khác nhau, do đó nhu cầu cũng khác nhau.
Nếu bạn là người đam mê mã nguồn mở và có hứng thú hoặc hiểu biết về MLOps/DevOps, hy vọng bài viết này giúp ích cho các bạn!",https://viblo.asia/p/top-5-cong-cu-mlops-nguon-mo-giup-nang-cao-nang-suat-lam-viec-BQyJKqRwVMe
12,How to Deploy a Spring Boot Application on Apache Tomcat: A Complete Guide,"Apache Tomcat is an open-source Java servlet container that implements the Java Servlet, JavaServer Pages (JSP), Java Expression Language (EL), and WebSocket technologies. It is widely used for deploying Java-based web applications, particularly those developed using the Spring Boot framework.
Compatibility:
Spring Boot, by default, comes with an embedded Tomcat server, which allows developers to run Spring Boot applications without requiring external servers. However, in production environments, deploying Spring Boot applications on external Tomcat servers is common for scalability, security, and operational reasons.
War Deployment:
Spring Boot applications can be packaged as .jar (executable jar with an embedded server) or .war (for deployment in external servlet containers like Tomcat). When using .war packaging, Tomcat can be used to host the Spring Boot application.
1. Install Tomcat:
Download Apache Tomcat from the official site: Apache Tomcat.
2. Convert Spring Boot App to a WAR:
Spring Boot applications are packaged as .jar files by default, but to deploy them on an external Tomcat server, you need to package them as a .war.
To do this:
Update pom.xml for a Maven-based project to package it as a WAR by changing the packaging element:
war
Ensure your main application class extends SpringBootServletInitializer to enable deployment as a WAR:
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.boot.web.servlet.support.SpringBootServletInitializer;
@SpringBootApplication
public class MyApplication extends SpringBootServletInitializer {
public static void main(String[] args) {
SpringApplication.run(MyApplication.class, args);
}
}
war
3. Build the WAR File:
Use Maven (or Gradle if you use Gradle) to package the application:
mvn clean package
This will generate a .war file in the target directory.
4. Configure Tomcat for Deployment:
Place the .war file in the webapps directory of your Tomcat installation.
Alternatively, you can deploy the WAR via the Tomcat Manager (if installed and configured). Access the Tomcat Manager at http://localhost:8080/manager (or your custom port), log in, and use the ""Deploy"" section to upload the WAR file.
5. Access the Application:
Once deployed, Tomcat automatically extracts and runs the WAR file. You can access the deployed application at http://localhost:8080/{app-name}, where {app-name} is the name of your .war file.
1. Change Default Port:
If you want to run multiple services on the same machine, you might want to change Tomcat’s port number.
Open conf/server.xml and locate the <Connector element
<Connector port=""8080"" protocol=""HTTP/1.1"" connectionTimeout=""20000"" redirectPort=""8443"" />
Change the port value to the desired number.
2. Enable Tomcat Manager
Tomcat Manager is a useful web-based tool to manage applications deployed on Tomcat. However, it’s often disabled by default. To enable it, modify the tomcat-users.xml file (found in conf/) and add a user with manager role:
<role rolename=""manager-gui""/>
<user username=""admin"" password=""admin_password"" roles=""manager-gui""/>
3.Logging
Use Tomcat’s logging system for monitoring errors and application behavior. Logs can be found under the logs folder in the Tomcat installation directory.
4. Context Path
To modify the context path (the URL at which the app is accessible), you can either rename the WAR file (e.g., renaming myapp.war to ROOT.war makes it accessible at /) or modify the conf/server.xml file.
Tomcat is a lightweight servlet container used for running Java web applications.
You can deploy Spring Boot applications on Tomcat by packaging them as WAR files.
Ensure proper configuration in pom.xml and extend SpringBootServletInitializer to make the app deployable on Tomcat.
Manage and monitor the application using Tomcat’s Manager interface or logs.
This setup is commonly used in production for hosting Spring Boot apps that require more robust application management than the embedded Tomcat offers.",https://viblo.asia/p/how-to-deploy-a-spring-boot-application-on-apache-tomcat-a-complete-guide-n1j4lOxDVwl
13,Hiểu về kiểu dữ liệu Boolean trong PHP,"Kiểu dữ liệu Boolean là một phần không thể thiếu trong lập trình PHP, đóng vai trò quan trọng trong việc điều khiển luồng chương trình và thực hiện các phép so sánh logic. Bài viết này sẽ giúp bạn hiểu rõ về Boolean, cách tạo biến Boolean, chuyển đổi kiểu dữ liệu sang Boolean và cách sử dụng chúng trong các câu lệnh điều kiện.
Boolean là gì?
Trong PHP, Boolean là một kiểu dữ liệu chỉ có thể nhận một trong hai giá trị:
true: Đại diện cho trạng thái đúng hoặc khẳng định.
false: Đại diện cho trạng thái sai hoặc phủ định.
Các giá trị Boolean thường được sử dụng trong các câu lệnh điều kiện, so sánh và các phép toán logic. Điều quan trọng cần lưu ý là cả true và false đều không phân biệt chữ hoa chữ thường. Do đó, true, True, truE, TRUE đều hợp lệ.
Tạo biến Boolean và chuyển đổi kiểu dữ liệu
Bạn có thể tạo một biến Boolean bằng cách gán trực tiếp các giá trị true hoặc false:
<?php
$isAvailable = true;  // Boolean variable set to true
$isLoggedIn = false;  // Boolean variable set to false

var_dump($isAvailable); // Output: bool(true)
var_dump($isLoggedIn);  // Output: bool(false)
?>
Hàm var_dump() được sử dụng ở đây để hiển thị kiểu và giá trị của các biến.
Chuyển đổi Boolean
Trong PHP, nhiều kiểu dữ liệu có thể được chuyển đổi thành Boolean. Một giá trị có thể được ép kiểu thành boolean bằng cách sử dụng (bool). Nói chung điều này là không cần thiết bởi vì khi một giá trị được sử dụng trong một ngữ cảnh logic, nó sẽ tự động được hiểu là một giá trị kiểu bool.
Khi chuyển đổi sang boolean, các giá trị sau sẽ trả về false:
Giá trị boolean false.
Giá trị số nguyên 0.
Các giá trị số thực 0.0 và -0.0.
Chuỗi rỗng, """".
Chuỗi chứa duy nhất một ký tự số 0, ""0"".
Mảng rỗng.
Kiểu null.
Các biến chưa được khởi tạo (unset).
Một số giá trị phức tạp khác.
Tất cả các giá trị khác được coi là true, bao gồm resource và NAN.
Ví dụ:
<?php
$values = [0, 1, """", ""Hello"", [], [1, 2, 3], null, 0.0, 3.14];
foreach ($values as $value) {
    var_dump((bool)$value);
}
?>
Kết quả:
bool(false) // 0
bool(true) // 1
bool(false) // """"
bool(true) // ""Hello""
bool(false) // []
bool(true) // [1, 2, 3]
bool(false) // null
bool(false) // 0.0
bool(true) // 3.14
Sử dụng Boolean trong câu lệnh điều kiện và Toán tử so sánh
Boolean được sử dụng chủ yếu trong các câu lệnh điều kiện như if, else và switch để kiểm soát luồng của chương trình.
Ví dụ:
<?php
$isUserLoggedIn = true;

if ($isUserLoggedIn) {
    echo ""Welcome, User!""; // This message will be displayed
} else {
    echo ""Please log in."";
}
?>
Trong ví dụ này, vì $isUserLoggedIn là true, thông báo ""Welcome, User!"" sẽ được hiển thị.
PHP cung cấp nhiều toán tử so sánh khác nhau trả về giá trị Boolean:
So sánh bằng (==): Kiểm tra xem các giá trị có bằng nhau hay không.
So sánh đồng nhất (===): Kiểm tra xem các giá trị và kiểu dữ liệu có giống hệt nhau hay không.
So sánh khác (!= hoặc <>): Kiểm tra xem các giá trị có khác nhau hay không.
So sánh không đồng nhất (!==): Kiểm tra xem các giá trị hoặc kiểu dữ liệu có khác nhau hay không.
Lớn hơn (>) và Nhỏ hơn (<): Được sử dụng cho các phép so sánh số.
Ví dụ:
<?php
$a = 10;
$b = ""10"";

var_dump($a == $b);   // Output: bool(true)  - values are equal
var_dump($a === $b);  // Output: bool(false) - types are different
var_dump($a > 5);     // Output: bool(true)  - 10 is greater than 5
var_dump($a != 10);   // Output: bool(false) - 10 is equal to 10
?>
Toán tử logic và một số lưu ý khi sử dụng Boolean
Toán tử logic được sử dụng để kết hợp hoặc phủ định các biểu thức Boolean:
&& (AND): Trả về true nếu cả hai toán hạng đều là true.
|| (OR): Trả về true nếu ít nhất một toán hạng là true.
! (NOT): Phủ định một biểu thức Boolean.
Ví dụ:
<?php
$isMember = true;
$hasPaid = false;

if ($isMember && $hasPaid) {
    echo ""Access granted."";  // Will not be executed
} else {
    echo ""Access denied."";   // Output: Access denied.
}

$isAdmin = !$isMember;
var_dump($isAdmin);  // Output: bool(false) - negates true to false
?>
Một số lưu ý khi sử dụng Boolean
1. So sánh lỏng lẻo
Sử dụng == có thể dẫn đến kết quả không mong muốn do việc tự động chuyển đổi kiểu dữ liệu. Luôn sử dụng === cho các phép so sánh nghiêm ngặt.
VD:
<?php
var_dump(0 == false);  // Output: bool(true)
var_dump(0 === false); // Output: bool(false)
?>
Trong trường hợp đầu tiên, 0 và false được coi là bằng nhau vì 0 được chuyển đổi thành false khi so sánh lỏng lẻo.
2. Giá trị được coi là false
Một số giá trị như 0, ""0"" và mảng rỗng được coi là false, điều này đôi khi có thể dẫn đến lỗi logic.
VD:
<?php
$var = ""0"";

if ($var) {
    echo ""True"";  // This will not be displayed
} else {
    echo ""False""; // Output: False
}
?>
3. Đánh giá ngắn mạch
Toán tử logic && và || sử dụng đánh giá ngắn mạch. Điều này có nghĩa là nếu kết quả được xác định bởi toán hạng đầu tiên, toán hạng thứ hai sẽ không được đánh giá.
VD:
<?php
$x = false && foo(); // foo() will not be called because $x is false
$y = true || foo();  // foo() will not be called because $y is true
?>
4. Phân biệt chữ hoa chữ thường
true và false không phân biệt chữ hoa chữ thường, vì vậy True, FALSE và fAlSe đều hợp lệ nhưng không được khuyến khích sử dụng để đảm bảo tính nhất quán.
Cảm ơn các bạn đã theo dõi.",https://viblo.asia/p/hieu-ve-kieu-du-lieu-boolean-trong-php-r1QLxrQp4Aw
14,8 Must-Have Software for Programmers on Mac,"Here are 8 essential applications that every programmer should consider installing on their new Mac, along with a brief overview of their features.
1. Video Recording and Streaming: OBS Studio

Features:
Allows users to customize their streaming setup by choosing between different capture modes like displaying a virtual webcam or selecting specific windows or areas to stream.
Offers a comprehensive yet user-friendly interface for configuring stream settings, allowing granular control over audio, video, and encoding parameters.
Enables the creation of an unlimited number of scenes and seamless transitions between them, enhancing the overall streaming experience.
2. Video Player: IINA

Features:
Customizable interface themes for a personalized viewing experience.
Adjustable default behaviors and UI elements to suit individual workflows.
Control over subtitle loading preferences, giving users the flexibility to choose whether subtitles are loaded automatically.
Configurable keybindings for a tailored and efficient video playback control scheme.
Fine-grained adjustments for video, audio, and subtitle settings, allowing users to optimize playback quality to their liking.
3. Online Video Downloader: Downie 4

Features:
Downloads online videos from various platforms to local storage for offline viewing.
Streamlined downloading process by simply dragging and dropping video links onto the Downie interface or pasting them directly into the application.
Automatic video detection and download option presentation upon link input, simplifying the user experience.
4. Development Environment: ServBay

Features:
Comes pre-configured with multiple programming languages, database components, and a wide array of extensions, eliminating the need for manual compilation and setup.
Enables the use of unregistered domains and TLDs for local development, complete with free SSL certificates, simplifying the testing process.
Provides native support for HTTP/2 and HTTP/3 protocols, as well as the latest PHP 8.4, ensuring compatibility with modern web development practices.
Facilitates team collaboration by offering a unified development environment, allowing team members to share work and stay synchronized throughout the development lifecycle.
Maintains a clean system footprint by storing all software packages and data within the /Applications/ServBay directory, enabling easy backup, migration, and uninstallation when needed.
/Applications/ServBay
5. File Compression: Keka

Features:
File compression and extraction with support for password-protected archives, enhancing data security.
6. Screenshot Tool: CleanShot X

Features:
Versatile screenshot capture modes encompassing full screen, selected area, specific windows, and even menus, catering to diverse capture needs.
Provides a comprehensive set of annotation tools for enriching screenshots with text, shapes, highlights, and other visual elements.
Supports direct upload of captured screenshots to cloud storage services like iCloud or FTP servers, facilitating convenient cross-device access and sharing.
7. Vector Graphics Editor: Sketch

Features:
Professional-grade, lightweight vector drawing application on par with industry-standard tools like Adobe Photoshop, well-suited for designing websites, user interfaces, icons, and more.
Highly regarded among web and mobile designers, particularly for its intuitive interface and powerful features tailored for mobile app design workflows.
8. 4K Dynamic Wallpaper: Dynamic Wallpaper

Features:
Brings your desktop to life with an extensive library of stunning 4K ultra-high-definition video wallpapers, adding a visually immersive element to the macOS experience.",https://viblo.asia/p/8-must-have-software-for-programmers-on-mac-5pPLkR0yJRZ
15,20+ câu hỏi phỏng vấn Python bạn phải thuộc nằm lòng!,"Dưới đây là một số câu hỏi phỏng vấn Python phổ biến mà bạn có thể gặp phải:
Câu hỏi cơ bản
Python là gì?
Python là một ngôn ngữ lập trình đa năng, dễ đọc và dễ học, hỗ trợ lập trình hướng đối tượng, lập trình hàm và lập trình thủ tục.
PEP 8 là gì?
PEP 8 là một bộ quy tắc và hướng dẫn về cách viết mã Python sao cho dễ đọc và nhất quán.
Sự khác biệt giữa list và tuple là gì? List có thể thay đổi (mutable), trong khi tuple thì không thể thay đổi (immutable).
Câu hỏi nâng cao
Multithreading trong Python hoạt động như thế nào?
Python sử dụng Global Interpreter Lock (GIL) để quản lý các thread, đảm bảo rằng tại mỗi thời điểm chỉ có một thread được thực thi.
Deep copy và shallow copy khác nhau như thế nào?
Deep copy tạo ra một bản sao hoàn toàn mới của đối tượng, trong khi shallow copy chỉ sao chép các tham chiếu đến đối tượng.
Làm thế nào để quản lý bộ nhớ trong Python?
Python quản lý bộ nhớ thông qua một heap riêng và sử dụng trình thu gom rác để giải phóng bộ nhớ không sử dụng.
Câu hỏi về framework
Flask là gì?
Flask là một micro-framework cho Python, được sử dụng để phát triển các ứng dụng web.
Django là gì?
Django là một framework web cấp cao cho Python, giúp phát triển các ứng dụng web nhanh chóng và dễ dàng.
Câu hỏi về cú pháp và cấu trúc
Làm thế nào để xử lý ngoại lệ trong Python? Bạn có thể sử dụng các khối try, except, else, và finally để xử lý ngoại lệ.
Lambda function là gì?
Lambda function là một hàm ẩn danh được định nghĩa bằng từ khóa lambda, thường được sử dụng cho các thao tác ngắn gọn và đơn giản.
List comprehension là gì?
List comprehension là một cách ngắn gọn để tạo danh sách mới bằng cách lặp qua một iterable và áp dụng một biểu thức cho mỗi phần tử.
Câu hỏi về thư viện và module
Sự khác biệt giữa module và package là gì?
Module là một file chứa mã Python (ví dụ: math.py), trong khi package là một thư mục chứa nhiều module và có thể chứa các sub-packages. Package thường có một file __init__.py để đánh dấu thư mục đó là một package.
math.py
__init__.py
Làm thế nào để quản lý các thư viện bên ngoài trong Python?
Bạn có thể sử dụng pip, công cụ quản lý gói của Python, để cài đặt và quản lý các thư viện bên ngoài.
pip
Câu hỏi về hiệu suất và tối ưu hóa
Làm thế nào để tối ưu hóa mã Python?
Bạn có thể sử dụng các công cụ như cProfile để phân tích hiệu suất và tối ưu hóa các phần mã chậm.
cProfile
Sự khác biệt giữa range và xrange trong Python 2 là gì?
range trả về một danh sách, trong khi xrange trả về một đối tượng xrange, tiết kiệm bộ nhớ hơn. Lưu ý rằng xrange không tồn tại trong Python 3, thay vào đó range trong Python 3 hoạt động giống như xrange trong Python 2.
range
xrange
xrange
xrange
Làm thế nào để quản lý bộ nhớ hiệu quả trong Python?
Sử dụng các cấu trúc dữ liệu phù hợp và tránh tạo ra các đối tượng không cần thiết.
Câu hỏi về lập trình hướng đối tượng
Lớp và đối tượng là gì?
Lớp là một bản thiết kế cho các đối tượng, trong khi đối tượng là một thể hiện của lớp.
Sự khác biệt giữa phương thức tĩnh và phương thức lớp là gì?
Phương thức tĩnh được định nghĩa bằng từ khóa @staticmethod và không truy cập vào các thuộc tính của lớp, trong khi phương thức lớp được định nghĩa bằng từ khóa @classmethod và có thể truy cập vào các thuộc tính của lớp.
Đa hình trong Python là gì?
Đa hình cho phép các đối tượng của các lớp khác nhau có thể được xử lý bằng cùng một giao diện.
Hy vọng những câu hỏi này sẽ giúp bạn chuẩn bị tốt hơn cho buổi phỏng vấn. Nếu bạn cần thêm thông tin chi tiết về bất kỳ câu hỏi nào, hãy cho mình biết nhé! 😊
Tham khảo thêm một số câu hỏi phỏng vấn Python nữa ở đây nha: https://www.interviewbit.com/python-interview-questions/",https://viblo.asia/p/20-cau-hoi-phong-van-python-ban-phai-thuoc-nam-long-PAoJePqD41j
16,Nguyên tắc Solid: Nền tảng cho ứng dụng JavaScript và TypeScript bền vững,"Nguyên tắc SOLID là nền tảng của việc phát triển phần mềm rõ ràng, có khả năng mở rộng và dễ bảo trì. Bài viết này sẽ giải thích chi tiết từng nguyên tắc SOLID với các ví dụ thực tế trong cả JavaScript (JS) và TypeScript (TS).
Nguyên tắc trách nhiệm duy nhất (SRP) - Single Responsibility Principle
Nội dung của nguyên tắc: Một lớp hoặc module chỉ nên có một lý do để thay đổi. Nó chỉ nên chịu trách nhiệm cho một phần chức năng duy nhất.
1. Ví dụ trong JavaScript (React):
Trong React, chúng ta thường thấy các component chịu trách nhiệm cho quá nhiều việc—chẳng hạn như quản lý cả UI và logic nghiệp vụ.
Ví dụ về một đoạn mã vi phạm nguyên tắc
function UserProfile({ userId }) {
  const [user, setUser] = useState(null);

  useEffect(() => {
    fetchUserData();
  }, [userId]);

  async function fetchUserData() {
    const response = await fetch(`/api/users/${userId}`);
    const data = await response.json();
    setUser(data);
  }

  return <div>{user?.name}</div>;
}
Ở đây, component UserProfile vi phạm SRP vì nó xử lý cả việc render UI và lấy dữ liệu.
Đoạn mã sau khi sửa lại:
// Custom hook for fetching user data
function useUserData(userId) {
  const [user, setUser] = useState(null);

  useEffect(() => {
    async function fetchUserData() {
      const response = await fetch(`/api/users/${userId}`);
      const data = await response.json();
      setUser(data);
    }
    fetchUserData();
  }, [userId]);

  return user;
}

// UI Component
function UserProfile({ userId }) {
  const user = useUserData(userId); // Moved data fetching logic to a hook

  return <div>{user?.name}</div>;
}
Bằng cách sử dụng custom hook (useUserData), chúng ta tách logic lấy dữ liệu khỏi UI, giữ cho mỗi phần chịu trách nhiệm cho một nhiệm vụ duy nhất.
2. Ví dụ trong TypeScript (Angular):
Trong Angular, các service và component có thể trở nên lộn xộn với nhiều trách nhiệm khác nhau.
Ví dụ về đoạn mã vi phạm nguyên tắc:
@Injectable()
export class UserService {
  constructor(private http: HttpClient) {}

  getUser(userId: string) {
    return this.http.get(`/api/users/${userId}`);
  }

  updateUserProfile(userId: string, data: any) {
    // Updating the profile and handling notifications
    return this.http.put(`/api/users/${userId}`, data).subscribe(() => {
      console.log('User updated');
      alert('Profile updated successfully');
    });
  }
}
UserService này có nhiều trách nhiệm: tìm nạp, cập nhật và xử lý thông báo.
Đoạn mã sau khi sửa lại:
@Injectable()
export class UserService {
  constructor(private http: HttpClient) {}

  getUser(userId: string) {
    return this.http.get(`/api/users/${userId}`);
  }

  updateUserProfile(userId: string, data: any) {
    return this.http.put(`/api/users/${userId}`, data);
  }
}

// Separate notification service
@Injectable()
export class NotificationService {
  notify(message: string) {
    alert(message);
  }
}
Bằng cách tách việc xử lý thông báo thành một service riêng biệt (NotificationService), chúng ta đảm bảo rằng mỗi lớp có một trách nhiệm duy nhất.
Nguyên tắc Mở/Đóng (OCP) - Open/Closed Principle
Nội dung của nguyên tắc: Các thực thể phần mềm nên mở để mở rộng nhưng đóng để sửa đổi. Điều này có nghĩa là bạn sẽ có thể mở rộng hành vi của một module mà không cần thay đổi mã nguồn của nó.
1. Ví dụ trong JavaScript (React):
Bạn có thể có một hàm xác validation form hoạt động tốt nhưng có thể yêu cầu logic xác thực bổ sung trong tương lai.
Ví dụ về đoạn mã vi phạm nguyên tắc:
function validate(input) {
  if (input.length < 5) {
    return 'Input is too short';
  }
  if (!input.includes('@')) {
    return 'Invalid email';
  }
  return 'Valid input';
}
Bất cứ khi nào bạn cần một quy tắc validation mới, bạn sẽ phải sửa đổi hàm này, vi phạm OCP.
Đoạn mã sau khi sửa lại:
function validate(input, rules) {
  return rules.map(rule => rule(input)).find(result => result !== 'Valid') || 'Valid input';
}

const lengthRule = input => input.length >= 5 ? 'Valid' : 'Input is too short';
const emailRule = input => input.includes('@') ? 'Valid' : 'Invalid email';

validate('test@domain.com', [lengthRule, emailRule]);
Bây giờ, chúng ta có thể mở rộng các quy tắc validation mà không cần sửa đổi hàm validate ban đầu, tuân thủ OCP.
2. Ví dụ trong TypeScript (Angular):
Trong Angular, các service và component nên được thiết kế để cho phép thêm các tính năng mới mà không cần sửa đổi logic cốt lõi.
Ví dụ về đoạn mã vi phạm nguyên tắc:
export class NotificationService {
  send(type: 'email' | 'sms', message: string) {
    if (type === 'email') {
      // Send email
    } else if (type === 'sms') {
      // Send SMS
    }
  }
}
Service này vi phạm OCP vì bạn sẽ cần sửa đổi phương thức send mỗi khi bạn thêm một loại thông báo mới (ví dụ: thông báo đẩy).
Đoạn mã sau khi sửa lại:
interface Notification {
  send(message: string): void;
}

@Injectable()
export class EmailNotification implements Notification {
  send(message: string) {
    // Send email logic
  }
}

@Injectable()
export class SMSNotification implements Notification {
  send(message: string) {
    // Send SMS logic
  }
}

@Injectable()
export class NotificationService {
  constructor(private notifications: Notification[]) {}

  notify(message: string) {
    this.notifications.forEach(n => n.send(message));
  }
}
Giờ đây, việc thêm các loại thông báo mới chỉ yêu cầu tạo các lớp mới mà không cần thay đổi chính NotificationService.
Nguyên tắc thay thế Liskov (LSP) - Liskov Substitution Principle
Nội dung của nguyên tắc: Các kiểu con phải có thể thay thế cho các kiểu cơ sở của chúng. Các lớp hoặc component dẫn xuất sẽ có thể thay thế các lớp cơ sở mà không ảnh hưởng đến tính chính xác của chương trình.
1. Ví dụ trong JavaScript (React):
Khi sử dụng các higher-order component (HOCs) hoặc render các component khác nhau một cách có điều kiện, LSP giúp đảm bảo rằng tất cả các component đều hoạt động theo cách có thể dự đoán được.
Ví dụ về đoạn mã vi phạm nguyên tắc:
function Button({ onClick }) {
  return <button onClick={onClick}>Click me</button>;
}

function LinkButton({ href }) {
  return <a href={href}>Click me</a>;
}

<Button onClick={() => {}} />;
<LinkButton href=""/home"" />;
Ở đây, Button và LinkButton không nhất quán. Một cái sử dụng onClick và cái còn lại sử dụng href, gây khó khăn cho việc thay thế.
Đoạn mã sau khi sửa lại:
function Clickable({ children, onClick }) {
  return <div onClick={onClick}>{children}</div>;
}

function Button({ onClick }) {
  return <Clickable onClick={onClick}>
    <button>Click me</button>
  </Clickable>;
}

function LinkButton({ href }) {
  return <Clickable onClick={() => window.location.href = href}>
    <a href={href}>Click me</a>
  </Clickable>;
}
Giờ đây, cả Button và LinkButton đều hoạt động tương tự nhau, tuân thủ LSP.
2. Ví dụ trong TypeScript (Angular)
Ví dụ về đoạn mã vi phạm nguyên tắc:
class Rectangle {
  constructor(protected width: number, protected height: number) {}

  area() {
    return this.width * this.height;
  }
}

class Square extends Rectangle {
  constructor(size: number) {
    super(size, size);
  }

  setWidth(width: number) {
    this.width = width;
    this.height = width; // Breaks LSP
  }
}
Việc sửa đổi setWidth trong Square vi phạm LSP vì Square hoạt động khác với Rectangle.
Đoạn mã sau khi sửa lại:
class Shape {
  area(): number {
    throw new Error('Method not implemented');
  }
}

class Rectangle extends Shape {
  constructor(private width: number, private height: number) {
    super();
  }

  area() {
    return this.width * this.height;
  }
}

class Square extends Shape {
  constructor(private size: number) {
    super();
  }

  area() {
    return this.size * this.size;
  }
}
Bây giờ, Square và Rectangle có thể được thay thế cho nhau mà không vi phạm LSP.
Nguyên tắc phân tách giao diện (ISP) - Interface Segregation Principle
Nội dung của nguyên tắc: Clients không nên bị ép buộc phải phụ thuộc vào các giao diện mà chúng không sử dụng.
1. Ví dụ trong JavaScript (React):
Các component React đôi khi nhận được các props không cần thiết, dẫn đến code bị ràng buộc chặt chẽ và cồng kềnh.
Ví dụ về đoạn mã vi phạm nguyên tắc:
function MultiPurposeComponent({ user, posts, comments }) {
  return (
    <div>
      <UserProfile user={user} />
      <UserPosts posts={posts} />
      <UserComments comments={comments} />
    </div>
  );
}
Ở đây, component phụ thuộc vào nhiều props, mặc dù nó có thể không phải lúc nào cũng sử dụng chúng.
Đoạn mã sau khi sửa lại:
function UserProfileComponent({ user }) {
  return <UserProfile user={user} />;
}

function UserPostsComponent({ posts }) {
  return <UserPosts posts={posts} />;
}

function UserCommentsComponent({ comments }) {
  return <UserComments comments={comments} />;
}
Bằng cách chia component thành các component nhỏ hơn, mỗi component chỉ phụ thuộc vào dữ liệu mà nó thực sự sử dụng.
2. Ví dụ trong TypeScript (Angular):
Ví dụ về đoạn mã vi phạm nguyên tắc:
interface Worker {
  work(): void;
  eat(): void;
}

class HumanWorker implements Worker {
  work() {
    console.log('Working');
  }
  eat() {
    console.log('Eating');
  }
}

class RobotWorker implements Worker {
  work() {
    console.log('Working');
  }
  eat() {
    throw new Error('Robots do not eat'); // Violates ISP
  }
}
Ở đây, RobotWorker bị ép buộc phải triển khai một phương thức eat không liên quan.
Đoạn mã sau khi sửa lại:
interface Worker {
  work(): void;
}

interface Eater {
  eat(): void;
}

class HumanWorker implements Worker, Eater {
  work() {
    console.log('Working');
  }

  eat() {
    console.log('Eating');
  }
}

class RobotWorker implements Worker {
  work() {
    console.log('Working');
  }
}
Bằng cách tách các interface Worker và Eater, chúng ta đảm bảo rằng các clients chỉ phụ thuộc vào những gì chúng cần.
Nguyên tắc đảo ngược phụ thuộc (DIP) - Dependency Inversion Principle
Nội dung của nguyên tắc: Các module cấp cao không nên phụ thuộc vào các module cấp thấp. Cả hai nên phụ thuộc vào các lớp trừu tượng (ví dụ: interface).
1. Ví dụ trong JavaScript (React):
Ví dụ về đoạn mã vi phạm nguyên tắc:
function fetchUser(userId) {
  return fetch(`/api/users/${userId}`).then(res => res.json());
}

function UserComponent({ userId }) {
  const [user, setUser] = useState(null);

  useEffect(() => {
    fetchUser(userId).then(setUser);
  }, [userId]);

  return <div>{user?.name}</div>;
}
Ở đây, UserComponent được gắn chặt với hàm fetchUser.
Đoạn mã sau khi sửa lại:
function UserComponent({ userId, fetchUserData }) {
  const [user, setUser] = useState(null);

  useEffect(() => {
    fetchUserData(userId).then(setUser);
  }, [userId, fetchUserData]);

  return <div>{user?.name}</div>;
}

// Usage
<UserComponent userId={1} fetchUserData={fetchUser} />;
Bằng cách inject fetchUserData vào component, chúng ta có thể dễ dàng hoán đổi việc triển khai để test hoặc các trường hợp sử dụng khác nhau.
2. Ví dụ trong TypeScript (Angular):
Ví dụ về đoạn mã vi phạm nguyên tắc:
@Injectable()
export class UserService {
  constructor(private http: HttpClient) {}

  getUser(userId: string) {
    return this.http.get(`/api/users/${userId}`);
  }
}

@Injectable()
export class UserComponent {
  constructor(private userService: UserService) {}

  loadUser(userId: string) {
    this.userService.getUser(userId).subscribe(user => console.log(user));
  }
}
UserComponent được gắn chặt với UserService, gây khó khăn cho việc hoán đổi UserService.
Đoạn mã sau khi sửa lại:
interface UserService {
  getUser(userId: string): Observable<User>;
}

@Injectable()
export class ApiUserService implements UserService {
  constructor(private http: HttpClient) {}

  getUser(userId: string) {
    return this.http.get<User>(`/api/users/${userId}`);
  }
}

@Injectable()
export class UserComponent {
  constructor(private userService: UserService) {}

  loadUser(userId: string) {
    this.userService.getUser(userId).subscribe(user => console.log(user));
  }
}
Bằng cách phụ thuộc vào một interface (UserService), UserComponent hiện được tách rời khỏi việc triển khai cụ thể của ApiUserService.
Các bước tiếp theo
Cho dù bạn đang làm việc trên front-end với các framework như React hoặc Angular hay back-end với Node.js, các nguyên tắc SOLID đóng vai trò như một hướng dẫn để đảm bảo rằng kiến trúc phần mềm của bạn luôn vững chắc.
Để tích hợp đầy đủ các nguyên tắc này vào dự án của bạn:
Thực hành thường xuyên: Refactor các codebase hiện có để áp dụng các nguyên tắc SOLID và xem xét code để đảm bảo tuân thủ.
Phối hợp với nhóm của bạn: Khuyến khích các phương pháp hay nhất thông qua review code và thảo luận xung quanh clean architecture.
Luôn tò mò: Nguyên tắc SOLID chỉ là khởi đầu. Hãy khám phá các mẫu kiến trúc khác như MVC, MVVM hoặc CQRS được xây dựng dựa trên những nguyên tắc cơ bản này để cải thiện hơn nữa thiết kế của bạn.
Kết luận
Nguyên tắc SOLID rất hiệu quả để đảm bảo rằng code của bạn rõ ràng, dễ bảo trì và có khả năng mở rộng, ngay cả trong các framework JavaScript và TypeScript như React và Angular. Việc áp dụng các nguyên tắc này cho phép các developer viết code linh hoạt và có thể tái sử dụng, dễ dàng mở rộng và refactor khi các yêu cầu phát triển. Bằng cách tuân theo SOLID, bạn có thể làm cho codebase của mình trở nên mạnh mẽ và sẵn sàng cho sự phát triển trong tương lai.",https://viblo.asia/p/nguyen-tac-solid-nen-tang-cho-ung-dung-javascript-va-typescript-ben-vung-aNj4vv1v46r
17,Kết hợp sức mạnh của Three.js và react để tạo ra trải nghiệm 3D tuyệt vời trên web,"Ngày nay, việc tạo ra những trải nghiệm web hấp dẫn ngày càng trở nên quan trọng hơn bao giờ hết. Và đồ họa 3D đóng một vai trò rất lớn trong việc đó. Cho dù bạn đang xây dựng các bản giới thiệu sản phẩm tương tác, các thành phần UI sống động hay thậm chí là trò chơi, việc tích hợp Three.js với React mang đến cho các nhà phát triển một cách thức mượt mà để thêm các yếu tố 3D vào trang web một cách dễ dàng.
Vậy Three.js là gì?
Three.js là một thư viện JavaScript giúp việc kết xuất đồ họa 3D bằng WebGL trở nên dễ dàng hơn. WebGL cực kỳ mạnh mẽ, nhưng việc sử dụng trực tiếp lại rất phức tạp. Three.js đơn giản hóa quy trình đó, cung cấp các công cụ để xây dựng các đối tượng 3D, đèn, máy ảnh và hoạt ảnh với ít nỗ lực hơn.
Tại sao nên sử dụng Three.js với React?
React giúp dễ dàng chia nhỏ giao diện người dùng thành các thành phần có thể tái sử dụng. Nếu kết hợp với Three.js, bạn có thể xây dựng các cảnh 3D của mình theo cách tương tự — từng thành phần một. Điều này giúp cho code của bạn trở nên mô-đun hơn và dễ bảo trì hơn.
May mắn thay, có một thư viện có tên là react-three-fiber, cung cấp cầu nối giữa Three.js và React, cho phép kết xuất trực tiếp các phần tử Three.js trong các thành phần React.
Thiết lập dự án React 3D của bạn dễ dàng
Để bắt đầu với Three.js và React, chúng ta hãy cài đặt các dependency cần thiết. Chúng ta sẽ sử dụng:
three: Thư viện Three.js cốt lõi.
@react-three/fiber: Trình kết xuất React cho Three.js.
@react-three/drei: Thư viện hỗ trợ với các công cụ hữu ích cho các tác vụ Three.js phổ biến.
Bước 1: Cài đặt các Dependency
Mở terminal của bạn và tạo một ứng dụng React mới. Sau đó, cài đặt các thư viện cần thiết:
npx create-react-app threejs-react-demo
cd threejs-react-demo
npm install three @react-three/fiber @react-three/drei
Bước 2: Xây dựng thành phần khối lập phương 3D
Chúng ta sẽ xây dựng một khối lập phương xoay đơn giản sẽ phản ứng với các tương tác của người dùng như nhấp chuột và di chuột. Hãy bắt đầu bằng cách tạo một file có tên Scene.js bên trong thư mục src của bạn và dán code sau:
// Scene.js
import React, { useRef, useState } from 'react';
import { Canvas, useFrame } from '@react-three/fiber';

const RotatingCube = () => {
  const meshRef = useRef();
  const [hovered, setHovered] = useState(false);
  const [clicked, setClicked] = useState(false);

  // Rotate the cube on every frame update
  useFrame(() => {
    meshRef.current.rotation.x += 0.01;
    meshRef.current.rotation.y += 0.01;
  });

  return (
    <mesh
      ref={meshRef}
      scale={clicked ? 1.5 : 1}
      onClick={() => setClicked(!clicked)}
      onPointerOver={() => setHovered(true)}
      onPointerOut={() => setHovered(false)}
    >
      <boxGeometry args={[1, 1, 1]} />
      <meshStandardMaterial color={hovered ? 'hotpink' : 'orange'} />
    </mesh>
  );
};

const Scene = () => {
  return (
    <Canvas style={{ height: '100vh' }}>
      <ambientLight intensity={0.5} />
      <pointLight position={[10, 10, 10]} />
      <RotatingCube />
    </Canvas>
  );
};

export default Scene;
Đoạn mã trên hoạt động thế nào?
Canvas: Thành phần Canvas hoạt động giống như một container cho cảnh 3D của bạn, giống như một phần tử <canvas> trong HTML.
mesh: Một đối tượng 3D chứa hình học (hình dạng) và vật liệu (diện mạo). Ở đây, nó được sử dụng để tạo một khối lập phương.
boxGeometry: Hình học tích hợp sẵn để tạo hình dạng khối lập phương.
meshStandardMaterial: Một vật liệu hỗ trợ ánh sáng cơ bản.
useRef: Một hook để tham chiếu đến mesh để chúng ta có thể xoay nó bằng JavaScript.
useFrame: Một hook từ @react-three/fiber chạy trên mỗi khung hình động, cho phép chúng ta xoay khối lập phương liên tục.
Tương tác: Khối lập phương thay đổi tỷ lệ và màu sắc khi nhấp hoặc di chuột, thêm một hình thức tương tác người dùng đơn giản.
Bước 3: Sử dụng thành phần cảnh trong ứng dụng của bạn
Bây giờ, chúng ta hãy kết xuất cảnh 3D của mình bên trong tệp App.js chính.
// App.js
import React from 'react';
import Scene from './Scene';

function App() {
  return (
    <div style={{ height: '100vh', width: '100vw', margin: 0, padding: 0 }}>
      <Scene />
    </div>
  );
}

export default App;
Làm cho cảnh 3D của bạn trở nên tương tác tốt hơn
Chúng ta đã xây dựng một khối lập phương xoay đơn giản, nhưng hãy cùng khám phá cách chúng ta có thể cải thiện nó. Dưới đây là một số cách để làm cho cảnh của bạn trở nên tương tác hơn:
1. Thêm đổ bóng
Bóng đổ tạo thêm sự chân thực cho các cảnh 3D của bạn. Bạn có thể bật bóng bằng cách thêm các dòng này:
<Canvas shadows>
  <ambientLight intensity={0.5} />
  <spotLight position={[10, 10, 10]} angle={0.3} castShadow />
  <mesh receiveShadow>
    <planeGeometry args={[10, 10]} />
    <meshStandardMaterial color=""gray"" />
  </mesh>
</Canvas>
2. Nhập mô hình 3D
Bạn có thể nhập các mô hình 3D phức tạp hơn (như tệp GLTF/GLB) bằng cách sử dụng hook useGLTF từ @react-three/drei. Dưới đây là một ví dụ nhanh:
import { useGLTF } from '@react-three/drei';

const Model = () => {
  const { scene } = useGLTF('/path-to-your-model/model.gltf');
  return <primitive object={scene} />;
};
Một số mẹo hữu ích về hiệu suất cho các cảnh 3D lớn
Tối ưu hóa các mô hình 3D của bạn bằng cách giảm số lượng đa giác.
Sử dụng ít đèn hơn vì việc tính toán ánh sáng có thể tốn kém.
Bật tính năng loại bỏ Frustum để tránh kết xuất các đối tượng ngoài màn hình.
Tải chậm các thành phần nặng bằng thành phần Suspense của React.
Kết luận
Bạn vừa hoàn thành việc tự tạo một khối lập phương 3D cơ bản bằng cách sử dụng Three.js trong React! Với thiết lập này, bạn có thể bắt đầu thử nghiệm với các cảnh, hoạt ảnh và mô hình phức tạp hơn. Cảm ơn các bạn đã theo dõi.",https://viblo.asia/p/ket-hop-suc-manh-cua-threejs-va-react-de-tao-ra-trai-nghiem-3d-tuyet-voi-tren-web-0gdJz6ejJz5
18,💻 Common Useful Linux Commands 🐧,"Linux FileSystem Structure
/
├── bin/              # Essential user binaries (e.g., ls, cat), accessible by all users.
├── boot/             # Files needed to boot the system, like the kernel and bootloader configs.
├── dev/              # Device files representing hardware (e.g., /dev/sda for hard drives).
├── etc/              # System-wide configuration files (e.g., fstab, hosts).
│   ├── init.d/       # Service scripts (start, stop, restart)
│   ├── nginx/        # Configuration for the nginx web server
│   └── ssh/          # SSH configuration files
├── home/             # User home directories for personal data.
│   └── user/         # Directory for user 'user',
├── lib/              # Shared libraries essential for the system and kernel modules.
├── media/            # Mount points for removable media, external devices like USBs or DVDs.
├── mnt/              # Temporary mount directory
├── opt/              # Optional software packages or third-party apps.
├── proc/             # Process and kernel information
├── root/             # Home directory for root user
├── sbin/             # System binaries for administrative tasks (e.g., reboot, iptables).
├── tmp/              # Temporary files, often cleared on reboot.
├── usr/              # User programs and libraries, typically for installed packages.
│   ├── bin/          # User commands
│   ├── lib/          # User libraries
│   └── share/        # Shared files
└── var/              # Variable files (logs, databases, etc.)
    ├── log/          # Log files
    ├── cache/        # Application cache data
    └── tmp/          # Temporary files created by applications
File and Directory Operations
ls – Lists the contents of a directory.
ls
ls -l        # Long listing format
ls -a        # List all files, including hidden ones
ls – Lists the contents of a directory.
ls
ls
ls -l        # Long listing format
ls -a        # List all files, including hidden ones
cd – Changes the current directory.
cd /path/to/directory
cd ..        # Go up one directory
cd ~         # Go to the home directory
cd – Changes the current directory.
cd
cd /path/to/directory
cd ..        # Go up one directory
cd ~         # Go to the home directory
mkdir – Creates a new directory.
mkdir new_directory
mkdir – Creates a new directory.
mkdir
mkdir new_directory
rmdir – Removes an empty directory.
rmdir directory_name
rmdir – Removes an empty directory.
rmdir
rmdir directory_name
cp – Copies files or directories.
cp source_file destination
cp -r source_directory destination_directory   # Copy directories recursively
cp – Copies files or directories.
cp
cp source_file destination
cp -r source_directory destination_directory   # Copy directories recursively
mv – Moves or renames files and directories.
mv old_name new_name
mv file_name /path/to/destination/
mv – Moves or renames files and directories.
mv
mv old_name new_name
mv file_name /path/to/destination/
rm – Removes files or directories.
rm file_name
rm -r directory_name   # Remove directories recursively
rm – Removes files or directories.
rm
rm file_name
rm -r directory_name   # Remove directories recursively
touch – Creates an empty file or updates the timestamp of an existing file.
touch file_name
touch – Creates an empty file or updates the timestamp of an existing file.
touch
touch file_name
File Viewing & Manipulation
cat – Displays the contents of a file.
cat file_name
cat – Displays the contents of a file.
cat
cat file_name
less – Allows you to view file contents page by page.
less file_name
less – Allows you to view file contents page by page.
less
less file_name
head – Shows the first 10 lines of a file (default).
head file_name
head -n 5 file_name    # Show the first 5 lines
head – Shows the first 10 lines of a file (default).
head
head file_name
head -n 5 file_name    # Show the first 5 lines
tail – Shows the last 10 lines of a file (default).
tail file_name
tail -n 5 file_name    # Show the last 5 lines
tail – Shows the last 10 lines of a file (default).
tail
tail file_name
tail -n 5 file_name    # Show the last 5 lines
grep – Searches for patterns within files.
grep 'search_term' file_name
grep -r 'search_term' /path/to/directory    # Search recursively in directories
grep – Searches for patterns within files.
grep
grep 'search_term' file_name
grep -r 'search_term' /path/to/directory    # Search recursively in directories
Permissions & Ownership
chmod – Changes file permissions.
chmod 755 file_name    # Gives read, write, execute permissions to the owner and read, execute to others
chmod +x script.sh     # Make file executable
chmod – Changes file permissions.
chmod
chmod 755 file_name    # Gives read, write, execute permissions to the owner and read, execute to others
chmod +x script.sh     # Make file executable
chown – Changes the file owner and group.
chown user:group file_name
chown – Changes the file owner and group.
chown
chown user:group file_name
umask – Sets default file creation permissions.
umask 022   # Sets default permissions to 755 for directories and 644 for files
umask – Sets default file creation permissions.
umask
umask 022   # Sets default permissions to 755 for directories and 644 for files
Process Management
ps – Displays the currently running processes.
ps
ps aux      # Show all processes
ps – Displays the currently running processes.
ps
ps
ps aux      # Show all processes
top – Displays real-time system processes and resource usage.
top
top – Displays real-time system processes and resource usage.
top
top
kill – Terminates a process by its PID.
kill process_id
kill -9 process_id    # Forcefully kill a process
kill – Terminates a process by its PID.
kill
kill process_id
kill -9 process_id    # Forcefully kill a process
htop – Interactive process viewer (requires installation).
htop
htop – Interactive process viewer (requires installation).
htop
htop
System Information
df – Shows disk space usage.
df -h      # Human-readable format
df – Shows disk space usage.
df
df -h      # Human-readable format
du – Shows disk usage for files and directories.
du -h /path/to/directory
du – Shows disk usage for files and directories.
du
du -h /path/to/directory
free – Displays memory usage.
free -h    # Human-readable format
free – Displays memory usage.
free
free -h    # Human-readable format
uname – Shows system information.
uname -a   # Display all system info
uname – Shows system information.
uname
uname -a   # Display all system info
uptime – Shows how long the system has been running.
uptime
uptime – Shows how long the system has been running.
uptime
uptime
whoami – Displays the current logged-in user.
whoami
whoami – Displays the current logged-in user.
whoami
whoami
hostname – Displays or sets the system's hostname.
hostname
hostname – Displays or sets the system's hostname.
hostname
hostname
lscpu – Displays CPU architecture information.
lscpu
lscpu – Displays CPU architecture information.
lscpu
lscpu
Network Commands
ping – Tests connectivity to a host.
ping google.com
ping – Tests connectivity to a host.
ping
ping google.com
ifconfig – Displays network interface information (may require net-tools installation on some systems).
ifconfig
ifconfig – Displays network interface information (may require net-tools installation on some systems).
ifconfig
ifconfig
ip – Configures network interfaces and routing.
ip addr show      # Show IP addresses of network interfaces
ip route show     # Show routing table
ip – Configures network interfaces and routing.
ip
ip addr show      # Show IP addresses of network interfaces
ip route show     # Show routing table
curl – Fetches data from a URL.
curl https://example.com
curl – Fetches data from a URL.
curl
curl https://example.com
wget – Downloads files from the web.
wget https://example.com/file.zip
wget – Downloads files from the web.
wget
wget https://example.com/file.zip
Package Management
apt-get (for Debian/Ubuntu-based distributions) – Installs, updates, or removes software packages.
sudo apt-get update           # Update package list
sudo apt-get install package  # Install a package
sudo apt-get remove package   # Remove a package
apt-get (for Debian/Ubuntu-based distributions) – Installs, updates, or removes software packages.
apt-get
sudo apt-get update           # Update package list
sudo apt-get install package  # Install a package
sudo apt-get remove package   # Remove a package
yum (for RedHat/CentOS-based distributions) – Installs, updates, or removes software packages.
sudo yum update               # Update package list
sudo yum install package      # Install a package
sudo yum remove package       # Remove a package
yum (for RedHat/CentOS-based distributions) – Installs, updates, or removes software packages.
yum
sudo yum update               # Update package list
sudo yum install package      # Install a package
sudo yum remove package       # Remove a package
File Compression
tar – Archives or extracts files.
tar -czvf archive_name.tar.gz /path/to/directory   # Create a compressed archive
tar -xzvf archive_name.tar.gz                      # Extract a compressed archive
tar – Archives or extracts files.
tar
tar -czvf archive_name.tar.gz /path/to/directory   # Create a compressed archive
tar -xzvf archive_name.tar.gz                      # Extract a compressed archive
zip – Compresses files into a zip archive.
zip archive_name.zip file1 file2
zip – Compresses files into a zip archive.
zip
zip archive_name.zip file1 file2
unzip – Extracts a zip archive.
unzip archive_name.zip
unzip – Extracts a zip archive.
unzip
unzip archive_name.zip
Miscellaneous
echo – Prints a message or variables to the terminal.
echo ""Hello, World!""
echo – Prints a message or variables to the terminal.
echo
echo ""Hello, World!""
date – Displays or sets the system date and time.
date
date – Displays or sets the system date and time.
date
date
alias – Creates an alias for a command.
alias ll='ls -la'    # Create a shortcut for 'ls -la'
alias – Creates an alias for a command.
alias
alias ll='ls -la'    # Create a shortcut for 'ls -la'
history – Shows the command history.
history
history – Shows the command history.
history
history
clear – Clears the terminal screen.
clear
clear – Clears the terminal screen.
clear
clear
These are just a few of the many powerful commands in Linux, but they cover most of the common operations you'll perform daily.
If you found this helpful, let me know by leaving a 👍 or a comment!, or if you think this post could help someone, feel free to share it! Thank you very much! 😃",https://viblo.asia/p/common-useful-linux-commands-gwd4325bVX9
19,"Bóc tách hàm băm (hashing) trong Java: Khám phá Hashmap, Hashset và phương thức Hashcode()","Băm hay Hashing trong Java là một khái niệm nâng cao cho phép lưu trữ và truy xuất dữ liệu một cách hiệu quả nhất. Cho dù bạn là lập trình viên dày dặn kinh nghiệm hay là người mới bắt đầu, việc hiểu cách thức hoạt động của Hashing sẽ giúp nâng cao kỹ năng lập trình của bạn, cho phép mã của bạn chạy nhanh hơn rất nhiều.
Trong bài viết này, chúng ta sẽ phân tích mọi thứ về Hashing với ba trụ cột: HashMap, HashSet và phương thức hashCode(); các ví dụ mã đơn giản được bao gồm để bạn có thể bắt đầu ngay lập tức!
Hashing là gì?
Nói một cách đơn giản, hashing hay băm là một thủ tục chuyển đổi dữ liệu thành các giá trị số có kích thước cố định, thường được gọi là mã Hashing. Mã Hashing là duy nhất cho dữ liệu và cho phép các cấu trúc dữ liệu dựa trên Hashing tìm thấy dữ liệu nhanh chóng. Mục đích của Hasing là để tìm kiếm, chèn và xóa hiệu quả.
Tại sao nó lại quan trọng?
Tốc độ: Hashing có thể cung cấp khả năng truy xuất dữ liệu gần như ngay lập tức.
Loại bỏ trùng lặp: Hashing cho phép thêm các mục nhập duy nhất vào các cấu trúc dữ liệu như HashSet.
Bây giờ, chúng ta hãy cùng tìm hiểu cách Java sử dụng Hashing thông qua HashMap, HashSet và phương thức hashCode()!
HashMap trong Java: Lưu trữ dữ liệu theo cặp Key - Value
Một HashMap lưu trữ dữ liệu theo các cặp key - value, sử dụng Hashing để nhanh chóng tìm thấy giá trị được liên kết với một khóa cụ thể. Mỗi khóa được chuyển đổi thành một mã Hashing, xác định nơi cặp key - value sẽ được lưu trữ.
Vậy tại sao nên sử dụng HashMap?
Tra cứu nhanh: Bạn có thể tìm hoặc cập nhật giá trị bằng khóa trong thời gian không đổi.
Linh hoạt: Nó cho phép các giá trị null và các giá trị trùng lặp nhưng thực thi các khóa duy nhất.
Ví dụ minh họa về HashMap:
import java.util.HashMap;

public class HashMapExample {
    public static void main(String[] args) {
        HashMap<Integer, String> userMap = new HashMap<>();

        userMap.put(101, ""Alice"");
        userMap.put(102, ""Bob"");
        userMap.put(103, ""Charlie"");

        System.out.println(""User with ID 101: "" + userMap.get(101));

        userMap.remove(102);
        System.out.println(""After removing ID 102: "" + userMap);
    }
}
Trong ví dụ này, chúng ta lưu trữ ID của người dùng làm khóa và tên của họ làm giá trị. HashMap sử dụng mã băm của khóa để định vị giá trị một cách nhanh chóng, giúp việc truy xuất diễn ra nhanh chóng.
HashSet trong Java: Đảm bảo tính duy nhất của dữ liệu
Một HashSet được sử dụng để lưu trữ các phần tử duy nhất. Nó dựa vào phương thức hashCode() để đảm bảo không có hai phần tử nào giống hệt nhau. Nếu bạn cần một bộ sưu tập đảm bảo không có bản sao, HashSet là cấu trúc dữ liệu phù hợp với bạn!
Tại sao nên sử dụng HashSet?
Ngăn chặn trùng lặp: Nó tự động lọc ra các mục nhập trùng lặp.
Các thao tác nhanh chóng: Chèn, xóa và tra cứu nhanh chóng bằng cách sử dụng Hashing.
Ví dụ về HashSet:
import java.util.HashSet;

public class HashSetExample {
    public static void main(String[] args) {
        HashSet<String> userSet = new HashSet<>();

        userSet.add(""Alice"");
        userSet.add(""Bob"");
        userSet.add(""Charlie"");
        userSet.add(""Alice""); 
        System.out.println(""Is Bob in the set? "" + userSet.contains(""Bob""));

        userSet.remove(""Charlie"");

        System.out.println(""Users in the set: "" + userSet);
    }
}
Trong ví dụ này, ""Alice"" được thêm vào hai lần, nhưng HashSet chỉ lưu trữ nó một lần, đảm bảo không có bản sao. Đó chính là sự kỳ diệu của Hashing!
Phương thức hashCode()
Mọi đối tượng trong Java đều có một phương thức hashCode() được kế thừa từ lớp Object. Phương thức hashCode() tạo ra một mã Hashing số đại diện duy nhất cho dữ liệu của đối tượng. Tuy nhiên, nếu bạn đang làm việc với các đối tượng tùy chỉnh trong các bộ sưu tập như HashMap hoặc HashSet, điều cần thiết là ghi đè hashCode() và các phương thức equals().
Tại sao phải ghi đè hashCode()?
Để đảm bảo các đối tượng tùy chỉnh có thể được Hashing và so sánh đúng cách.
Để ngăn chặn các sự cố với việc lưu trữ hoặc tra cứu các đối tượng trong các cấu trúc dựa trên Hashing.
Ví dụ: Đối tượng tùy chỉnh với hashCode() và equals():
import java.util.Objects;

class User {
    private int id;
    private String name;

    public User(int id, String name) {
        this.id = id;
        this.name = name;
    }

    @Override
    public int hashCode() {
        return Objects.hash(id, name);
    }

    @Override
    public boolean equals(Object obj) {
        if (this == obj) return true;
        if (obj == null || getClass() != obj.getClass()) return false;
        User user = (User) obj;
        return id == user.id && name.equals(user.name);
    }

    @Override
    public String toString() {
        return name + "" (ID: "" + id + "")"";
    }
}

public class CustomObjectExample {
    public static void main(String[] args) {
        HashSet<User> users = new HashSet<>();

        users.add(new User(101, ""Alice""));
        users.add(new User(102, ""Bob""));
        users.add(new User(101, ""Alice"")); 

        System.out.println(""Users in the set: "" + users);
    }
}
Trong ví dụ này, chúng tôi đảm bảo rằng hai đối tượng User có cùng ID và tên được coi là bằng nhau, do đó HashSet có thể tránh thêm bản sao.
Cách thức hashCode() và equals() hoạt động cùng nhau
Khi một đối tượng được thêm vào HashMap hoặc HashSet, Java trước tiên sẽ kiểm tra hashCode() để xác định vị trí của đối tượng. Sau đó, phương thức equals() đảm bảo rằng các đối tượng được so sánh đúng cách để xác định sự bằng nhau.
Best pratices tốt nhất:
Luôn ghi đè hashCode() và equals() cùng nhau. Nếu equals() chỉ ra rằng hai đối tượng là bằng nhau, chúng phải có cùng hashCode().
Lời khuyên khi sử dụng Hash trong Java dành cho mọi cấp độ lập trình
Người mới bắt đầu: Hãy bắt đầu bằng cách nắm bắt cách HashMap và HashSet sử dụng mã Hashing để lưu trữ dữ liệu. Tập trung vào ứng dụng thực tế hơn là các chi tiết phức tạp.
Lập trình viên trung cấp: Ghi đè hashCode() và equals() trong các lớp của bạn để quan sát tác động của chúng đối với các bộ sưu tập dựa trên Hashing.
Lập trình viên cao cấp: Khám phá xung đột Hashing và tối ưu hóa hiệu suất trong các ứng dụng quy mô lớn, nơi hiệu quả Hashing chính là chìa khóa.
Cảm ơn các bạn đã theo dõi.",https://viblo.asia/p/boc-tach-ham-bam-hashing-trong-java-kham-pha-hashmap-hashset-va-phuong-thuc-hashcode-y3RL13Xp4ao
20,Tạo hiệu ứng cuộn trang mượt mà khi lăn chuột với thư viện Framer Motion trong React,"Link: https://thuanhmdev.github.io/react-framer-motion-smooth-when-wheel/
Trong thế giới phát triển web hiện đại, trải nghiệm người dùng là yếu tố quan trọng hàng đầu. Một trong những chi tiết nhỏ nhưng có thể tạo nên sự khác biệt lớn chính là hiệu ứng cuộn trang mượt mà. Thay vì cuộn trang theo cách thông thường, chúng ta có thể tạo ra một trải nghiệm cuộn mượt mà, tạo cảm giác cao cấp và chuyên nghiệp hơn cho website.
Trong bài viết này, chúng ta sẽ tìm hiểu cách tạo hiệu ứng cuộn trang mượt mà bằng cách sử dụng thư viện Framer Motion trong React. Framer Motion là một thư viện animation mạnh mẽ cho React, cung cấp các API đơn giản để tạo ra các hiệu ứng phức tạp.
npm install framer-motion
npm install framer-motion
Hãy xem xét code của component SmoothScroll:
import { motion, useScroll, useSpring, useTransform } from ""framer-motion"";
import React, { useLayoutEffect, useRef, useState } from ""react"";

export default function SmoothScroll({
  children,
}: {
  children: React.ReactNode;
}) {
  const { scrollYProgress } = useScroll();
  const smoothProgress = useSpring(scrollYProgress, {
    mass: 0.1, 
    stiffness: 100, 
    damping: 30, 
  });

  const [contentHeight, setContentHeight] = useState(0);
  const contentRef = useRef<HTMLDivElement>(null);

  const y = useTransform(
    smoothProgress,
    [0, 1],
    contentHeight ? [0, -(contentHeight - window.innerHeight)] : [0, 0]
  );

  useLayoutEffect(() => {
    const handleResize = () => {
      if (contentRef.current) {
        setContentHeight(contentRef.current.scrollHeight);
      }
    };

    handleResize();
    window.addEventListener(""resize"", handleResize);

    return () => {
      window.removeEventListener(""resize"", handleResize);
    };
  }, []);

  return (
    <>
      <div style={{ height: contentHeight }} />
      <motion.div
        className=""scrollBody""
        style={{
          y,
          position: ""fixed"",
          top: 0,
          left: 0,
          right: 0,
          overflowY: ""hidden"",
        }}
        ref={contentRef}
      >
        {children}
      </motion.div>
    </>
  );
}
Hãy phân tích từng phần của code:
3.1. Import các hook cần thiết
import { motion, useScroll, useSpring, useTransform } from ""framer-motion"";
import React, { useLayoutEffect, useRef, useState } from ""react"";
3.2 Sử dụng các hook của Framer Motion
const { scrollYProgress } = useScroll();
const smoothProgress = useSpring(scrollYProgress, {
  mass: 0.1,
  stiffness: 100,
  damping: 30,
});
useScroll() theo dõi tiến trình cuộn của trang.
useSpring() tạo ra một animation mượt mà dựa trên physics, với các tham số có thể điều chỉnh:
mass: Điều chỉnh ""quán tính"" của animation. Giá trị càng lớn, animation càng chậm khi bắt đầu và kết thúc.
mass: Điều chỉnh ""quán tính"" của animation. Giá trị càng lớn, animation càng chậm khi bắt đầu và kết thúc.
stiffness: Điều chỉnh ""độ đàn hồi"". Giá trị càng cao, animation càng nhanh và ""bật"" mạnh hơn.
stiffness: Điều chỉnh ""độ đàn hồi"". Giá trị càng cao, animation càng nhanh và ""bật"" mạnh hơn.
damping: Điều chỉnh ""độ mượt"". Kiểm soát mức độ animation sẽ dừng lại mượt mà.
damping: Điều chỉnh ""độ mượt"". Kiểm soát mức độ animation sẽ dừng lại mượt mà.
3.3 Quản lý state và ref
const [contentHeight, setContentHeight] = useState(0);
const contentRef = useRef<HTMLDivElement>(null);
contentHeight lưu trữ chiều cao của nội dung.
contentRef tham chiếu đến phần tử DOM chứa nội dung.
3.4 Tính toán giá trị transform
const y = useTransform(
  smoothProgress,
  [0, 1],
  contentHeight ? [0, -(contentHeight - window.innerHeight)] : [0, 0]
);
useTransform được sử dụng để chuyển đổi giá trị cuộn thành giá trị dịch chuyển Y. Nó kiểm tra contentHeight để tránh các giá trị không hợp lệ khi component mới được mount.
3.5. Xử lý sự kiện resize
useLayoutEffect(() => {
    const handleResize = () => {
      if (contentRef.current) {
        setContentHeight(contentRef.current.scrollHeight);
      }
    };

    handleResize();
    window.addEventListener(""resize"", handleResize);

    return () => {
      window.removeEventListener(""resize"", handleResize);
    };
  }, []);
Đoạn code này đảm bảo rằng contentHeight được cập nhật khi kích thước cửa sổ thay đổi.
3.6 Render component
return (
  <>
    <div style={{ height: contentHeight }} />
    <motion.div
      className=""scrollBody""
      style={{
        y,
        position: ""fixed"",
        top: 0,
        left: 0,
        right: 0,
        overflowY: ""hidden"",
      }}
      ref={contentRef}
    >
      {children}
    </motion.div>
  </>
);
Một div trống với chiều cao bằng contentHeight được tạo ra để giữ cho scrollbar của trình duyệt hoạt động bình thường.</br>
motion.div chứa nội dung thực sự, với style position: fixed và transform y để tạo hiệu ứng cuộn mượt mà.</br>
3.7 Sử dụng SmoothScroll
Để sử dụng component này trong ứng dụng của bạn:
function App() {
  return (
    <SmoothScroll>
      {/* Nội dung của trang web của bạn */}
      <h1>Welcome to My Smooth Scrolling Website</h1>
      <p>This content will scroll smoothly.</p>
      {/* ... */}
    </SmoothScroll>
  );
}
Component SmoothScroll này sử dụng sức mạnh của Framer Motion để tạo ra hiệu ứng cuộn mượt mà, nâng cao trải nghiệm người dùng trên trang web của bạn. Bằng cách điều chỉnh các tham số của useSpring, bạn có thể tùy chỉnh cảm giác của animation để phù hợp với thiết kế của mình.
link source: https://github.com/thuanhmdev/react-framer-motion-smooth-when-wheel",https://viblo.asia/p/tao-hieu-ung-cuon-trang-muot-ma-khi-lan-chuot-voi-thu-vien-framer-motion-trong-react-zXRJ8Dz2JGq
21,Hướng dẫn sử dụng React Router để xây dựng ứng dụng trang đơn (SPA),"Ứng dụng trang đơn (Single Page Applications) đã tạo nên một cuộc cách mạng trong lĩnh vực phát triển web. Chúng mang đến trải nghiệm người dùng linh hoạt và năng động hơn so với các ứng dụng đa trang truyền thống. Các ứng dụng web truyền thống yêu cầu tải lại toàn bộ trang cho hầu hết mọi thao tác nhấp chuột của người dùng. Mặt khác, SPA chỉ tải một trang HTML duy nhất và cập nhật nội dung trang một cách linh hoạt khi người dùng tương tác với ứng dụng. Sự linh hoạt này mang lại cảm giác như đang sử dụng ứng dụng desktop và cho ra kết quả tương tác nhạy bén hơn.
Để theo kịp những gì bài viết này đề cập, bạn nên có kiến thức cơ bản về React và cách thiết lập một dự án React. Nếu bạn đã có đầy đủ kiến thức, hãy cùng bắt đầu nhé.
React Router và React Router DOM là gì?
React Router là một thư viện mạnh mẽ quản lý điều hướng và định tuyến trong các ứng dụng React. React Router DOM được sử dụng dành riêng cho các ứng dụng web và có một số API dành riêng cho DOM.
Khi chúng ta đi sâu vào thế giới của React Router DOM, chúng ta sẽ khám phá các khái niệm cốt lõi của nó đồng thời trình bày việc triển khai chúng trong một ứng dụng React. Trọng tâm của chúng ta sẽ là xây dựng một hệ thống điều hướng đơn giản với các liên kết đến các thành phần khác nhau, minh họa cách định cấu hình các tuyến đường, xử lý khớp tuyến đường và triển khai điều hướng.
Vào cuối bài viết này, bạn sẽ có kiến thức vững chắc về cách sử dụng React Router DOM để tạo trải nghiệm điều hướng liền mạch và linh hoạt trong các ứng dụng trang đơn của mình.
Cách cài đặt React Router
Như đã giải thích ở trên, React-router-DOM được sử dụng riêng để tích hợp chức năng định tuyến vào các ứng dụng web. Vì vậy, để sử dụng nó trong ứng dụng React của bạn, bạn cần cài đặt gói react-router-dom bằng cách chạy lệnh này trong thiết bị đầu cuối ứng dụng React của bạn:
npm install react-router-dom
Sau khi cài đặt thành công, bạn có thể bắt đầu định tuyến trong dự án React của mình.
Các khái niệm cốt lõi trong React Router DOM
1. BrowserRouter
BrowserRouter là một thành phần cha chứa tất cả các thành phần tuyến đường. Tất cả các tuyến đường mà bạn sử dụng trong một ứng dụng phải được khai báo trong BrowserRouter. Quan trọng nhất, nó lưu trữ vị trí hiện tại trong thanh địa chỉ của trình duyệt bằng cách sử dụng URL, điều này rất hữu ích trong quá trình điều hướng.
Để sử dụng BrowserRouter, bạn cần nhập nó từ react-router-dom trong tệp App.jsx của bạn.
import { BrowserRouter } from ""react-router-dom"";

function App() {

  return (
    <BrowserRouter>

    </BrowserRouter>
  );
}

export default App;
BrowserRouter có thuộc tính basename được sử dụng để đặt URL cơ sở cho tất cả các tuyến đường trong một ứng dụng. Điều này rất quan trọng nếu ứng dụng của bạn được lưu trữ trong một thư mục con trên một tên miền.
<BrowserRouter basename=""/shop"">

</BrowserRouter>
Việc thêm /shop làm basename sẽ đảm bảo rằng tất cả các đường dẫn tuyến đường đều tương đối với /shop.
2. Routes
Thành phần này là sự thay thế trực tiếp cho switch được sử dụng trong các phiên bản trước của React Router. Nó cũng hoạt động giống như một thành phần cha và hiển thị routes con phù hợp đầu tiên, đảm bảo rằng thành phần chính xác được hiển thị dựa trên URL hiện tại.
Để khai báo các routes, hãy nhập routes từ react-router-dom và đặt nó trong thành phần BrowserRouter.
import { BrowserRouter, routes } from ""react-router-dom"";

function App() {

  return (
    <BrowserRouter>
        <Routes>

        </Routes>
    </BrowserRouter>
  );
}

export default App;
3. Route
Route là một thành phần con bao gồm hai thuộc tính: path và element. path có thể là bất kỳ tên đường dẫn nào được chỉ định trong khi thuộc tính element là thành phần cần được hiển thị. Một tuyến đường hiển thị một thành phần cụ thể khi đường dẫn được chỉ định khớp với URL.
Một ứng dụng có thể có bao nhiêu route tùy ý và tất cả chúng phải được khai báo bên trong thành phần Routes. Giả sử chúng ta có thành phần <Home> và <Pricing>, chúng ta sẽ phải nhập thành phần Route và đặt nó trong Routes.
import { BrowserRouter, Routes, Route } from ""react-router-dom"";

//ALL COMPONENTS IMPORTS COMES HERE

function App() {

  return (
    <BrowserRouter>
        <Routes>
            <Route path=""/"" element={<Home/>}/>
                <Route path=""pricing"" element={<Pricing/>}/>
        </Routes>
    </BrowserRouter>
  );
}

export default App;
4. Undeclared Routes
Có một cách để xử lý các Route không tồn tại trong ứng dụng của bạn, giống như trang Lỗi 404. Để làm điều này, hãy tạo một thành phần khác mang thông báo Không Tìm Thấy và thêm route.
Đặt tên đường dẫn thành * và chuyển thành phần làm element.
import { BrowserRouter, Routes, Route } from ""react-router-dom"";

//ALL COMPONENTS IMPORTS COMES HERE

function App() {

  return (
    <BrowserRouter>
        <Routes>
            <Route path=""/"" element={<Home/>}/>
                <Route path=""pricing"" element={<Pricing/>}/>
                <Route path=""*"" element={<PageNotFound/>}/>
        </Routes>
    </BrowserRouter>
  );
}

export default App;
5. Nested Routes
Trong một số trường hợp, các routes có thể có tuyến con hoặc sub-routes.
import { BrowserRouter, Routes, Route } from ""react-router-dom"";

//ALL COMPONENTS IMPORTS COMES HERE

function App() {

  return (
    <BrowserRouter>
        <Routes>
            <Route path=""/"" element={<Home/>}/>
                <Route path=""pricing"" element={<Pricing/>}/>
                <Route path=""categories"" element={<Categories/>}>
                    <Route path=""male"" element={<Male/>}/>
                        <Route path=""female"" element={<Female/>}/>
                </Route>
                <Route path=""*"" element={<PageNotFound/>}/>
        </Routes>
    </BrowserRouter>
  );
}

export default App;
Khi điều hướng đến các phần tử lồng nhau, URL trên trình duyệt sẽ hiển thị như /categories/male và /categories/female.
6. Link
Nó hoạt động giống như thuộc tính anchor href. Nó có thuộc tính to chỉ định nơi Link sẽ đưa người dùng đến sau khi nhấp vào. Thông thường, đường dẫn đến trang của thành phần được truyền cho thuộc tính to.
Các liên kết thường được đặt trong thành phần Navbar, vì vậy chúng ta sẽ đặt hai liên kết trỏ đến đường dẫn của thành phần trong Routes đã khai báo của chúng ta.
import { Link } from ""react-router-dom"";
export default function PageNav() {
  return (
  <>
        <Link to=""/"">Home</Link>
        <Link to=""pricing"">Pricing</Link>
  </>
  );
}
Lưu ý: Nếu bạn đang thực hành trong khi đọc bài viết này, điều quan trọng cần lưu ý là thành phần PageNav được tạo ở đây nên được đặt trong tệp App.jsx của bạn và cụ thể là ngay sau thẻ BrowserRouter mở trước Routes. Điều này là để đảm bảo PageNav luôn ở trên cùng như một menu điều hướng mặc dù định tuyến qua các thành phần khác nhau.
import { BrowserRouter, Routes, Route } from ""react-router-dom"";

//ALL COMPONENTS IMPORTS COMES HERE

function App() {

  return (
    <BrowserRouter>
        <PageNav/>
        <Routes>
            <Route path=""/"" element={<Home/>}/>
                <Route path=""pricing"" element={<Pricing/>}/>
                <Route path=""categories"" element={<Categories/>}>
                    <Route path=""male"" element={<Male/>}/>
                        <Route path=""female"" element={<Female/>}/>
                </Route>
                <Route path=""*"" element={<PageNotFound/>}/>
        </Routes>
    </BrowserRouter>
  );
}

export default App;
7. NavLink
NavLink thực hiện chức năng tương tự như Link và cũng có thuộc tính to. Nhưng nó khác ở chỗ nó có thuộc tính class. Các thuộc tính class là active, isPending và isTransitioning. Điều này làm cho nó linh hoạt hơn Link và bạn có thể sử dụng nó để thêm kiểu có điều kiện trong quá trình tương tác của người dùng.
import { NavLink } from ""react-router-dom"";
export default function PageNav() {
  return (
      <>
        <NavLink to=""/"">Home</NavLink>
        <NavLink to=""pricing"">Pricing</NavLink>
    </>
  );
  }
8. Outlet
Việc có các phần tử con bên trong phần tử tuyến đường cha nghĩa là có một lớp trừu tượng trong việc hiển thị UI của tuyến đường con. Đây là lúc thành phần Outlet phát huy tác dụng. Bạn thêm nó vào tuyến đường cha – trong ví dụ của chúng ta, đó sẽ là thành phần Categories.
import { NavLink, Outlet } from ""react-router-dom"";
export default function Categories() {
  return (
 <>
          <NavLink to=""men"">
            Men
          </NavLink>
          <NavLink to=""women"">
            Women
          </NavLink>
      <Outlet />
</>
  );
}
Điều này cho phép hiển thị UI của tuyến đường con trong tuyến đường lồng nhau.
9. Hook useNavigate
Hook này trả về một hàm cho phép điều hướng theo chương trình giữa các tuyến đường.
Có một số cách để sử dụng hàm điều hướng trong ứng dụng của bạn. Đầu tiên, chúng ta cần nhập hook useNavigate và khởi tạo nó là navigate.
import { useNavigate } from ""react-router-dom"";

export default function Homepage() {
  const navigate = useNavigate();

  return (
    <>
      <h1>This is the Homepage</h1>


    </>
  );
}
Chúng ta có thể sử dụng navigate theo các cách sau trong ứng dụng của mình:
Gắn nó vào một nút thông qua thuộc tính onClick với đường dẫn dự định được điều hướng đến, được truyền cho hàm navigate.
<button onClick={() => navigate(""/categories"")}>Go to Categories</button>
Sử dụng nó với thành phần Link.
<Link to={navigate(""/categories"")}>Go to Categories</Link>
Sử dụng một số thay vì đường dẫn thành phần trong hàm navigate. Số này phải chỉ định số lần điều hướng ngược trong ngăn xếp lịch sử mà bạn muốn đến.
<Link to={navigate(-1)}>Go one step backwards</Link>
11. Hook useParams
Trả về một đối tượng của params động nhận được từ URL hiện tại được khớp bởi đường dẫn của Route. Các tuyến đường cha chuyển tất cả params cho các tuyến đường con của chúng.
Ví dụ bên dưới cho thấy thành phần OrderPage sẽ được hiển thị cho mọi customer với id duy nhất của họ. Khi URL khớp với /customer/123, :id sẽ là 123.
import { useParams } from ""react-router-dom"";

function App() {
  const {id} = useParams()
  return (
    <BrowserRouter>
        <Routes>
        <Route path=""customer"">
            <Route path="":id"" element={<OrderPage/>}/>
           </Route>
        </Routes>
    </BrowserRouter>
  );
}

export default App;
Tại thời điểm này, chúng ta đã triển khai đầy đủ React Router trong dự án điều hướng nhỏ của mình.
Để biết thêm thông tin chi tiết và các khái niệm về React Router, bạn có thể truy cập trang tài liệu chính thức của React Router.
Kết luận
Trong bài viết này, chúng ta đã khám phá các khái niệm và triển khai Client Side Routing (CSR) trong ứng dụng web React. React Router, thông qua thư viện React-Router-DOM dành cho web của nó, cho phép CSR, cho phép các ứng dụng cập nhật URL bằng một cú nhấp chuột vào liên kết mà không cần yêu cầu máy chủ cho một tài liệu mới.
Chức năng này nâng cao trải nghiệm người dùng bằng cách cung cấp điều hướng nhanh hơn và tương tác liền mạch hơn trong ứng dụng. Bằng cách tận dụng CSR, các nhà phát triển có thể xây dựng các ứng dụng trang đơn (SPA) hiệu quả và phản hồi nhanh hơn, từ đó cải thiện hiệu suất và sự hài lòng của người dùng.
Cảm ơn các bạn đã theo dõi.",https://viblo.asia/p/huong-dan-su-dung-react-router-de-xay-dung-ung-dung-trang-don-spa-38X4E8mDVN2
22,Cấu hình sử dụng chung chứng chỉ CA,"Để cấu hình sử dụng chung chứng chỉ CA (ca.crt) cho cả Elasticsearch và Logstash trong file values.yaml, bạn cần đảm bảo rằng cả hai dịch vụ đều có đường dẫn đến chứng chỉ này và sao chép nó vào vị trí mà cả hai dịch vụ đều có thể truy cập. Dưới đây là hướng dẫn chi tiết:
Bước 1: Cấu hình values.yaml cho Elasticsearch
Thêm cấu hình cho chứng chỉ trong phần secretMounts để sao chép chứng chỉ CA vào Elasticsearch:
# values.yaml cho Elasticsearch
secretMounts:
  - name: elastic-certificates
    secretName: elastic-certificates  # Tên secret chứa chứng chỉ
    path: /usr/share/elasticsearch/config/certs  # Đường dẫn đến thư mục chứa chứng chỉ
    defaultMode: 0755
Bước 2: Cấu hình values.yaml cho Logstash
Trong phần cấu hình cho Logstash, bạn cũng cần đảm bảo rằng Logstash có thể truy cập chứng chỉ CA. Thêm phần sau vào file values.yaml của Logstash:
# values.yaml cho Logstash
secretMounts:
  - name: elastic-certificates
    secretName: elastic-certificates  # Tên secret chứa chứng chỉ
    path: /usr/share/logstash/config/certs  # Đường dẫn đến thư mục chứa chứng chỉ
    defaultMode: 0755

elasticsearch:
  hosts:
    - ""https://10.152.183.37:9200""
  user: ""elastic""
  password: ""your_password""
  ssl:
    enabled: true
    cacert: ""/usr/share/logstash/config/certs/ca.crt""  # Đường dẫn đến chứng chỉ CA
Bước 3: Tạo Secret chứa Chứng chỉ CA
Trước khi triển khai Elasticsearch và Logstash, bạn cần tạo một Kubernetes secret chứa chứng chỉ CA. Bạn có thể thực hiện điều này bằng lệnh sau:
kubectl create secret generic elastic-certificates --from-file=ca.crt=/path/to/your/ca.crt -n <namespace>
kubectl create secret generic elastic-certificates --from-file=ca.crt=/path/to/your/ca.crt -n <namespace>
Thay thế /path/to/your/ca.crt bằng đường dẫn đến file chứng chỉ trên máy của bạn, và <namespace> bằng namespace mà bạn đang sử dụng.
Bước 4: Triển khai lại Elasticsearch và Logstash
Sau khi đã cấu hình và tạo secret, bạn có thể triển khai lại cả hai dịch vụ:
# Triển khai lại Elasticsearch
helm upgrade <elasticsearch-release-name> elastic/elasticsearch -f values.yaml -n <namespace>

# Triển khai lại Logstash
helm upgrade <logstash-release-name> elastic/logstash -f values.yaml -n <namespace>
Bước 5: Kiểm tra Kết nối
Sau khi triển khai lại, hãy kiểm tra log của Logstash để xác nhận rằng nó có thể kết nối thành công đến Elasticsearch mà không gặp lỗi SSL:
kubectl logs <logstash-pod-name> -n <namespace>
Nếu bạn thấy thông báo kết nối thành công, điều này có nghĩa là chứng chỉ đã được cấu hình đúng và cả Elasticsearch và Logstash có thể sử dụng chung chứng chỉ CA.",https://viblo.asia/p/cau-hinh-su-dung-chung-chung-chi-ca-2oKLnmy1JQO
23,Những công cụ AI giúp tái sử dụng code cực kỳ hiệu quả,"Việc tái sử dụng code là một phần quan trọng trong phát triển phần mềm. Bài viết này sẽ cùng bạn khám phá những lợi ích của việc tái sử dụng code và cách các công cụ AI có thể hỗ trợ bạn thực hiện việc này một cách hiệu quả.
Lợi ích của việc tái sử dụng Code
Tái sử dụng code mang lại nhiều lợi thế, giúp quá trình phát triển phần mềm nhanh chóng và hiệu quả hơn. Việc sử dụng code đã được kiểm chứng giúp bạn tránh lặp lại các tác vụ giống nhau, tuân thủ nguyên tắc ""DRY"" (Don't Repeat Yourself) và tập trung vào những phần quan trọng hơn của dự án. Sau đây là chi tiết các lợi ích mà có thể bạn chưa biết:
1. Tiết kiệm thời gian và rút ngắn chu kỳ phát triển
Một lợi ích quan trọng của việc tái sử dụng code là khả năng tiết kiệm thời gian. Bạn không cần phải dành hàng giờ để viết lại các hàm hoặc tác vụ phổ biến đã có giải pháp. Điều này cho phép bạn nhanh chóng thêm các phần này vào dự án mới và dành nhiều thời gian hơn để xây dựng các tính năng độc đáo cho sản phẩm.
2. Nâng cao chất lượng Code
Code đã được kiểm tra và chứng minh thường sẽ ít mắc lỗi hơn. Sử dụng code có thể tái sử dụng đã được kiểm tra trước đồng nghĩa với việc bạn có thể tin tưởng vào khả năng hoạt động chính xác của nó, giảm thiểu nguy cơ lỗi trong sản phẩm cuối cùng.
3. Tăng tính nhất quán
Việc tái sử dụng code giúp tạo sự nhất quán giữa các dự án khác nhau. Các hàm và logic giống nhau được áp dụng trong mọi dự án, đảm bảo mọi thứ hoạt động theo cách tương tự. Điều này giúp cơ sở code dễ hiểu và bảo trì hơn.
4. Tăng cường sự hợp tác và chia sẻ kiến thức
Khi các nhóm chia sẻ và tái sử dụng các thành phần, việc cộng tác trở nên dễ dàng hơn. Các thành viên trong nhóm có thể làm việc cùng nhau hiệu quả hơn bằng cách sử dụng các đoạn code quen thuộc. Điều này cũng giúp lan tỏa kiến thức trong nhóm, vì mọi người đều được hưởng lợi từ việc hiểu và sử dụng cùng một code.
5. Hiệu quả về chi phí
Việc tái sử dụng các giải pháp hiện có giúp giảm chi phí. Vì bạn dành ít thời gian hơn để tạo code mới từ đầu, quá trình phát triển trở nên rẻ hơn. Điều này cũng có thể dẫn đến việc hoàn thành dự án nhanh hơn, từ đó tiết kiệm tiền về lâu dài.
Các lựa chọn phổ biến để tái sử dụng Code
Có một số công cụ và nền tảng mà nhà phát triển dựa vào để tìm code có thể tái sử dụng. Có những lựa chọn được sử dụng rộng rãi như Google, StackOverflow và tài liệu hướng dẫn. Mặt khác, có những công cụ AI mới hơn như ChatGPT, Gemini và Codiumate.
1. Công cụ Search của Google
Google là một trong những công cụ được sử dụng rộng rãi nhất để tìm kiếm các đoạn code và hướng dẫn. Các nhà phát triển thường tìm kiếm các giải pháp cụ thể, đọc qua các bài đăng trên blog, diễn đàn và tài liệu chính thức để tìm thấy những gì họ cần.
Ưu điểm:
Truy cập vào một lượng lớn thông tin, hướng dẫn và tài nguyên trên web.
Tuyệt vời để tìm kiếm các giải pháp thích hợp hoặc các ví dụ cụ thể.
Hữu ích cho việc khám phá tài liệu và tìm hiểu các công nghệ mới.
Nhược điểm:
Việc tìm kiếm giải pháp phù hợp thường liên quan đến việc sàng lọc rất nhiều nội dung.
Không phải tất cả các tài nguyên đều đáng tin cậy hoặc cập nhật.
Có thể mất nhiều thời gian hơn để tìm kiếm code cụ thể, có thể tái sử dụng so với các nền tảng khác.
Cách sử dụng:
Bạn có thể đưa ra một tìm kiếm nhanh về lập trình, các đoạn code trên Google và nó sẽ hiển thị kết quả bao gồm các bài báo và ấn phẩm liên quan đến câu hỏi của bạn. Việc xem qua các bài viết này có thể giúp bạn tìm được chính xác đoạn code bạn đang tìm kiếm.
2. Stack Overflow
Stack Overflow là một diễn đàn trực tuyến phổ biến, nơi các nhà phát triển đặt câu hỏi và chia sẻ giải pháp cho các vấn đề lập trình, và nó là một người bạn đáng tin cậy của các nhà phát triển trong nhiều năm qua.
Nó có một thư viện khổng lồ gồm các câu trả lời và đoạn code do cộng đồng nhà phát triển cung cấp. Hiện tại, nó cũng có tính năng AI mà chúng ta sẽ thảo luận trong phần tiếp theo. Bạn có thể truy cập các giải pháp khác nhau cho cùng một vấn đề do các nhà phát triển cung cấp.
Ưu điểm:
Bộ sưu tập lớn các ví dụ và giải pháp code thực tế.
Các câu trả lời thường được các người dùng khác xem xét và đánh giá, giúp bạn dễ dàng tìm thấy code chất lượng cao.
Bao gồm một loạt các chủ đề và vấn đề về lập trình.
Nhược điểm:
Yêu cầu tìm kiếm thủ công, có thể mất nhiều thời gian.
Một số giải pháp có thể đã lỗi thời hoặc không phù hợp với thực tiễn phát triển hiện đại.
Không phải lúc nào cũng dễ dàng tìm thấy code phù hợp hoàn hảo với nhu cầu dự án cụ thể của bạn.
Cách sử dụng:
Bạn có thể mở trang web trong trình duyệt của mình và tìm kiếm bất kỳ câu hỏi nào bạn cần và câu trả lời được đánh giá cao nhất sẽ được cung cấp. Nhờ đó mà bạn có thể hoàn thiện công việc code một cách dễ dàng.
Bên cạnh đó, Stack Overflow còn có công cụ OverFlow AI vô cùng hữu ích mà bạn có thể tham khảo sử dụng. Tuy nhiên công cụ này hiện chỉ khả dụng cho Teams Enterprise, các doanh nghiệp sẽ mua gói này để dùng cho nhân viên của họ.
3. Các công cụ AI hữu ích
ChatGPT và Gemini là các công cụ AI hỗ trợ các nhà phát triển bằng cách cung cấp các đoạn code và trả lời các câu hỏi về lập trình. Chúng được biết đến với tính linh hoạt, hỗ trợ nhiều ngôn ngữ lập trình và đưa ra các đề xuất nhanh chóng về cách giải quyết các vấn đề về lập trình.
Các công cụ này đang phát triển mỗi ngày và thực sự đã cách mạng hóa lĩnh vực công nghệ. Chúng có thể cung cấp các đoạn code một cách nhanh chóng và kèm theo nguồn.
Ưu điểm:
Có thể tạo code cho nhiều ngôn ngữ và framework khác nhau.
Cung cấp các giải thích và ví dụ nhanh chóng cũng như nguồn gốc.
Dễ sử dụng cho cả người mới bắt đầu và nhà phát triển có kinh nghiệm.
Nhược điểm:
Các công cụ AI này mang tính chất đa năng, vì vậy chúng có thể không phải lúc nào cũng cung cấp code được tối ưu hóa hoặc chính xác nhất cho các tác vụ phức tạp và đôi khi gặp khó khăn với ngữ cảnh.
Vì chúng không chỉ tập trung vào lập trình, nên chúng có thể bỏ sót các chi tiết quan trọng hoặc đề xuất các giải pháp lỗi thời.
Cách sử dụng:
Lấy ví dụ về công cụ ChatGPT: Bạn đặt câu hỏi với đầy đủ dữ kiện và công cụ sẽ đưa ra câu trả lời mà bạn cần.
Ví dụ bạn đưa ra câu hỏi: ""Bạn có thể tạo một hàm để xác thực đầu vào của biểu mẫu trong JavaScript không?""
Câu trả lời ChatGPT đưa ra: Dưới đây là một hàm JavaScript có thể tái sử dụng để xác thực đầu vào của biểu mẫu. Hàm này sẽ xác thực xem đầu vào có trống hay không và tùy chọn kiểm tra các điều kiện như địa chỉ email hợp lệ, độ dài ký tự tối thiểu và xác thực số. Bạn có thể mở rộng nó tùy thuộc vào yêu cầu của biểu mẫu.
function validateForm(inputs) {
  let errors = {};

  inputs.forEach(input => {
    const { name, value, type, required, minLength, pattern } = input;

    // Check if the input is required and empty
    if (required && !value) {
      errors[name] = `${name} is required`;
      return;
    }

    // Check for minimum length
    if (minLength && value.length < minLength) {
      errors[name] = `${name} must be at least ${minLength} characters`;
      return;
    }

    // Check for valid email format
    if (type === 'email' && !/\S+@\S+\.\S+/.test(value)) {
      errors[name] = 'Invalid email address';
      return;
    }

    // Check if the input matches the specified pattern (e.g., phone number)
    if (pattern && !new RegExp(pattern).test(value)) {
      errors[name] = `${name} is invalid`;
      return;
    }
  });

  return Object.keys(errors).length ? errors : true;
}

// Example usage:
const inputs = [
  { name: 'username', value: 'john_doe', required: true, minLength: 5 },
  { name: 'email', value: 'john@example', type: 'email', required: true },
  { name: 'password', value: '1234', required: true, minLength: 6 },
];

const validation = validateForm(inputs);
console.log(validation);
Câu trả lời này sẽ giúp bạn không phải lo lắng về logic và vẫn có chỗ để tùy chỉnh. Chỉ cần đảm bảo kiểm tra code trước khi dùng để đảm bảo nó hoạt động cho mục đích của bạn.
4. Codiumate
Codiumate là một trợ lý lập trình mã nguồn mở dựa trên AI được thiết kế đặc biệt để nâng cao quy trình phát triển phần mềm. Nó được đào tạo chỉ dành riêng cho mục đích hỗ trợ các nhà phát triển bằng cách cung cấp code chất lượng cao và có thể tái sử dụng, các bài kiểm tra lặp đi lặp lại, đánh giá PR và hoàn thành code.
Nó cũng có tính năng trò chuyện cho phép bạn đặt câu hỏi. Nhưng theo tôi, tính năng ấn tượng nhất mà nó có là tùy chọn nhập toàn bộ cơ sở code của bạn hoặc các phần của nó để nó có ngữ cảnh cho code bạn cần từ nó.
Khi nói đến việc tái sử dụng code hiệu quả, việc có một công cụ được thiết kế riêng cho mục đích đó sẽ tạo ra sự khác biệt. Và thật tuyệt khi Codiumate cũng là một công cụ mã nguồn mở.
Ưu điểm:
Codiumate là chuyên dụng. Mặc dù các công cụ khác như ChatGPT rất hữu ích cho nhiều tác vụ, nhưng trọng tâm của Codiumate cho phép nó cung cấp các đề xuất chính xác và phù hợp hơn để tái sử dụng code.
Nó có thể quét nhanh các cơ sở code lớn, xác định các cơ hội để tái sử dụng code và cung cấp các tùy chọn tích hợp liền mạch.
Nó sở hữu các tính năng như tạo thử nghiệm tự động và phát hiện lỗi thời gian thực.
Nó có tính năng hoàn thành code thông minh, trợ lý lập trình và tác nhân PR cho các bài đánh giá Yêu cầu Kéo (tiện ích mở rộng của Chrome) khiến nó trở thành một giải pháp duy nhất.
Hạn chế:
Là một công cụ mã nguồn mở, Codiumate có thể yêu cầu một số thiết lập hoặc tùy chỉnh ban đầu để phù hợp với môi trường phát triển cụ thể của bạn. Điều này có thể mất thêm một chút thời gian ban đầu, đặc biệt là đối với các nhà phát triển không quen thuộc với công cụ này. Nhưng một khi nó được tích hợp, việc tiết kiệm thời gian và cải thiện chất lượng code khiến nó xứng đáng với nỗ lực bỏ ra. Bạn có thể tìm thấy liên kết để thiết lập tại đây.
Vì Codiumate được thiết kế đặc biệt để tái sử dụng code, nên nó có thể không phải là công cụ tốt nhất để xử lý các truy vấn rộng hơn bên ngoài lĩnh vực lập trình. Nếu bạn đang tìm kiếm lời khuyên chung chung hơn hoặc trợ giúp cho các tác vụ không liên quan đến code, bạn có thể vẫn cần sử dụng các công cụ khác như ChatGPT. Nhưng để tái sử dụng code tập trung và hiệu quả, Codiumate là một lựa chọn tuyệt vời.
Cách sử dụng:
Sau khi thiết lập Codiumate (mất khoảng 2 phút), bạn có thể truy cập tính năng trò chuyện, nơi bạn có thể thêm toàn bộ cơ sở code hoặc đánh dấu một phần của nó để làm ngữ cảnh nhằm nhận được kết quả cụ thể hơn.
Kết luận
Tái sử dụng code là một phương pháp hữu ích giúp quá trình phát triển trở nên nhanh chóng và dễ dàng hơn. Nó cho phép các nhà phát triển tập trung vào việc tạo các tính năng mới trong khi vẫn giữ cho các dự án của họ nhất quán. Tái sử dụng code cũng giúp các nhóm làm việc cùng nhau tốt hơn và chia sẻ kiến thức dễ dàng hơn.
Có rất nhiều nền tảng AI có sẵn như để giúp tái sử dụng code và mỗi nền tảng đều có những lợi ích riêng (và tôi đã không đề cập đến tất cả chúng ở đây.
Bạn nên tận dụng các công cụ AI này và đưa ra lựa chọn dựa trên công cụ nào phù hợp với bạn hơn. Codiumate và OverflowAI nổi bật hơn so với phần còn lại đối với tôi, nhưng công cụ phù hợp sẽ phụ thuộc vào nhu cầu của bạn vào thời điểm đó.
Cuối cùng, các công cụ AI tồn tại để giúp quá trình phát triển trở nên tinh gọn, rẻ hơn và cuối cùng là giúp cuộc sống của bạn dễ dàng hơn với tư cách là một lập trình viên.
Cảm ơn các bạn đã theo dõi.",https://viblo.asia/p/nhung-cong-cu-ai-giup-tai-su-dung-code-cuc-ky-hieu-qua-qPoL7ZpeJvk
24,Những cách tốt nhất để giúp các đoạn code JavaScript trở nên gọn gàng hơn,"Viết code JavaScript sao cho trở nên gọn gàng là điều tối quan trọng để giúp bạn có thể duy trì cơ sở các đoạn code có chất lượng cao, dễ đọc, dễ hiểu và dễ dàng bảo trì. Các đoạn code sạch không chỉ cải thiện sự hợp tác giữa các thành viên trong nhóm mà còn làm giảm khả năng xuất hiện lỗi. Trong bài viết này, chúng ta sẽ khám phá một số cách thức quan trọng để viết mã JavaScript trở nên gọn gàng hơn, nhờ đógiúp bạn tạo ra các ứng dụng hiệu quả, có khả năng mở rộng và dễ dàng bảo trì.
1. Sử dụng Comments một cách hợp lý
Mặc dù các comments có thể hữu ích trong việc giải thích logic phức tạp, nhưng việc lạm dụng chúng có thể làm lộn xộn các đoạn mã của bạn. Thay vào đó, bạn hãy cố gắng viết ra các đoạn mã có thể tự giải thích bằng cách sử dụng tên rõ ràng và mô tả cho các biến, hàm và mô-đun. Chỉ thêm comments khi thực sự cần thiết, chẳng hạn như để làm rõ logic không rõ ràng hoặc đưa ra ngữ cảnh lý do tại sao một số quyết định nhất định được đưa ra.
Ví dụ: Thay vì viết // biến để lưu trữ số lượng người dùng, chỉ cần đặt tên biến là userCount và tránh thêm comments vào là được.
Lợi ích khi làm điều này:
Làm cho mã sạch hơn bằng cách giảm sự lộn xộn không cần thiết.
Khuyến khích viết mã trực quan và dễ đọc hơn.
Nhận xét tập trung vào việc cung cấp thông tin thiết yếu hơn là giải thích logic đơn giản.
2. Viết các bài Unit tests
Unit test hay kiểm thử đơn vị là một phần không thể thiếu trong việc tối ưu hóa các đoạn mã sạch, vì chúng đảm bảo rằng các hàm của bạn hoạt động như mong đợi. Các khung kiểm tra như Jest hoặc Mocha cho phép bạn viết các bài unit test tự động để xác minh tính chính xác của mã. Thường xuyên kiểm tra không chỉ giúp phát hiện lỗi sớm mà còn xây dựng sự tự tin rằng những thay đổi mới sẽ không phá vỡ các tính năng hiện có.
Ví dụ: Bạn có thể viết một bài unit test để xác minh rằng hàm addNumbers (a, b) trả về tổng chính xác của hai số.
Lợi ích khi sử dụng unit test:
Cải thiện độ tin cậy của mã bằng cách xác định lỗi sớm.
Đảm bảo rằng các bản cập nhật mới không giới thiệu hồi quy.
Cung cấp một mạng lưới an toàn để tái cấu trúc.
3. Viết ra các đoạn code module có thể tái sử dụng
Các đoạn mã mô-đun là cách thực hành chia ứng dụng của bạn thành các phần nhỏ hơn, có thể quản lý được và độc lập. Bằng cách viết các hàm và mô-đun thực hiện một tác vụ cụ thể, bạn có thể dễ dàng kiểm tra, sử dụng lại và bảo trì mã của mình dễ dàng hơn. Trong JavaScript, điều này thường có nghĩa là bạn phải sử dụng các mô-đun ES6 hoặc CommonJS để cấu trúc lại các đoạn mã của bạn một cách hợp lý.
Ví dụ: Thay vì sao chép mã để xác thực biểu mẫu trên nhiều trang, hãy tạo một hàm validateForm có thể sử dụng lại mà có thể được nhập và sử dụng ở bất kỳ đâu cần thiết.
Lợi ích khi sử dụng các đoạn mã mô-đun có thể tái sử dụng:
Thúc đẩy việc tái sử dụng mã và giảm trùng lặp.
Cải thiện khả năng bảo trì vì những thay đổi trong một mô-đun không ảnh hưởng đến các mô-đun khác.
Giúp kiểm tra đơn vị dễ dàng hơn, vì các mô-đun nhỏ hơn sẽ dễ kiểm tra hơn.
4. Tuân thủ các quy ước nhất quán trong quá trình viết code
Sự nhất quán trong phong cách mã hóa làm cho cơ sở các đoạn code của bạn trở nên đồng nhất và có thể dự đoán được, điều này rất quan trọng khi nhiều lập trình viên đang làm việc trên cùng một dự án. Thiết lập các quy ước mã hóa cho những thứ như thụt lề, khoảng cách và định dạng và tuân thủ chúng trong suốt dự án. Các công cụ như ESLint có thể tự động thực thi các quy tắc này, nhờ đó giúp bạn ngăn ngừa các lỗi thường gặp.
Ví dụ: Sử dụng camelCase cho tên biến, luôn mở dấu ngoặc nhọn trên cùng một dòng và duy trì thụt lề nhất quán (ví dụ: 2 hoặc 4 khoảng trắng) là những cách thực hành tốt cần tuân theo.
Lợi ích khi bạn tuân thủ các quy ước trong quá trình viết code:
Cải thiện sự hợp tác nhóm bằng cách đảm bảo tính đồng nhất.
Giúp mã dễ đọc và điều hướng hơn.
Giảm tải nhận thức bằng cách tuân theo các mẫu quen thuộc.
5. Sử dụng tên biến và tên hàm có ý nghĩa
Sử dụng tên có ý nghĩa và mô tả cho các biến và hàm là nền tảng cho việc sản xuất ra các đoạn mã JavaScript gọn gàng. Khi bạn chọn những cái tên rõ ràng và cụ thể, bạn sẽ giúp cho mã dễ hiểu hơn cho người khác và cho chính bạn khi bạn xem lại nó. Tên mô tả ngăn ngừa nhầm lẫn và giảm nhu cầu sử dụng comments không cần thiết.
Ví dụ: Thay vì đặt tên biến là x, hãy sử dụng userCount nếu nó chứa số lượng người dùng. Đối với một hàm tính tổng giá, calculateTotalPrice cung cấp thông tin nhiều hơn nhiều so với func1.
Lợi ích khi bạn đặt tên biến và hàm có ý nghĩa:
Cải thiện khả năng đọc mã.
Giảm sự phụ thuộc vào các nhận xét.
Giúp việc gỡ lỗi và bảo trì dễ dàng hơn.
Kết luận
Việc viết ra các đoạn mã JavaScript gọn gàng sẽ khiến ứng dụng của bạn trở nên rõ ràng, tăng cường khả năng bảo trì và đảm bảo dự án hoạt động lâu dài hơn. Bằng cách sử dụng tên có ý nghĩa, tuân theo các quy ước nhất quán, viết mã mô-đun, bao gồm các bài kiểm thử đơn vị và sử dụng comments một cách hợp lý, bạn có thể cải thiện đáng kể chất lượng các đoạn mã của mình. Những chiến lược này không chỉ giúp mã của bạn dễ hiểu và bảo trì hơn mà còn giúp việc cộng tác với các lập trình viên khác trong cùng một dự án trở nên hiệu quả hơn.",https://viblo.asia/p/nhung-cach-tot-nhat-de-giup-cac-doan-code-javascript-tro-nen-gon-gang-hon-BQyJKqr7VMe
25,Logstash cho bộ lọc dựa trên dữ liệu log request và response,"Dưới đây là cấu hình Logstash cho bộ lọc dựa trên dữ liệu log request và response mà bạn đã cung cấp. Cấu hình này sẽ sử dụng plugin grok để phân tích cú pháp các log và plugin mutate để xử lý các trường cần thiết.
Cấu Hình Logstash Cho request và response
input {
  kafka {
    bootstrap_servers => ""10.100.30.32:9092""
    topics => [""ESMART-CATEGORY-LOGS"", ""ESMART-GATEWAY-LOGS""]
    group_id => ""log_consumer_group""
    auto_offset_reset => ""earliest""
  }
}

filter {
  # Sử dụng mutate để lọc đi ký tự không mong muốn
  mutate {
    gsub => [""message"", ""\u001b|\n"", """"]
  }

  # Phân tích log response
  if ""RESPONSE"" in [message] {
    grok {
      match => {
        ""message"" => """"""
          {""instant"":{""epochSecond"":%{NUMBER:epoch_sec},"" +
          ""\""nanoOfSecond\"":%{NUMBER:nano_sec},"" +
          ""\""thread\"":\""%{DATA:thread}\"","" +
          ""\""level\"":\""%{WORD:level}\"","" +
          ""\""loggerName\"":\""%{DATA:logger_name}\"","" +
          ""\""message\"":\""%{GREEDYDATA:log_message}\"","" +
          ""\""endOfBatch\"":%{GREEDYDATA:end_of_batch},"" +
          ""\""loggerFqcn\"":\""%{DATA:logger_fqcn}\"","" +
          ""\""contextMap\"":{\""traceId\"":\""%{DATA:trace_id}\"","" +
          ""\""spanId\"":\""%{DATA:span_id}\"","" +
          ""\""className\"":\""%{DATA:class_name}\"","" +
          ""\""clientIp\"":\""%{IP:client_ip}\"","" +
          ""\""clientMessageId\"":\""%{DATA:client_message_id}\"","" +
          ""\""clientTime\"":\""%{TIMESTAMP_ISO8601:client_time}\"","" +
          ""\""duration\"":%{NUMBER:duration},"" +
          ""\""methodName\"":\""%{DATA:method_name}\"","" +
          ""\""path\"":\""%{DATA:path}\"","" +
          ""\""stackTrace\"":\""%{GREEDYDATA:stack_trace}\""},"" +
          ""\""threadId\"":%{NUMBER:thread_id},"" +
          ""\""threadPriority\"":%{NUMBER:thread_priority},"" +
          ""\""logType\"":\""%{WORD:log_type}\"","" +
          ""\""application\"":\""%{DATA:application}\"","" +
          ""\""localIp\"":\""%{IP:local_ip}\""}
        """"""
      }
    }
  }

  # Phân tích log request
  if ""REQUEST"" in [message] {
    grok {
      match => {
        ""message"" => """"""
          {""instant"":{""epochSecond"":%{NUMBER:epoch_sec},"" +
          ""\""nanoOfSecond\"":%{NUMBER:nano_sec},"" +
          ""\""thread\"":\""%{DATA:thread}\"","" +
          ""\""level\"":\""%{WORD:level}\"","" +
          ""\""loggerName\"":\""%{DATA:logger_name}\"","" +
          ""\""message\"":\""%{GREEDYDATA:log_message}\"","" +
          ""\""endOfBatch\"":%{GREEDYDATA:end_of_batch},"" +
          ""\""loggerFqcn\"":\""%{DATA:logger_fqcn}\"","" +
          ""\""contextMap\"":{\""traceId\"":\""%{DATA:trace_id}\"","" +
          ""\""spanId\"":\""%{DATA:span_id}\"","" +
          ""\""className\"":\""%{DATA:class_name}\"","" +
          ""\""clientIp\"":\""%{IP:client_ip}\"","" +
          ""\""clientMessageId\"":\""%{DATA:client_message_id}\"","" +
          ""\""clientTime\"":\""%{TIMESTAMP_ISO8601:client_time}\"","" +
          ""\""duration\"":%{NUMBER:duration},"" +
          ""\""methodName\"":\""%{DATA:method_name}\"","" +
          ""\""path\"":\""%{DATA:path}\"","" +
          ""\""stackTrace\"":\""%{GREEDYDATA:stack_trace}\""},"" +
          ""\""threadId\"":%{NUMBER:thread_id},"" +
          ""\""threadPriority\"":%{NUMBER:thread_priority},"" +
          ""\""logType\"":\""%{WORD:log_type}\"","" +
          ""\""application\"":\""%{DATA:application}\"","" +
          ""\""localIp\"":\""%{IP:local_ip}\""}
        """"""
      }
    }
  }

  # Tạo các trường mới
  mutate {
    add_field => {
      ""ts"" => ""%{epoch_sec}""
      ""ip"" => ""%{client_ip}""
      ""msgId"" => ""%{client_message_id}""
      ""duration"" => ""%{duration}""
      ""method"" => ""%{method_name}""
      ""path"" => ""%{path}""
      ""app"" => ""%{application}""
      ""localIp"" => ""%{local_ip}""
      ""logType"" => ""%{log_type}""
      ""type"" => ""%{+log_type}""  # Đánh dấu loại log (REQUEST/RESPONSE)
    }
    remove_field => [
      ""instant"", ""thread"", ""level"", ""logger_name"", ""message"", 
      ""endOfBatch"", ""logger_fqcn"", ""contextMap"", 
      ""threadId"", ""threadPriority""
    ]
  }

  # Chuyển đổi timestamp
  date {
    match => [ ""client_time"", ""ISO8601"" ]
    timezone => ""+07:00""
    target => ""@timestamp""
  }
  
  ruby {
    code => ""event.set('indexDay', event.get('@timestamp').time.localtime('+07:00').strftime('%Y%m%d'))""
  }
}

output {
  elasticsearch {
    hosts => [""http://10.152.183.57:9200""]
    template => ""/usr/share/logstash/templates/logstash_template.json""
    template_name => ""logstash""
    template_overwrite => true
    index => ""logstash-%{indexDay}""
    document_type => ""_doc""
  }
  stdout {
    codec => rubydebug
  }
}",https://viblo.asia/p/logstash-cho-bo-loc-dua-tren-du-lieu-log-request-va-response-GyZJZyBQ4jm
26,Tích hợp Function Calling vào hệ thống Chatbot.,"Ngày nay, các hệ thống chatbot đang ngày càng phổ biến với mục tiêu giải quyết nhiều bài toán thực tế như chăm sóc khách hàng tự động, hỗ trợ bán hàng, và nhiều lĩnh vực khác. Điểm mạnh của chatbot là khả năng tạo ra các cuộc hội thoại tự nhiên, thay thế cách tương tác thông qua các lệnh cứng nhắc. Điều này giúp người dùng dễ dàng trao đổi thông tin với hệ thống hơn, và từ đó cải thiện trải nghiệm tổng thể.
Một hệ thống chatbot không chỉ đơn thuần là tương tác với người dùng, mà còn phải xử lý nhiều tầng logic phức tạp, từ thuật toán đến việc kết nối với cơ sở dữ liệu. Tuy nhiên, các ngôn ngữ lập trình là ngôn ngữ có cấu trúc, yêu cầu tuân thủ nghiêm ngặt các quy tắc về cú pháp. Điều này tạo ra thách thức lớn khi phải chuyển đổi những câu hỏi hay yêu cầu tự nhiên từ người dùng thành các lệnh mà hệ thống có thể hiểu và xử lý.
Ví dụ, khi khách hàng nói “Tôi muốn mua món đồ A” hoặc “Bán cho anh món đồ A này nhé”, hệ thống cần phải hiểu được và chuyển những yêu cầu này thành lệnh như buy(item_name='A'). Tuy nhiên, sự đa dạng trong cách người dùng diễn đạt khiến việc chuẩn hóa và hiểu đúng trở nên khó khăn.
buy(item_name='A')
Hiện nay, các Mô hình Ngôn ngữ Lớn (LLM) đã tích hợp thêm một tính năng quan trọng: Function Calling. Đây là một đột phá giúp các mô hình không chỉ phản hồi dựa trên dữ liệu đã được huấn luyện mà còn có khả năng tương tác với các hàm và API bên ngoài để thực hiện các tác vụ chuyên sâu hoặc truy xuất thông tin từ các nguồn khác.
Với tính năng Function Calling, thay vì chỉ xử lý ngôn ngữ tự nhiên, mô hình có thể tự động gọi các hàm cụ thể và trích xuất chính xác tham số từ yêu cầu của người dùng. Điều này mở rộng đáng kể phạm vi ứng dụng của LLM, cho phép hệ thống thực hiện các tác vụ như tìm kiếm thông tin theo thời gian thực, tính toán, và thậm chí thực hiện các hành động theo yêu cầu.
Một số mô hình ngôn ngữ tiên tiến đã tích hợp tính năng này bao gồm:
OpenAI: GPT-3.5 Turbo, GPT-4, …
Google AI: Gemini 1.0, 1.5 Pro, 1.5 Flash, …
4.1.	Tạo các hàm để giao tiếp với hệ thống
Giả sử bạn đang phát triển một hệ thống chatbot phục vụ cho việc bán hàng online, các trường hợp sử dụng (use cases) cơ bản của hệ thống này bao gồm ""Tạo đơn hàng"" và ""Hủy đơn hàng"". Để xử lý hai yêu cầu này, ta cần tạo hai hàm:
# Hàm tạo đơn hàng
def create_orders(product_name: str, quantity: int, address: str, phone_number: str):
    ''' Các tham số nhận vào:
        -   product_name: Tên sản phẩm người dùng cần mua.
        -   quantity: Số lượng sản phẩm
        -   address: địa chỉ giao đến
        -   phone_number: Số điện thoại người dùng
    '''
    # Xử lý logic ở đây

# Hàm hủy đơn hàng
def cancel_orders(order_code: str):
    ''' Các tham số nhận vào:
        -   orders_code: mã của đơn hàng
    '''
    # Xử lý logic ở đây
4.2.	 Mô tả các hàm
Để các Mô hình Ngôn ngữ Lớn (LLM) có thể hiểu và gọi đúng các hàm trong hệ thống, ta cần mô tả các hàm và các tham số của chúng. Đây là bước quan trọng giúp LLM biết được chính xác hành động cần thực hiện khi nhận yêu cầu từ người dùng.
Hàm create_orders:
Đây là hàm dùng để tạo đơn hàng mới khi người dùng gửi yêu cầu mua sản phẩm. Khi hàm này được gọi, nó nhận vào các thông tin cần thiết để tạo một đơn hàng bao gồm:
create_orders
product_name: Tên của sản phẩm mà người dùng muốn mua.
quantity: Số lượng sản phẩm người dùng muốn đặt.
address: Địa chỉ giao hàng của người dùng, nơi sản phẩm sẽ được chuyển đến.
phone_number: Số điện thoại của người dùng để liên hệ trong quá trình giao hàng.
Sau khi nhận được các tham số này, hệ thống sẽ xử lý logic để tạo đơn hàng dựa trên yêu cầu của người dùng.
Ví dụ về mô tả hàm create_orders cho việc Tạo đơn hàng:
create_orders
create_orders_declaration = genai.protos.FunctionDeclaration(
    name='create_orders',
    description='This is the function used to create a new order when a user requests an order',
    parameters= genai.protos.Schema(
        type=genai.protos.Type.OBJECT,
        properties={
            'product_name': genai.protos.Schema(
                type=genai.protos.Type.STRING,
                description='Name of the product',
            ),
            'quantity': genai.protos.Schema(
                type=genai.protos.Type.INTEGER,
                description='Number of products the user wants to buy',
            ),
            'address': genai.protos.Schema(
                type=genai.protos.Type.STRING,
                description= 'Address the user wants to deliver to',
            ),
            'phone_number': genai.protos.Schema(
                type=genai.protos.Type. STRING,
                description= 'Phone number of the user',
            ),
        },
    ),
)
Hàm cancel_orders:
Hàm này được sử dụng để hủy một đơn hàng đã được tạo. Khi người dùng yêu cầu hủy đơn hàng, hàm sẽ nhận vào tham số:
cancel_orders
order_code: Mã đơn hàng mà người dùng muốn hủy.
Hệ thống sẽ kiểm tra đơn hàng với mã đơn hàng này và thực hiện quy trình hủy đơn hàng theo yêu cầu.
Ví dụ về mô tả hàm cancel_orders cho việc Hủy đơn hàng:
cancel_orders
cancel_orders_declaration = genai.protos.FunctionDeclaration(
    name='cancel_orders',
    description='This function is used to cancel an order that the user has created',
    parameters= genai.protos.Schema(
        type=genai.protos.Type.OBJECT,
        properties={
            'order_code': genai.protos.Schema(
                type=genai.protos.Type.STRING,
                description= 'Order code created',
            ),
        },
    ),
)
4.3.	Tích hợp vào LLM
Chúng ta đã định nghĩa hai hàm quan trọng cho hệ thống chatbot bán hàng trực tuyến. Tiếp theo, chúng ta cần tích hợp hai hàm này vào hệ thống chatbot thông qua tham số tools, giúp chatbot có khả năng hiểu được các hàm đó và thực hiện các tác vụ liên quan đến bán hàng một cách linh hoạt.
tools
tools = genai.protos.Tool(
    function_declarations=[
        create_orders_declaration,
        cancel_orders_declaration
    ],
)

INSTRUCTION = """"""You are a chatbot that helps users perform basic tasks related to online sales, \
such as creating orders and canceling orders. Based on the user's command, extract the necessary \
information to meet the user's request.
If any information is unclear, ask for clarification instead of filling in the information yourself. 
Ask the user if the information is correct before performing the function. If feedback is needed, use Vietnamese.""""""

model = genai.GenerativeModel(
    model_name='gemini-1.5-flash',
    tools=tools,
    system_instruction=INSTRUCTION,
    safety_settings={
        HarmCategory.HARM_CATEGORY_HATE_SPEECH:
        HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HARASSMENT:
        HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT:
        HarmBlockThreshold.BLOCK_NONE,
    },
)
4.4.	Triển khai thử nghiệm
Sau khi hoàn tất các bước xây dựng, tôi đã tiến hành triển khai và thử nghiệm hệ thống để đánh giá hiệu suất hoạt động. Chúng tôi đã xây dựng một bộ các bài kiểm tra nhằm kiểm tra khả năng hiểu yêu cầu của người dùng của mô hình gemini-1.5-flash, đồng thời xác định xem nó có thể đưa ra các hàm và tham số tương ứng hay không.
Dưới đây là các kịch bản cho việc tạo đơn và hủy đơn hàng:
Tạo đơn hàng:
Bạn:  Tôi muốn mua hàng
Bot: Bạn muốn mua sản phẩm gì?
Bạn:  Tôi muốn mua 2 áo sơ mi trắng
Bot: Bạn muốn đặt hàng giao đến địa chỉ nào?
Bạn:  Tôi muốn giao đến Đại học Quốc Gia Hà Nội
Bot: Số điện thoại của bạn là gì?
Bạn:  012345678
Bot: Bạn có chắc chắn muốn đặt mua 2 áo sơ mi trắng, giao đến Đại học Quốc Gia Hà Nội, số điện thoại 012345678?
Bạn:  có
Bot:
Hàm được gọi:  name: ""create_orders""
args {
fields {
key: ""product_name""
value {
string_value: ""áo sơ mi trắng""
},
key: "" quantity""
value {
value: 2
},
key: ""address""
value {
string_value: ""Đại học Quốc Gia Hà Nội""
},
key: ""phone_number""
value {
string_value: ""012345678""
}
}
}
Hủy đơn hàng:
Bạn:  tôi muốn hủy đơn
Bot: Bạn có thể cho tôi biết mã đơn hàng bạn muốn hủy không?
Bạn:  mã là ABC123
Bot: Bạn có chắc chắn muốn hủy đơn hàng mã ABC123?
Bạn:  có
Bot:
Hàm được gọi:  name: ""cancel_orders""
args {
fields {
key: ""order_code""
value {
string_value: ""ABC123""
}
}
}
Mọi người có thể thấy, mô hình đã có khả năng trích xuất hàm phù hợp cùng với các tham số tương ứng khi đã có đủ thông tin cần thiết. Chiến lược mà chúng tôi đã đề ra trong phần INSTRUCTION là yêu cầu chatbot đặt câu hỏi để làm rõ những thông tin còn thiếu và xác nhận lại các thông tin mà người dùng đã cung cấp.
INSTRUCTION
5.1.	Ưu điểm
Function calling có một số ưu điểm như sau:
Tăng cường khả năng tự động hóa: Function calling cho phép các mô hình ngôn ngữ tự động thực hiện các tác vụ cụ thể khi có nhu cầu, như gọi API, truy xuất dữ liệu, hoặc tương tác với các hệ thống khác. Điều này giúp tăng cường tính linh hoạt và khả năng tự động hóa.
Tối ưu hóa luồng hội thoại: Thay vì chỉ phản hồi bằng văn bản, function calling cho phép mô hình gọi các hàm thực hiện nhiệm vụ, từ đó tạo ra các phản hồi có tính năng động và phù hợp hơn trong các tình huống phức tạp.
Giảm tải xử lý của người dùng: Người dùng không cần phải thực hiện các thao tác thủ công, mà mô hình có thể tự động hiểu và thực hiện các lệnh theo yêu cầu. Điều này giúp tiết kiệm thời gian và công sức.
5.2.	Nhược điểm
Function calling mang đến nhiều lợi ích vượt trội, tuy nhiên, nó cũng tồn tại một số hạn chế nhất định:
Tốn khá nhiều tài nguyên: Đối với các mô hình không có Context Caching như LLM của OpenAI, việc sử dụng Function calling sẽ tốn rất nhiều tokens. Nguyên nhân là do mỗi lần gửi request đến mô hình, chúng ta cần gửi toàn bộ phần mô tả các hàm cùng với câu yêu cầu của người dùng.
Khả năng còn hạn chế: Các mô hình miễn phí như Gemini-1.5-Flash chỉ trả về tối đa 1 function cho 1 lần query. Điều này khiến mô hình không thể xử lý các câu chứa nhiều yêu cầu.
Tăng độ trễ phản hồi: Khi function calling đòi hỏi gọi các API bên ngoài hoặc xử lý các tác vụ phức tạp, thời gian phản hồi có thể bị chậm đi, làm giảm trải nghiệm người dùng.
Bảo mật và quyền riêng tư: Khi gọi các hàm bên ngoài, dữ liệu có thể phải được gửi qua các mô hình của bên thứ ba, làm tăng rủi ro bảo mật và quyền riêng tư nếu không được xử lý cẩn thận.
Function calling mang lại nhiều lợi ích vượt trội, đặc biệt trong việc nâng cao tính tự động hóa và khả năng tương tác của các hệ thống sử dụng mô hình ngôn ngữ lớn (LLM). Nhờ tính năng này, các mô hình không chỉ phản hồi theo cách thụ động mà còn có khả năng gọi và thực hiện các hàm cụ thể, tạo ra các trải nghiệm động và thông minh hơn cho người dùng.
Function calling ngoài việc sử dụng trực tiếp với các mô hình ngôn ngữ lớn hiện nay, nó còn có thể tích hợp vào các hệ thống khác. Một ví dụ điển hình cho việc liên hệ function calling với các hệ thống khác là tích hợp Function Calling trong hệ thống RAG (Retrieval-Augmented Generation). Trong hệ thống RAG, mô hình ngôn ngữ sẽ kết hợp khả năng truy xuất thông tin từ các nguồn dữ liệu bên ngoài với việc tạo ra văn bản dựa trên các thông tin đó. FC có thể giúp tối ưu hóa và mở rộng thêm khả năng của RAG.",https://viblo.asia/p/tich-hop-function-calling-vao-he-thong-chatbot-pgjLNvPwL32
27,Cải thiện hiệu quả phát triển Laravel: 5 lệnh Artisan ít được biết đến,"
Laravel nổi tiếng với cú pháp thanh lịch và các tính năng thân thiện với lập trình viên, và công cụ dòng lệnh mạnh mẽ Artisan càng tăng thêm giá trị. Trong khi nhiều lập trình viên quen thuộc với các lệnh Artisan phổ biến, có một số lệnh ít được biết đến có thể nâng cao hiệu quả phát triển của bạn. Bài viết này khám phá năm lệnh như vậy và cách chúng có thể tối ưu hóa quy trình làm việc của bạn.
1. php artisan event:generate
php artisan event:generate
Hệ thống sự kiện của Laravel là một cách mạnh mẽ để tách biệt các thành phần khác nhau của ứng dụng của bạn, làm cho nó dễ bảo trì và mở rộng hơn. Lệnh event:generate tự động tạo ra các lớp sự kiện và trình lắng nghe dựa trên các sự kiện mà bạn đã định nghĩa trong ứng dụng của mình. Điều này có thể tiết kiệm cho bạn một lượng thời gian đáng kể, đặc biệt là khi làm việc với các dự án có nhiều sự kiện.
event:generate
Sử dụng lệnh này rất đơn giản:
php artisan event:generate
Lệnh này quét ứng dụng của bạn để tìm các sự kiện đã đăng ký và tạo ra các lớp và trình lắng nghe cần thiết. Bằng cách tự động hóa quy trình này, bạn có thể tập trung vào việc viết logic sự kiện mà không phải lo lắng về mã mẫu.
2.Công cụ Triển khai Một Nhấp Chuột cho Môi Trường Phát Triển Laravel
Việc triển khai môi trường phát triển cần thiết cho các dự án Laravel có thể được đơn giản hóa bằng cách sử dụng các công cụ như Servbay hoặc Docker. Tôi khuyên bạn nên sử dụng Servbay, một công cụ quản lý môi trường phát triển toàn diện được thiết kế đặc biệt cho lập trình viên. Nó đơn giản hóa việc thiết lập và bảo trì môi trường phát triển, cho phép các lập trình viên bắt đầu lập trình mà không tốn quá nhiều thời gian vào việc cấu hình. Bằng cách tích hợp nhiều ngôn ngữ lập trình, cơ sở dữ liệu, dịch vụ web, dịch vụ DNS và các công cụ phát triển thiết yếu khác, Servbay cung cấp một môi trường làm việc toàn diện và hiệu quả.
3.php artisan vendor:publish --tag=laravel-assets
Khi sử dụng các gói trong Laravel, bạn thường cần công bố các tài nguyên chứa trong những gói đó, chẳng hạn như tệp cấu hình, giao diện và tài sản công cộng. Lệnh vendor:publish thường được sử dụng để công bố tài nguyên gói, nhưng nhiều lập trình viên có thể không biết rằng nó cũng cho phép bạn công bố các thẻ tài nguyên cụ thể.
Ví dụ, để công bố các tài sản mặc định của Laravel, bao gồm các tệp cấu hình và tài nguyên công cộng, bạn có thể sử dụng lệnh sau:
php artisan vendor:publish --tag=laravel-assets
Lệnh này cung cấp một cách rõ ràng và có tổ chức để quản lý tài nguyên từ các gói khác nhau, đảm bảo ứng dụng của bạn luôn được cấu trúc tốt và dễ bảo trì.
php artisan vendor:publish --tag=laravel-assets
4.php artisan optimize
Tối ưu hóa ứng dụng Laravel của bạn là rất quan trọng để đảm bảo hiệu suất tối ưu. Lệnh optimize là một công cụ mạnh mẽ có thể thực hiện nhiều tối ưu hóa khác nhau, bao gồm bộ nhớ cache tuyến đường, bộ nhớ cache cấu hình và tự động tải lớp.
Để chạy tối ưu hóa, chỉ cần thực hiện:
php artisan optimize
Lệnh này đặc biệt hữu ích trong các môi trường sản xuất, nơi mà mỗi cải tiến hiệu suất đều quan trọng. Đáng chú ý, lệnh optimize cũng xóa các giao diện đã biên dịch, đảm bảo ứng dụng của bạn phản ánh bất kỳ thay đổi nào được thực hiện đối với các mẫu Blade.
php artisan optimize
5.php artisan make:policy PostPolicy
Trong khi nhiều lập trình viên Laravel quen thuộc với các lệnh make:model và make:controller, lệnh make:policy ít được biết đến hơn. Các chính sách trong Laravel cung cấp một cách thuận tiện để tổ chức logic ủy quyền, xác định quyền truy cập cho các hành động khác nhau trong ứng dụng của bạn.
Để tạo một chính sách, hãy chạy lệnh sau:
php artisan make:policy PostPolicy
Lệnh này tạo ra một lớp chính sách mới trong thư mục App\Policies. Bạn có thể định nghĩa logic ủy quyền của mình trong lớp này, giúp dễ dàng quản lý và bảo trì quyền truy cập trong ứng dụng của bạn.
php artisan make:policy PostPolicy
6.php artisan down --message=""Chế độ Bảo trì""
Trong quá trình bảo trì hoặc cập nhật, bạn có thể muốn đưa ứng dụng Laravel của mình vào chế độ bảo trì để thực hiện các tác vụ cần thiết. Lệnh down cho phép bạn kích hoạt chế độ bảo trì và hiển thị một thông điệp tùy chỉnh cho người dùng.
Để kích hoạt chế độ bảo trì, hãy chạy:
php artisan down --message=""Chế độ Bảo trì""
Lệnh này thông báo cho người dùng rằng ứng dụng đang trong quá trình bảo trì và sẽ trở lại sớm. Trong khi đó, bạn có thể thực hiện các cập nhật hoặc thay đổi mà không ảnh hưởng đến trải nghiệm của người dùng.
php artisan down --message=""Chế độ Bảo trì""",https://viblo.asia/p/cai-thien-hieu-qua-phat-trien-laravel-5-lenh-artisan-it-duoc-biet-den-y37LdY8YVov
28,Hiểu bản chất histogram trong oracle database,"Histogram đóng vai trò như một công cụ mạnh mẽ giúp Oracle hiểu rõ hơn về cách phân phối dữ liệu. Bằng cách cung cấp cái nhìn chi tiết về các giá trị trong các cột, histogram cho phép cost-based optimizer đưa ra quyết định tốt hơn, từ đó tối ưu hóa hiệu suất truy vấn.
Histogram ghi lại sự phân bố của dữ liệu khi chúng ta analyze một bảng hoặc index. Với thông tin này, cost-based optimizer có thể quyết định sử dụng index cho các điều kiện mà nó biết sẽ trả về một số lượng ít các hàng, và bỏ qua index khi điều kiện giới hạn sẽ trả về nhiều hàng. Việc sử dụng histogram không chỉ giới hạn cho các index. Bất kỳ cột nào của một bảng đều có thể xây dựng histogram trên đó.
Lý do chính để tạo ra các histogram là để giúp trình tối ưu hóa lập kế hoạch đúng đắn nếu dữ liệu trong một bảng bị lệch nhiều. Ví dụ, nếu một hoặc hai giá trị chiếm phần lớn tỷ lệ của bảng, các index liên quan có thể không giúp giảm số lượng I/O cần thiết để xử lý truy vấn. Việc tạo ra một histogram cho phép cost-based optimizer biết khi nào việc sử dụng index là phù hợp, hoặc khi 80% bảng sẽ được trả về do giá trị trong mệnh đề WHERE.
Khi tạo histogram, chúng ta cần chỉ định kích thước cụ thể. Kích thước này liên quan đến số lượng bucket cho histogram. Mỗi bucket sẽ chứa thông tin về giá trị của cột và số lượng hàng.
Truy vấn trên tạo ra một histogram gồm mười bucket trên bảng COMPANY, như được minh họa trong hình bên dưới. Các giá trị của cột COMPANY_CODE được chia vào mười bucket như hình. Ví dụ này cho thấy một số lượng lớn (80%) giá trị của COMPANY_CODE bằng 1430. Như được thể hiện trong hình, hầu hết các bucket có Width Balanced (cân bằng theo chiều rộng) chứa chỉ 3 hàng, trong khi một bucket chứa đến 73 hàng. Trong Height Balanced (cân bằng theo chiều cao) của phân phối này, mỗi bucket có cùng số lượng hàng và phần lớn các điểm kết thúc của bucket đều là 1430, phản ánh sự phân bố không đồng đều của dữ liệu.

Histograms của Oracle được cân bằng theo chiều cao thay vì cân bằng theo chiều rộng. Do đó, tất cả các bucket trong histogram chứa cùng một số lượng hàng. Điểm bắt đầu và kết thúc của một bucket được xác định bởi số lượng hàng chứa các giá trị đó. Histogram cân bằng theo chiều rộng xác định các giá trị phạm vi cho mỗi bucket, sau đó đếm số lượng hàng trong phạm vi đó, nhưng đây không phải là một tùy chọn lý tưởng.
Cột chứa dữ liệu phân bố không đồng đều
Giả sử bạn có một bảng ORDERS chứa thông tin về các đơn đặt hàng và cột ORDER_STATUS chỉ ra trạng thái của từng đơn hàng. Nếu trạng thái ""Delivered"" chiếm 90% dữ liệu và các trạng thái còn lại chỉ chiếm 10%, thì việc sử dụng histograms có thể giúp Oracle tối ưu hóa truy vấn tốt hơn.
Truy vấn cột có giá trị thường xuyên lặp lại
Nếu bạn có bảng CUSTOMERS với cột REGION chứa các khu vực địa lý của khách hàng, và một số khu vực có nhiều khách hàng hơn hẳn, histogram có thể cải thiện cách Oracle tối ưu hóa các truy vấn. Nếu một khu vực chứa 80% khách hàng, Oracle có thể sử dụng histogram để chọn một kế hoạch truy vấn khác (ví dụ: dùng full table scan thay vì index scan) khi lọc dữ liệu.
Truy vấn với các cột có dữ liệu phân bố khá đồng đều
Nếu cột AGE trong bảng EMPLOYEES chứa độ tuổi của nhân viên và phân phối dữ liệu khá đồng đều, việc tạo histograms trên cột này sẽ không mang lại nhiều lợi ích về hiệu suất.Ở đây, không tạo histogram vì dữ liệu phân phối đều, Oracle sẽ không thu được thêm thông tin nào hữu ích.
Histogram trong các truy vấn với nhiều giá trị DISTINCT
Cột ZIP_CODE trong bảng ADDRESSES có nhiều giá trị khác nhau, nhưng một số mã bưu điện có nhiều bản ghi hơn những mã khác. Histogram sẽ giúp Oracle hiểu rõ hơn về phân phối của các giá trị và chọn kế hoạch truy vấn phù hợp hơn.
TIP
Nếu dữ liệu trong một bảng có dữ liệu phân bố không đồng đều, histograms sẽ giúp cost-based optimizer có cái nhìn cân đối hơn về sự phân phối dữ liệu (bằng cách chia dữ liệu thành các bucket). Tuy nhiên, việc áp dụng histograms trên các cột có dữ liệu khá đồng đều sẽ không mang lại lợi ích về hiệu suất.
Theo mặc định, Oracle tạo 254 bucket trong một histogram. Chúng ta có thể chỉ định các giá trị SIZE từ 1 đến 254.
Nếu anh em muốn trao đổi thêm về bài viết, hãy kết nối với mình qua LinkedIn và Facebook:
LinkedIn: https://www.linkedin.com/in/nguyentrungnam/
Facebook: https://www.facebook.com/trungnam.nguyen.395/
Rất mong được kết nối và cùng thảo luận!",https://viblo.asia/p/hieu-ban-chat-histogram-trong-oracle-database-PAoJePmZ41j
29,Giải pháp lưu trữ phân tầng trong kafka,"Đơn vị lưu trữ cơ bản của Kafka là partition replicas. Partittion không thể được tách nhỏ và lưu trữ giữa nhiều broker, và cũng không thể được tách nhỏ và lưu trữ trên nhiều ổ đĩa khác nhau trên cùng một broker. Do đó, kích thước của partition bị giới hạn bởi dung lượng có sẵn trên một mount point duy nhất.
Khi cấu hình Kafka, chúng ta sẽ định nghĩa một danh sách các thư mục nơi các partition sẽ được lưu trữ — đây là tham số log.dirs (không nên nhầm lẫn với vị trí mà Kafka lưu trữ log lỗi của nó, được cấu hình trong tệp log4j.properties). Cấu hình thông thường bao gồm một thư mục cho mỗi mount point mà Kafka sẽ sử dụng.
Vào cuối năm 2018, cộng đồng Apache Kafka bắt đầu hợp tác trong một dự án đầy tham vọng nhằm bổ sung thêm giải pháp lưu trữ phân tầng (tiered storage) cho Kafka. Công việc trên dự án này vẫn đang tiếp diễn và dự kiến sẽ được phát hành trong phiên bản 3.0.
Động lực cho dự án này khá rõ ràng: Kafka hiện đang được sử dụng để lưu trữ một lượng lớn dữ liệu, hoặc do thông lượng cao hoặc thời gian duy trì dữ liệu lâu dài. Những điều này gây ra các mối quan ngại, lo lắng sau:
Chúng ta sẽ bị giới hạn về lượng dữ liệu có thể lưu trữ trong một partition. Kết quả là, thời gian duy trì tối đa và số lượng partition không chỉ phụ thuộc vào yêu cầu của sản phẩm mà còn bởi các giới hạn về kích thước ổ cứng vật lý.
Lựa chọn về ổ cứng và kích thước cluster của chúng ta phụ thuộc vào yêu cầu lưu trữ. Các cluster thường lớn hơn cần thiết nếu độ trễ và thông lượng là yếu tố chính, điều này làm tăng chi phí.
Thời gian di chuyển các partition từ broker này sang broker khác, chẳng hạn khi mở rộng hoặc thu hẹp cluster, bị ảnh hưởng bởi kích thước của các phân vùng. Phân vùng lớn làm cho cluster ít linh hoạt hơn. Ngày nay, các kiến trúc thường hướng đến độ linh hoạt tối đa, tận dụng các tùy chọn triển khai linh hoạt trên cloud.
Trong cách tiếp cận lưu trữ phân tầng, Kafka Cluster được cấu hình với hai tầng lưu trữ: cục bộ (local tier) và từ xa (remote tier). Local tier sẽ giống như tầng lưu trữ hiện tại của Kafka—sử dụng các ổ cứng trên các broker của Kafka để lưu trữ các đoạn log. Remote tier mới sẽ sử dụng các hệ thống lưu trữ chuyên dụng, chẳng hạn như HDFS hoặc S3, để lưu trữ các đoạn log đã hoàn tất.
Người dùng Kafka có thể chọn thiết lập chính sách lưu trữ riêng biệt cho mỗi tầng. Vì local storage thường đắt đỏ hơn nhiều so với remote tier, nên thời gian lưu trữ cho local tier thường chỉ kéo dài vài giờ hoặc thậm chí ngắn hơn, trong khi thời gian lưu trữ cho remote tier có thể lâu hơn nhiều—vài ngày hoặc thậm chí vài tháng.
Local storage có độ trễ thấp hơn đáng kể so với remote storage, rất hiệu quả đối với các ứng dụng nhạy cảm với độ trễ. Các ứng dụng khôi phục từ sự cố hoặc các ứng dụng cần dữ liệu cũ hơn dữ liệu trong local tier sẽ được trích xuất từ remote tier.
Kiến trúc hai tầng lưu trữ trong phương pháp lưu trữ phân tầng cho phép mở rộng lưu trữ mà không phụ thuộc vào bộ nhớ và CPU trong cụm Kafka. Điều này giúp Kafka trở thành giải pháp lưu trữ lâu dài. Hơn nữa, nó có tác dụng giảm lượng dữ liệu lưu trữ cục bộ trên các broker Kafka, từ đó giảm lượng dữ liệu cần sao chép trong quá trình phục hồi và cân bằng lại. Các phân đoạn log (Log segments) luôn sẵn sàng trong remote tier nên không cần phải phục hồi trên broker hoặc phục hồi một cách chậm chạp và được trích xuất trực tiếp từ remote tier. Vì không phải tất cả dữ liệu đều lưu trữ trên các broker, việc gia tăng thời gian lưu trữ dữ liệu không còn cần thiết mở rộng dung lượng lưu trữ của cụm Kafka và thêm các nút mới. Thời gian lưu trữ dữ liệu tổng thể vẫn có thể kéo dài hơn nhiều, loại bỏ nhu cầu về các quy trình dữ liệu riêng biệt để sao chép dữ liệu từ Kafka sang các kho lưu trữ bên ngoài, như hiện tại đang được thực hiện trong nhiều cách triển khai.
Thiết kế của lưu trữ phân tầng được tài liệu hóa chi tiết trong KIP-405, bao gồm một thành phần mới—RemoteLogManager và các tương tác với các chức năng hiện có, chẳng hạn như các bản sao theo kịp nhà leader và cuộc bầu chọn leader.
Một kết quả thú vị được tài liệu hóa trong KIP-405 là các tác động đến hiệu suất của lưu trữ phân tầng. Nhóm triển khai lưu trữ phân tầng đã đo lường hiệu suất trong một số trường hợp sử dụng:
Trường hợp đầu tiên là sử dụng với throughput cao. Trong trường hợp này, độ trễ tăng một chút (từ 21 ms lên 25ms trong p99), vì các broker cũng phải gửi các phân đoạn dữ liệu đến remote tier.
Trường hợp sử dụng thứ hai là khi một số consumer đang đọc dữ liệu cũ. Nếu không có lưu trữ phân tầng, việc đọc dữ liệu cũ của consumer có ảnh hưởng lớn đến độ trễ (21 ms so với 60 ms p99), nhưng khi lưu trữ phân tầng được kích hoạt, ảnh hưởng giảm đáng kể (25 ms so với 42 ms p99); điều này là do việc đọc lưu trữ phân tầng được thực hiện từ HDFS hoặc S3 qua đường truyền mạng. Đọc từ mạng không cạnh tranh với việc đọc từ local storage hoặc bộ nhớ page cache, và giữ cho bộ nhớ page cache không bị thay đổi với dữ liệu mới làm cho cache k bị invalid.
Điều này có nghĩa là, ngoài việc cung cấp lưu trữ vô hạn, chi phí thấp hơn và khả năng mở rộng, lưu trữ theo cấp độ còn mang lại sự độc lập giữa việc đọc dữ liệu lịch sử và việc đọc dữ liệu thời gian thực.
Nếu anh em muốn trao đổi thêm về bài viết, hãy kết nối với mình qua LinkedIn và Facebook:
LinkedIn: https://www.linkedin.com/in/nguyentrungnam/
Facebook: https://www.facebook.com/trungnam.nguyen.395/
Rất mong được kết nối và cùng thảo luận!",https://viblo.asia/p/giai-phap-luu-tru-phan-tang-trong-kafka-gwd432mXVX9
30,(Phần 3) Tìm hiểu về Ansible Playbooks,"Trong bài viết này, chúng ta sẽ khám phá Ansible Playbooks, về cơ bản là bản thiết kế cho các hành động tự động hóa.
Playbooks cho phép chúng ta xác định một công thức với tất cả các bước mà chúng ta muốn tự động hóa theo cách có thể lặp lại, đơn giản và nhất quán.
Ansible playbook cung cấp một môi trường thực thi đa máy, đa bước, mạch lạc. Trong hướng dẫn này, chúng tôi sẽ hướng dẫn bạn quy trình tạo Ansible playbook theo cách rất chi tiết.
Playbook Ansible là gì?
Cấu trúc của sổ tay hướng dẫn Ansible là gì?
Làm thế nào để viết một playbook Ansible?
Làm thế nào để chạy playbook Ansible?
Ví dụ về playbook Ansible
Sử dụng biến trong Ansible playbook
Sử dụng các tác vụ có điều kiện when
Sử dụng vòng lặp loop
Tips sử dụng Ansible playbook
Playbook Ansible là gì?
Playbook Ansible là một trong những thành phần cơ bản của Ansible vì chúng ghi lại và thực thi cấu hình của Ansible. Nhìn chung, playbook là cách chính để tự động hóa một tập hợp các tác vụ mà chúng ta muốn thực hiện trên máy từ xa.
Chúng giúp nỗ lực tự động hóa của chúng tôi bằng cách thu thập tất cả các tài nguyên cần thiết để sắp xếp các quy trình theo thứ tự hoặc tránh lặp lại các hành động thủ công. Playbook có thể được sử dụng lại và chia sẻ giữa mọi người và được thiết kế thân thiện với con người và dễ viết bằng YAML.
Sự khác biệt giữa playbook và vai trò trong Ansible là gì?
Ansible playbook có phạm vi rộng hơn và có khả năng sắp xếp nhiều plays  và vai trò trên nhiều máy chủ và nhóm khác nhau, trong khi vai trò là các thành phần tập trung hơn, nhắm vào các tác vụ và cấu hình cụ thể.
Khi nói đến cách sử dụng, Playbook thực hiện tự động hóa, trong khi vai trò được sử dụng để cấu trúc và đóng gói tự động hóa theo một hình thức có thể tái sử dụng.
Cấu trúc của sổ tay hướng dẫn Ansible là gì?
Một playbook bao gồm một hoặc nhiều task được chạy theo thứ tự cụ thể. Một plays là danh sách có thứ tự các tác vụ được chạy với nhóm máy chủ mong muốn.
Mỗi tác vụ đều được liên kết với một mô-đun chịu trách nhiệm cho một hành động và các tham số cấu hình của nó. Vì hầu hết các tác vụ đều có tính bất biến, chúng ta có thể chạy lại playbook một cách an toàn mà không có bất kỳ vấn đề nào.
Như đã thảo luận, Ansible được viết bằng YAML sử dụng phần mở rộng chuẩn .yml với cú pháp tối thiểu .
Chúng ta phải sử dụng khoảng trắng để căn chỉnh các phần tử dữ liệu có cùng thứ bậc để thụt lề. Các mục là con của các mục khác phải được thụt lề nhiều hơn các mục cha của chúng. Không có quy tắc nghiêm ngặt nào về số khoảng trắng được sử dụng để thụt lề, nhưng việc sử dụng hai khoảng trắng trong khi không được phép sử dụng ký tự Tab là khá phổ biến.
Dưới đây là một ví dụ về sổ tay hướng dẫn đơn giản chỉ có hai vở kịch, chúng ta có  hai nhiệm vụ:
---
- name: Example Simple Playbook
  hosts: all
  become: yes

  tasks:
  - name: Copy file example_file to /tmp with permissions
    ansible.builtin.copy:
      src: ./example_file
      dest: /tmp/example_file
      mode: '0644'

  - name: Add the user 'bob' with a specific uid 
    ansible.builtin.user:
      name: bob
      state: present
      uid: 1040

- name: Update postgres servers
  hosts: databases
  become: yes

  tasks:
  - name: Ensure postgres DB is at the latest version
    ansible.builtin.yum:
      name: postgresql
      state: latest

  - name: Ensure that postgresql is started
    ansible.builtin.service:
      name: postgresql
      state: started
Chúng ta định nghĩa rằng các vở kịch này sẽ được thực hiện với tư cách là người dùng root với tùy chọn become được đặt thành yes.
Bạn cũng có thể định nghĩa nhiều Từ khóa Playbook khác ở các cấp độ khác nhau như play, task, playbook để cấu hình hành vi của Ansible. Thậm chí, hầu hết các từ khóa này có thể được thiết lập tại thời gian chạy dưới dạng cờ dòng lệnh trong tệp cấu hình ansible, ansible.cfg hoặc inventory . Kiểm tra các quy tắc ưu tiên để hiểu cách Ansible hoạt động trong những trường hợp này.
Làm thế nào để viết một playbook Ansible?
Viết Ansible playbook bao gồm việc tạo tệp YAML chỉ định các máy chủ cần cấu hình và các tác vụ cần thực hiện trên các máy chủ này.
Thông thường, theo cách thực hành tốt nhất, để chỉ định máy chủ của bạn, bạn cần phải xác định Ansible inventory file. Để đơn giản hóa mọi thứ, chúng ta sẽ sử dụng localhost:
[local]
localhost ansible_connection=local
Tiếp theo, chúng ta hãy định nghĩa một tệp play sẽ ping tới máy chủ cục bộ:
- name: Play
 hosts: local
 tasks:
  - name: Ping my hosts
    ansible.builtin.ping:
Để chạy lệnh này, chúng ta có thể sử dụng:
ansible-playbook -i inventory.ini play.yaml
ansible-playbook -i inventory.ini play.yaml
PLAY [Play] ******************************************************************************************************************************************

TASK [Gathering Facts] *******************************************************************************************************************************
[WARNING]: Platform darwin on host localhost is using the discovered Python interpreter at /opt/homebrew/bin/python3.12, but future installation of
another Python interpreter could change the meaning of that path. See https://docs.ansible.com/ansible-
core/2.16/reference_appendices/interpreter_discovery.html for more information.
ok: [localhost]

TASK [Ping my hosts] *********************************************************************************************************************************
ok: [localhost]

PLAY RECAP *******************************************************************************************************************************************
localhost                  : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
Những điều cần lưu ý khi bạn viết sổ tay Ansible playbook:
YAML nhạy cảm với thụt lề, thường yêu cầu 2 khoảng trắng cho mỗi cấp độ thụt lề
Tận dụng các biến trong playbook của bạn để làm cho chúng năng động và linh hoạt hơn.
Biến có thể được định nghĩa ở nhiều nơi, bao gồm trong chính playbook, trong inventory, trong các tệp riêng biệt hoặc thậm chí được truyền qua dòng lệnh
Use handlers – đây là các tác vụ đặc biệt chỉ chạy khi được tác vụ khác thông báo. Chúng thường được sử dụng để khởi động lại dịch vụ khi cấu hình thay đổi.
Tận dụng các template – Ansible có thể sử dụng các template Jinja2 để tạo các tệp động dựa trên các biến
Tận dụng roles - Đối với các thiết lập phức tạp, hãy cân nhắc việc sắp xếp nhiệm vụ của bạn thành các vai trò. Điều này giúp giữ cho playbook của bạn luôn sạch sẽ và làm cho nhiệm vụ của bạn có thể tái sử dụng nhiều hơn
Chúng ta có thể sử dụng cờ –limit để giới hạn việc thực thi Playbook cho các máy chủ cụ thể. Ví dụ:
ansible-playbook example-simple-playbook.yml --limit host1
Sử dụng các biến trong Ansible playbook
Biến là giá trị mà bạn có thể sử dụng lại trong Playbook hoặc các đối tượng Ansible khác. Chúng chỉ có thể chứa các chữ cái, số và dấu gạch dưới và bắt đầu bằng các chữ cái.
Biến có thể được định nghĩa trong Ansible ở nhiều cấp độ, vì vậy hãy xem thứ tự ưu tiên của biến để hiểu cách chúng được áp dụng. Ví dụ, chúng ta có thể đặt biến ở phạm vi toàn cục cho tất cả các máy chủ, ở phạm vi máy chủ cho một máy chủ cụ thể hoặc ở phạm vi phát cho một lần phát cụ thể.
Để thiết lập biến host và biến nhóm, hãy tạo các thư mục groupvars và hostvars . Ví dụ, để xác định biến cho nhóm cơ sở dữ liệu , hãy tạo tệp groupvars/databases. Thiết lập các biến mặc định chung trong tệp *groupvars/all *.
Ngoài ra, để xác định biến máy chủ cho một máy chủ cụ thể, hãy tạo một tệp có cùng tên với máy chủ trong thư mục hostsvars.
Để thay thế bất kỳ biến nào trong thời gian chạy, hãy sử dụng cờ -e .
Phương pháp trực tiếp nhất để định nghĩa biến là sử dụng khối vars ở đầu play. Chúng được định nghĩa bằng cú pháp YAML chuẩn.
- name: Example Variables Playbook
  hosts: all
  vars:
    username: bob
    version: 1.2.3
Một cách khác là định nghĩa các biến trong tệp YAML bên ngoài.
- name: Example Variables Playbook
  hosts: all
  vars_files:
    - vars/example_variables.yml
Để sử dụng chúng trong các tác vụ, chúng ta phải tham chiếu chúng bằng cách đặt tên của chúng bên trong dấu ngoặc kép  {{ username }} bằng cú pháp Jinja2 :
- name: Example Variables Playbook
  hosts: all
  vars:
    username: bob

  tasks:
  - name: Add the user {{ username }}
    ansible.builtin.user:
      name: ""{{ username }}""
      state: present
Nếu giá trị của biến bắt đầu bằng dấu ngoặc nhọn, chúng ta phải trích dẫn toàn bộ biểu thức để YAML có thể diễn giải cú pháp một cách chính xác.
Chúng ta cũng có thể định nghĩa các biến có nhiều giá trị dưới dạng danh sách.
package:
  - foo1
  - foo2
  - foo3
Bạn cũng có thể tham chiếu các giá trị riêng lẻ từ danh sách. Ví dụ, để chọn giá trị đầu tiên foo1 :
package: ""{{ package[0] }}""
Một lựa chọn khả thi khác là định nghĩa các biến bằng cách sử dụng từ điển YAML. Ví dụ:
package: ""{{ package[0] }}""
dictionary_example: 
  - foo1: one
  - foo2: two
Tương tự như vậy, để lấy trường đầu tiên từ từ điển:
dictionary_example['foo1']
Để tham chiếu các biến lồng nhau, chúng ta phải sử dụng ký hiệu ngoặc  hoặc dấu chấm. Ví dụ, để lấy giá trị examplename2 từ cấu trúc này:
vars:
  var1:
    foo1:
      field1: example_name_1
      field2: example_name_2
tasks:
- name: Create user for field2 value
  user: 
    name: ""{{ var1['foo1']['field2'] }}""
Chúng ta có thể tạo biến bằng cách sử dụng câu lệnh register để ghi lại đầu ra của lệnh hoặc tác vụ, sau đó sử dụng chúng trong các tác vụ khác.
- name: Example-2 Variables Playbook
  hosts: all

  tasks:
  - name: Run a script and register the output as a variable
    shell: ""find example_file""
    args:
      chdir: ""/tmp""
    register: example_script_output
Để debug giá trị của biến:
- name: Use the output variable of the previous task
    debug:
      var: examplescriptoutput*
Sử dụng các tác vụ có điều kiện trong Ansible playbook
Để kiểm soát luồng thực thi trong Ansible tốt hơn, chúng ta có thể tận dụng các điều kiện.
Điều kiện cho phép chúng ta chạy hoặc bỏ qua các tác vụ dựa trên việc các điều kiện nhất định có được đáp ứng hay không. Các biến, sự kiện hoặc kết quả của các tác vụ trước đó cùng với các toán tử có thể được sử dụng để tạo ra các điều kiện như vậy.
Một số ví dụ về trường hợp sử dụng có thể là cập nhật biến dựa trên giá trị của biến khác, bỏ qua tác vụ nếu biến có giá trị cụ thể, chỉ thực hiện tác vụ nếu dữ liệu từ máy chủ trả về giá trị cao hơn ngưỡng.
Để áp dụng một câu lệnh điều kiện đơn giản, chúng ta sử dụng tham số Ansible when trên một tác vụ . Nếu điều kiện được đáp ứng, tác vụ sẽ được thực thi. Nếu không, nó sẽ bị bỏ qua.
- name: Example Simple Conditional
  hosts: all
  vars:
    trigger_task: true

  tasks:
  - name: Install nginx
    apt:
      name: ""nginx""
      state: present
    when: trigger_task == true
Trong ví dụ trên, nhiệm vụ được thực thi khi điều kiện được đáp ứng với triggertask: true
Một mô hình phổ biến khác là kiểm soát việc thực hiện tác vụ dựa trên các thuộc tính của máy chủ từ xa mà chúng ta có thể lấy được từ facts . Hãy xem danh sách này với các facts thường dùng để có ý tưởng về tất cả các facts mà chúng ta có thể sử dụng trong các điều kiện.
- name: Example Facts Conditionals 
  hosts: all
  vars:
    supported_os:
      - RedHat
      - Fedora
tasks:
- name: Install nginx
  yum:
    name: ""nginx""
    state: present
  when: ansible_facts['distribution'] in supported_os
Có thể kết hợp nhiều điều kiện với các toán tử logic và nhóm chúng bằng dấu ngoặc đơn:
when: (colour==""green"" or colour==""red"") and (size=""small"" or size=""medium"")
Sau đó, khi câu lệnh hỗ trợ sử dụng danh sách trong trường hợp chúng ta có nhiều điều kiện mà tất cả đều cần phải đúng:
when:
  - ansible_facts['distribution'] == ""Ubuntu""
  - ansible_facts['distribution_version'] == ""20.04""
  - ansible_facts['distribution_release'] == ""bionic""
Một lựa chọn khác là sử dụng các điều kiện dựa trên các biến đã đăng ký mà chúng ta đã xác định trong các tác vụ trước:
- name: Example Registered Variables Conditionals
  hosts: all

  tasks:
  - name: Register an example variable
    ansible.builtin.shell: cat /etc/hosts
    register: hosts_contents

  - name: Check if hosts file contains ""localhost""
    ansible.builtin.shell: echo ""/etc/hosts contains localhost""
    when: hosts_contents.stdout.find(localhost) != -1
Cách sử dụng vòng lặp trong Ansible playbook
Ansible cho phép chúng ta lặp lại một tập hợp các mục trong một tác vụ để thực hiện nó nhiều lần với các tham số khác nhau mà không cần viết lại.
Ví dụ, để tạo một số tệp, chúng ta sẽ sử dụng một tác vụ lặp lại danh sách tên thư mục thay vì viết năm tác vụ với cùng một mô-đun.
Đọc thêm về cách sử dụng vòng lặp trong Ansible .
Để lặp lại một danh sách các mục đơn giản, hãy sử dụng từ khóa loop . Chúng ta có thể tham chiếu giá trị hiện tại với biến vòng lặp item .
- name: ""Create some files""
  ansible.builtin.file:
    state: touch
    path: /tmp/{{ item }}
  loop:
    - example_file1
    - example_file2
    - example_file3
Đầu ra của tác vụ trên sử dụng vòng lặp và mục :
TASK [Create some files] *********************************
changed: [host1] => (item=example_file1)
changed: [host1] => (item=example_file2)
changed: [host1] => (item=example_file3)
Bạn cũng có thể lặp lại các từ điển:
- name: ""Create some files with dictionaries""
  ansible.builtin.file:
    state: touch
    path: ""/tmp/{{ item.filename }}""
    mode: ""{{ item.mode }}""
  loop:
    - { filename: 'example_file1', mode: '755'}
    - { filename: 'example_file2', mode: '775'}
    - { filename: 'example_file3', mode: '777'}
Một mô hình hữu ích khác là lặp lại một nhóm máy chủ của kho lưu trữ:
- name: Show all the hosts in the inventory
  ansible.builtin.debug:
    msg: ""{{ item }}""
  loop: ""{{ groups['databases'] }}""
Bằng cách kết hợp các điều kiện và vòng lặp, chúng ta có thể chọn thực hiện tác vụ chỉ trên một số mục trong danh sách và bỏ qua đối với các mục khác:
- name: Execute when values in list are lower than 10
  ansible.builtin.command: echo {{ item }}
  loop: [ 100, 200, 3, 600, 7, 11 ]
  when: item < 10
Cuối cùng, một lựa chọn khác là sử dụng từ khóa until để thử lại tác vụ cho đến khi điều kiện là đúng.
- name: Retry a task until we find the word ""success"" in the logs
  shell: cat /var/log/example_log
  register: logoutput
  until: logoutput.stdout.find(""success"") != -1
  retries: 10
  delay: 15
Trong ví dụ trên, chúng ta kiểm tra tệp examplelog 10 lần, với độ trễ 15 giây giữa mỗi lần kiểm tra cho đến khi chúng ta tìm thấy từ success . Nếu chúng ta để tác vụ chạy và thêm từ success vào tệp examplelog sau một thời gian, chúng ta sẽ thấy tác vụ dừng thành công.
TASK [Retry a task until we find the word “success” in the logs] *********
FAILED - RETRYING: Retry a task until we find the word ""success"" in the logs (10 retries left).
FAILED - RETRYING: Retry a task until we find the word ""success"" in the logs (9 retries left).
changed: [host1]
Tips sử dụng Ansible playbook
Ghi nhớ những mẹo và thủ thuật này khi xây dựng playbook chiến lược sẽ giúp bạn làm việc hiệu quả hơn và nâng cao năng suất.
Giữ nó đơn giản nhất có thể
Cố gắng giữ cho nhiệm vụ của bạn đơn giản. Có nhiều tùy chọn và cấu trúc lồng nhau trong Ansible và bằng cách kết hợp nhiều tính năng, bạn có thể tạo ra các thiết lập khá phức tạp. Dành thời gian để đơn giản hóa các hiện vật Ansible của bạn sẽ có lợi về lâu dài.
Giữ nó đơn giản nhất có thể
Cố gắng giữ cho nhiệm vụ của bạn đơn giản. Có nhiều tùy chọn và cấu trúc lồng nhau trong Ansible và bằng cách kết hợp nhiều tính năng, bạn có thể tạo ra các thiết lập khá phức tạp. Dành thời gian để đơn giản hóa các hiện vật Ansible của bạn sẽ có lợi về lâu dài.
Đặt các artifacts Ansible của bạn dưới sự kiểm soát phiên bản
Người ta cho rằng lưu trữ sổ tay hướng dẫn trong git hoặc bất kỳ hệ thống kiểm soát phiên bản nào khác và tận dụng những lợi ích của nó là tốt nhất.
Đặt các artifacts Ansible của bạn dưới sự kiểm soát phiên bản
Người ta cho rằng lưu trữ sổ tay hướng dẫn trong git hoặc bất kỳ hệ thống kiểm soát phiên bản nào khác và tận dụng những lợi ích của nó là tốt nhất.
Luôn đặt tên mô tả cho các tasks, plays, and playbooks
Chọn những cái tên giúp bạn và những người khác hiểu nhanh chức năng và mục đích của artifacts
Luôn đặt tên mô tả cho các tasks, plays, and playbooks
Chọn những cái tên giúp bạn và những người khác hiểu nhanh chức năng và mục đích của artifacts
Cố gắng để dễ đọc
Sử dụng thụt lề nhất quán và thêm dòng trống giữa các tác vụ để dễ đọc hơn.
Cố gắng để dễ đọc
Sử dụng thụt lề nhất quán và thêm dòng trống giữa các tác vụ để dễ đọc hơn.
Luôn đề cập rõ ràng đến trạng thái của nhiệm vụ
Nhiều mô-đun có trạng thái mặc định cho phép chúng ta bỏ qua tham số trạng thái . Luôn tốt hơn là nêu rõ ràng trong những trường hợp này để tránh nhầm lẫn.
Luôn đề cập rõ ràng đến trạng thái của nhiệm vụ
Nhiều mô-đun có trạng thái mặc định cho phép chúng ta bỏ qua tham số trạng thái . Luôn tốt hơn là nêu rõ ràng trong những trường hợp này để tránh nhầm lẫn.
Sử dụng comment khi cần thiết
Sẽ có những lúc định nghĩa nhiệm vụ không đủ để giải thích toàn bộ tình huống, vì vậy bạn có thể thoải mái sử dụng bình luận cho các phần phức tạp hơn của sổ tay hướng dẫn
Sử dụng comment khi cần thiết
Sẽ có những lúc định nghĩa nhiệm vụ không đủ để giải thích toàn bộ tình huống, vì vậy bạn có thể thoải mái sử dụng bình luận cho các phần phức tạp hơn của sổ tay hướng dẫn",https://viblo.asia/p/phan-3-tim-hieu-ve-ansible-playbooks-zXRJ8DqMJGq
31,[Liệu bạn có biết] Bài 2: Cách xác định sử dụng kĩ thuật nào trong ML,"Trong bài toán Machine Learning của chúng ta chia ra làm 3 bài toán con là: Supervised Learning - Unsupervised Learning - Reinforcement Learning, nhưng trong đó Supervised Learning là bài toán mà chúng ta gặp thường xuyên nhất. Trong Supervised Learning có 2 loại chính là Classification - Regression và trong mỗi loại đó có rất nhiều thuật toán để xử lí, ví dụ như:
Classification: Support Vector Machine, Decision Trees, Random Forest,....
Regression: Linear Regression, Polynomial Regression, Ridge Regression, Lasso Regression,....
Vậy câu hỏi đặt ra là: có quá nhiều thuật toán để xử lí vậy làm sao chúng ta xác định được thuật toán nào phù hợp cho bài toán của mình
Ngày xưa chắc hẳn các bạn đã biết đến 2 phương pháp: GridsearchCV hoặc RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
Điều tệ nhất về những phương pháp này là mất quá nhiều thời gian để tìm kiếm thuật toán phù hợp vì phương pháp của Grid/Random Search là sử dụng vòng lặp để kiểm tra tất cả các trường hợp. Mặc dù cách này không tệ nhưng cũng chưa đủ tốt, vì vậy mình sẽ chỉ cho các bạn một cách mới để tìm ra thuật toán tốt nhất.
Lazy predict
Lazy predict giúp chúng ta xác định nên chọn thuật toán nào cho phù hợp với bài toán 1 cách nhanh, gọn, lẹ và dễ dàng
Các bạn có thể cài đặt Lazy Predict bằng lệnh:
pip install lazypredict
Điều tuyệt vời nhất về Lazy Predict là tốc độ,nó mất ít thời gian hơn nhiều để tìm ra thuật toán phù hợp với bộ dữ liệu của bạn, nhưng chỉ với các tham số mặc định.
Cách dùng
Sẽ có những thư viện mà các bạn chưa từng thấy bao giờ nhưng  cũng không cần phải import trước khi dùng Lazy predict
Classification
from lazypredict.Supervised import LazyClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

data = load_breast_cancer()
X = data.data
y= data.target

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.5,random_state =123)

clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)
models,predictions = clf.fit(X_train, X_test, y_train, y_test)

print(models)
Regression
from lazypredict.Supervised import LazyRegressor
from sklearn import datasets
from sklearn.utils import shuffle
import numpy as np

boston = datasets.load_boston()
X, y = shuffle(boston.data, boston.target, random_state=13)
X = X.astype(np.float32)

offset = int(X.shape[0] * 0.9)

X_train, y_train = X[:offset], y[:offset]
X_test, y_test = X[offset:], y[offset:]

reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)
models, predictions = reg.fit(X_train, X_test, y_train, y_test)

print(models)
Hi vọng bài viết này sẽ giúp các bạn trong việc xử lí bài toán về Machine Learning",https://viblo.asia/p/lieu-ban-co-biet-bai-2-cach-xac-dinh-su-dung-ki-thuat-nao-trong-ml-5OXLAg1Y4Gr
32,Python Guru Series 🐍🐍🐍 - Part 6: Lập trình và quản lý task vụ bất đồng bộ với Celery,"Xin chào các bạn, tiếp nối chủ đề lập trình bất đồng bộ trong Python.
Trong part 6 của serie ngày hôm nay, mình xin được giới thiệu tới mọi người về Celery.
Một thư viện mạnh mẽ được sử dụng để quản lý, chạy các tác vụ bất đồng bộ và hàng đợi tin nhắn trong ứng dụng Python.
Ok mình cùng bắt đầu thôi.
1. Tổng quan về Celery
Khi phát triển các ứng dụng Python web, phần lớn thời gian chúng ta sẽ xử lý với các  vụ yêu cầu kết quả ngay lập tức (các task vụ đồng bộ). Chẳng hạn như tìm thông tin mà cô gái mà bạn thích, số dư tài khoản ngân hàng, etc.
Bên cạnh đó cũng có rất nhiều task vụ không cần yêu cầu trả về kết quả ngay, đây là các task vụ bất đồng bộ (asynchronous task) ví dụ như gửi email thông báo tới người dùng, export file, etc.
Đối với các task vụ bất đồng bộ thế này, người dùng không cần thiết phải đợi tới khi task vụ đó hoàn thành. Ý tưởng để giải quyết bài toán này đó là chúng ta sẽ đưa ra một nơi khác để thực thi nhằm tránh block luồng chính, tăng khả năng đáp ứng của ứng dụng.
Ở các task vụ đơn giản để cải thiện hiệu suất chúng ta có thể đưa task vụ đó ra backgroud thread để xử lý.  Nhưng nếu số lượng requests nhiều và lỗi xảy ra trong quá trình thực thi task thì làm sao để quản lý hiệu quả. Tệ hơn là bạn có thể sẽ chịu ảnh hưởng GIL (Global Interpreter Lock) chỉ cho phép 1 thread thực thi tại một thời điểm.
Một giải pháp thay thế là sử dụng task queue để cung cấp khả năng xử lý độc lập với server. Lúc này cũng chính là lúc Celery bước vào cuộc chơi .
Celery thường được sử dụng cho các tác vụ có async không yêu cầu kết quả ngay lập tức hoặc được sử dụng để chạy các tác vụ định kỳ (scheduled tasks).
Minh họa một hệ thống FastAPI sử dụng Celery để quản lý task queue với Redis đóng vai trò là message broker và là nơi store backend result.
2. Tính năng chính
Xử lý tác vụ bất đồng bộ: Celery cho phép thực hiện các tác vụ mà không cần chờ đợi chúng hoàn thành, giúp cải thiện hiệu suất của ứng dụng.
Hàng đợi tác vụ: Celery quản lý các hàng đợi tin nhắn (message queue) và điều phối việc thực thi các tác vụ  cho các worker từ các hàng đợi này.
Scheduled tasks: Celery hỗ trợ chạy các tác vụ định kỳ, tương tự như cron jobs. Với Celery Beat bạn có thể lập lịch cho task vụ chạy định kỳ trong một khoảng thời gian xác định. Chẳng hạn như tự động gửi tin nhắn chúc ngủ ngon cho người thương lúc 10 pm tối.
Khả năng mở rộng: Celery có thể mở rộng dễ dàng mở rộng theo chiều ngang bằng cách tăng số lượng worker để xử lý các tác vụ song song.
Retry và quản lý lỗi: Khi một tác vụ thất bại, Celery có khả năng tự động retry tác vụ đó hoặc retry theo như cấu hình được chỉ định.
3. Cấu trúc chính của Celery
Workers: Đây là các process chịu trách nhiệm lấy và thực thi các tác vụ từ hàng đợi.
Broker: Đứng giữa ứng dụng và worker là các message broker. Thành phần chịu trách nhiệm quản lý hàng đợi tin nhắn (message queue). Các broker phổ biến nhất được sử dụng là Redis hoặc RabbitMQ, Amazon SQS. Celery sử dụng các broker này để gửi các tác vụ đến worker xử lý.
Task: Đơn vị thực thi công việc trong Celery được gọi là task. Task sẽ được các phân phối vào các worker để xử lý.
Backend Result: Nơi được sử dụng để lưu trữ kết quả của các task khi hoàn thành trong các Database như Redis, Postgres, etc.
Celery là hệ thống quản lý task vụ phổ biến trong Python. Dễ dàng tích hợp với các web framework như Django, Flask, FastAPI để tạo ra một ứng dụng web hiệu năng cao.
Ngoài ra, chúng ta có thể sử dụng Flower một công cụ để giám sát và quản lý các worker, task vụ Celery một cách trực quan qua giao diện web.

4. Tổng kết
Trên đây là phần giới thiệu tổng quan nhất về Celery và những ứng dụng của nó trong phát triển các ứng dụng bất đồng bộ trong Python.
Hi vọng bạn đọc sẽ nhận được các thông tin hữu ích.
Again, mình là Phan một chàng developer tò mò và tận tâm.
Hẹn gặp các bạn trong những bài viết sắp tới.",https://viblo.asia/p/python-guru-series-part-6-lap-trinh-va-quan-ly-task-vu-bat-dong-bo-voi-celery-yZjJY376VOE
33,Blockchain là gì? Aptos là gì?,"Hello mọi người, mình là Jeremy, đại sứ Aptos tại Việt Nam. Rất vui được bắt đầu chuỗi serial Build Cùng Aptos.
Để bắt đầu bài viết đầu tiên, chúng ta sẽ đi qua khái niệm cơ bản.
Khái niệm
Từ những năm 2018 đến nay, blockchain vẫn là hot keyword. Nhiều người thần thánh hoá nó, nhưng chưa thực sự hiểu về nó.

Blockchain là một từ ghép gồm block và chain. Block là nơi chứa dữ liệu và dữ liệu lưu trữ chủ yếu trên blockchain, đó là các transaction - giao dịch. Người A gửi người B 500 đồng - đó là một giao dịch. Dữ liệu đó sẽ ghi nhận vào block. 1 block sẽ có giới hạn chứa được bao nhiêu dữ liệu, khi tới giới hạn, 1 block mới sẽ được tạo ra , trong block mới có chứa mã hash của block trước. Các block này được liên kết với nhau tạo thành chain - mạng lưới. Bạn muốn thêm data trên blockchain, bạn cần có sự chấp thuận từ các validator trên mạng lưới (network) của hàng nghìn máy tính để xác thực tính hợp lệ của dữ liệu. Các dữ liệu lưu trên blockchain sẽ được mã hoá và lưu trữ trên nhiều máy tính (có nhiều bản copy trên toàn thế giới) và hoàn toàn không thể sửa đổi sau khi được công nhận.
Vậy tại sao blockchain sinh ra? Hay các lợi ích của blockchain? Điểm cốt lỗi của blockchain là gì?
Lợi ích
Tính minh bạch, bảo mật cao: Mọi giao dịch trên blockchain đều được ghi lại công khai và không thể xóa bỏ. Dữ liệu trên blockchain được mã hóa và phân tán trên hàng nghìn máy tính. Điều này tạo ra một lớp bảo mật cực kỳ cao, khiến việc giả mạo hoặc xóa bỏ thông tin trở nên gần như bất khả thi. Nhờ đó, blockchain đảm bảo tính minh bạch, độ tin cậy và an toàn cho mọi giao dịch.
Tính minh bạch, bảo mật cao: Mọi giao dịch trên blockchain đều được ghi lại công khai và không thể xóa bỏ. Dữ liệu trên blockchain được mã hóa và phân tán trên hàng nghìn máy tính. Điều này tạo ra một lớp bảo mật cực kỳ cao, khiến việc giả mạo hoặc xóa bỏ thông tin trở nên gần như bất khả thi. Nhờ đó, blockchain đảm bảo tính minh bạch, độ tin cậy và an toàn cho mọi giao dịch.
Tiết kiệm chi phí, giao dịch nhanh, đơn giản: Blockchain đã cách mạng hóa cách chúng ta chuyển tiền quốc tế. Thường bạn chuyển tiền từ Mỹ về Việt Nam có thể qua ngân hàng với một mức phí cao 5-10%, thời gian chờ phải vài ngày. Hay bạn có thể thông qua bên thứ 3, thời gian nhanh hơn, mức phí có thể rẻ hơn nhưng rủi ro có thể bị mất tiền nếu gặp bên lừa đảo. Với blockchain, không cần thông qua một bên thứ 3 nào cả, bạn có thể ở bất kỳ đâu trên thế giới chỉ cần kết nối internet có thể chuyển tiền cho bạn dù cách cả nước vòng Trái Đất với một vài thao tác đơn giản cùng mức phí vô cùng rẻ chỉ trong vòng vài giây (như Aptos có thể chưa tới $0.00001, chưa tới 1 giây).
Tiết kiệm chi phí, giao dịch nhanh, đơn giản: Blockchain đã cách mạng hóa cách chúng ta chuyển tiền quốc tế. Thường bạn chuyển tiền từ Mỹ về Việt Nam có thể qua ngân hàng với một mức phí cao 5-10%, thời gian chờ phải vài ngày. Hay bạn có thể thông qua bên thứ 3, thời gian nhanh hơn, mức phí có thể rẻ hơn nhưng rủi ro có thể bị mất tiền nếu gặp bên lừa đảo. Với blockchain, không cần thông qua một bên thứ 3 nào cả, bạn có thể ở bất kỳ đâu trên thế giới chỉ cần kết nối internet có thể chuyển tiền cho bạn dù cách cả nước vòng Trái Đất với một vài thao tác đơn giản cùng mức phí vô cùng rẻ chỉ trong vòng vài giây (như Aptos có thể chưa tới $0.00001, chưa tới 1 giây).
Giao dịch không giới hạn thời gian, địa điểm: Blockchain hoạt động không ngừng nghỉ, cho phép bạn thực hiện giao dịch bất cứ lúc nào, ở bất kỳ đâu trên thế giới. Không còn phải lo lắng về các vấn đề liên quan đến múi giờ hay thời gian làm việc của ngân hàng. Điều này đặc biệt hữu ích cho các giao dịch quốc tế, giúp rút ngắn thời gian thanh toán và tăng tính thuận tiện.
Giao dịch không giới hạn thời gian, địa điểm: Blockchain hoạt động không ngừng nghỉ, cho phép bạn thực hiện giao dịch bất cứ lúc nào, ở bất kỳ đâu trên thế giới. Không còn phải lo lắng về các vấn đề liên quan đến múi giờ hay thời gian làm việc của ngân hàng. Điều này đặc biệt hữu ích cho các giao dịch quốc tế, giúp rút ngắn thời gian thanh toán và tăng tính thuận tiện.
Ứng dụng
Ứng dụng của blockchain rất nhiều:
Tài chính: Thanh toán, chuyển tiền, cho vay, quản lý tài sản,...
Logistics: Theo dõi hàng hóa, quản lý chuỗi cung ứng.
Bất động sản: Quản lý quyền sở hữu, giao dịch bất động sản.
Y tế: Quản lý hồ sơ bệnh án, bảo mật dữ liệu y tế.
Chính phủ: Xác thực danh tính, bỏ phiếu điện tử.
Khái niệm

Aptos là một non-evm blockchain layer 1, nổi bật với
tốc độ xử lý nhanh, xử lý đồng thời nhiều giao dịch
ổn định kể cả khi có lượng traffic lớn
tính bảo mật cao, khả năng mở rộng lớn
phí gas rẻ mà vẫn đảm bảo tính phi tập trung
Sau đây là các con số vô cùng ấn tượng Aptos đã đạt được kể từ khi mainnet vào tháng 10 năm 2022:
Mạng lưới đã đạt mức TPS cao nhất với khả năng xử lý 13,300 giao dịch mỗi giây. Nắm giữ vị trí top đầu trong thế giới blockchain.
Có 3,9 triệu người dùng hoạt động hàng tháng.
Đã xử lý hơn 1,7 tỷ giao dịch.
Mạng lưới ngày càng phi tập trung với tổng cộng 400 Nodes và 147 Validator Nodes.
Các giao dịch thành công trong vòng 1 giây

Aptos đạt được hiệu suất cao nhờ vào việc sử dụng ngôn ngữ lập trình Move được thiết kế riêng cho các hệ thống blockchain. Move giúp các nhà phát triển viết code an toàn và hiệu quả hơn. Bên cạnh đó, cơ chế đồng thuận BFT và các công nghệ như Block-STM, MoveVM cũng đóng góp vào việc tăng cường khả năng mở rộng và bảo mật của mạng lưới.
Team
Đội ngũ của Aptos gồm các cựu thành viên của dự án Diem Blockchain  và các chuyên gia đến từ Google, Meta, Blackrock, Robinhood, Huobi, JP Morgan… Aptos còn có 2 co-founder Mo Shaikh và Avery Ching. Hiện Mo nhiệm làm cố vấn cho Uỷ ban Giao dịch hàng hóa Tương lai Hoa Kỳ (CFTC) vào tháng 05/2024


Aptos đã được đầu tư từ các quỹ đầu tư mạo hiểm lớn hàng đầu trong lĩnh vực tiền điện tử như Coinbase Venture, Binance Labs, Circle, Multicoin Capital, Jump Crypto, a16z, … (https://cryptorank.io/ico/aptos)

Bên cạnh đó, Aptos còn có sự hợp tác với các grant company - Google, Microsoft, SKT, Alibaba, ...



Và gần đây, USDT - đồng tiền stable coin lớn nhất thế giới, đã được đưa lên mạng lưới Aptos. Sự hợp tác chiến lược này đánh dấu một cột mốc quan trọng trong việc mở rộng hệ sinh thái của cả hai nền tảng. Điều này không chỉ củng cố vị thế của Aptos trong thị trường blockchain mà còn mở ra những cánh cửa mới cho sự phát triển của các ứng dụng tài chính phi tập trung.

Hệ sinh thái

Hệ sinh thái của Aptos đã có tương đối đầy đủ với hơn 200+ dApps, nhưng vẫn cần có dự án giàu sức sáng tạo để đem lại những trải nghiệm mới mẻ và đột phá cho người dùng. Chính vì thế, Aptos còn có các chương trình cung cấp grant khủng , và hiện nay còn có chương trình hackathon để thu hút thêm nhiều nhà phát triển tài năng tham gia xây dựng hệ sinh thái. Chương trình hackathon đang diễn ra (đến hết tháng 9) với giải thưởng lớn lên đến $500.000 dành cho những dự án xuất sắc nhất. Hãy tham gia ngay và giành giải nhé các bạn.

Tài liệu trích dẫn
Aptos Documents",https://viblo.asia/p/blockchain-la-gi-aptos-la-gi-38X4E8qoVN2
34,Types of Database Encryption in SQL Server,"SQL Server offers several encryption options to protect sensitive data. Here are the main types of database encryption in SQL Server:
Transparent Data Encryption (TDE)
Transparent Data Encryption (TDE) is a feature introduced in SQL Server 2008 that encrypts the entire database at the file level. It protects data at rest, including data and log files, without requiring changes to the application. TDE uses a database encryption key (DEK) to encrypt the data files and a server certificate or asymmetric key to protect the DEK.
To enable TDE:
Create a database master key and a certificate protected by the master key
Create a database encryption key (DEK) protected by the certificate
Set the database to use encryption
CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'strong_password';
CREATE CERTIFICATE MyServerCert WITH SUBJECT = 'My Database Encryption Key Certificate'; 
CREATE DATABASE ENCRYPTION KEY
WITH ALGORITHM = AES_256
ENCRYPTION BY SERVER CERTIFICATE MyServerCert;
ALTER DATABASE MyDatabase
SET ENCRYPTION ON;
Always Encrypted
Always Encrypted is a feature introduced in SQL Server 2016 that allows client applications to encrypt sensitive data and never reveal the encryption keys to SQL Server. It protects data at rest and in motion. Always Encrypted supports deterministic and randomized encryption modes.
To use Always Encrypted:
Configure the application to use Always Encrypted
Create encrypted columns in the database
Use SqlParameter objects to pass encrypted data to SQL Server
string connString = ""Server=database.example.com;Database=Clinic;Column Encryption Setting=enabled"";
using (SqlConnection conn = new SqlConnection(connString)) {
    SqlCommand cmd = conn.CreateCommand();
    cmd.CommandText = ""SELECT FirstName, LastName, BirthDate FROM Patients WHERE SSN = @SSN"";
    SqlParameter paramSSN = cmd.CreateParameter();
    paramSSN.ParameterName = ""@SSN"";
    paramSSN.Value = ""795-73-9838"";
    cmd.Parameters.Add(paramSSN);
    using (SqlDataReader reader = cmd.ExecuteReader()) {
        while (reader.Read()) {
            Console.WriteLine(""{0}, {1}, {2}"", reader[0], reader[1], ((DateTime)reader[2]).ToShortDateString());
        }
    }
}
Column-level Encryption
Column-level encryption allows you to encrypt individual columns in a table using symmetric keys. It provides a higher level of granularity compared to TDE. Column-level encryption uses the EncryptByKey and DecryptByKey functions.
To encrypt a column:
Create a database master key and a symmetric key
Open the symmetric key
Encrypt the column data using EncryptByKey
Close the symmetric key
CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'strong_password';
CREATE SYMMETRIC KEY SymKey_test WITH ALGORITHM = AES_256 ENCRYPTION BY MASTER KEY;

OPEN SYMMETRIC KEY SymKey_test DECRYPTION BY MASTER KEY;

UPDATE CustomerData.dbo.CustomerInfo
SET BankACCNumber_encrypt = EncryptByKey(Key_GUID('SymKey_test'), BankACCNumber);

CLOSE SYMMETRIC KEY SymKey_test;
In summary, SQL Server offers several encryption options to protect sensitive data at different levels - from encrypting the entire database to masking individual columns. The choice depends on the specific security requirements and performance considerations.
Reference URL :
https://github.com/MicrosoftDocs/sql-docs/blob/live/docs/relational-databases/security/encryption/sql-server-encryption.md
https://techdocs.broadcom.com/us/en/ca-enterprise-software/business-management/ca-service-management/17-3/administering/administering-ca-service-management/transparent-data-encryption-for-ca-service-management/enabling-transparent-data-encryption-for-microsoft-sql-server-database.html",https://viblo.asia/p/types-of-database-encryption-in-sql-server-5OXLAgOx4Gr
35,7 vấn đề thường gặp và giải pháp xây dựng Git Workflow,"Có thể nói Git workflow là xương sống của một team phát triển, hầu hết các hoạt động của team đều gắn liền với git workflow và align với chu trình phát triển. Do git chỉ là công cụ để quản lý version code, nó không quy định bất cứ cách vận hành team hay tổ chức nào phải gắn liền. Teamlead hoặc CTO cần xác định cách vận hành của team mình, cùng với đó là mô hình các nhánh (Branching model) để phục vụ nhu cầu phát triển. Từ đó giảm thời gian quản lý code và các vấn đề gây xung đột có thể làm giảm performance của team.
Từ khi CI/CD phát triển, tiến trình deploy ứng dụng có thể được tiến hành tự động ngay sau khi code được merge vào nhánh và ánh xạ tới môi trường cần thiết. Đây thực sự là một bước tiến vượt bậc nếu so sánh với việc build và deploy ứng dụng một cách manual như trước đây. Điều này dẫn đến các kịch bản CI/CD cần được thiết kế phù hợp với các môi trường, mục đích và nhánh hiện có. Mỗi tổ chức sẽ có cách đặt tên nhánh khác nhau, nhưng các process sẽ không nằm ngoài các cycle phát triển của team:
New feature/implement task : Developer thực thi các task (code) trên một nhánh độc lập, và môi trường thường là local.
Integration: Phase tích hợp, thường align với một môi trường hoàn chỉnh, các task branch sau khi được review sẽ được merge vào development branch để đưa vào tích hợp cùng hệ thống. Một tính năng sẽ chưa được nghiệm thu là CI deploy nếu nó không được tích hợp chung lên hệ thống. Đôi khi môi trường feature/task và Intergration có thể bị dùng chung với nhau do trong hệ thống có một số ứng dụng đặc thù, ứng dụng cloud... khiến việc phát triển hoàn toàn trên môi trường local là không thể.
Testing and fixbug : Sau khi đã tích hợp và hoạt động trên môi trường tích hợp, thường với ý nghĩa các task đã được tích hợp và hoạt động, các thay đổi này sẽ được chuyển sang môi trường test cho QA kiểm thử. Môi trường này cần tách biệt và được quản lý deployment phục vụ quá trình kiểm thử của QA.
User acceptance Testing - UAT : Sau khi pass qua phase kiểm thử và fix bug, code được merge sang nhánh UAT, tương đương với môi trường UAT. Môi trường này mang ý nghĩa là môi trường sát với production nhất, code đã được kiểm thử, không còn hoặc chỉ còn các issue nhỏ, sẵn sàng bàn giao cho khách hàng. User (người dùng cuối hoặc đại diện khách hàng) sẽ sử dụng môi trường này và confirm/đánh giá các tính năng đang được bàn giao. Nếu có bất cứ thay đổi hoặc sự không phù hợp nào sẽ được xác nhận từ môi trường này, tránh việc lên tới production mới phát hiện.
Release production: Sau khi vượt qua UAT, code được lưu trữ và triển khai lên môi trường production.
Trên đây là những phase phát triển chính của team, nhưng trên thực tế, code không đi theo một chiều như vậy. Sẽ luôn có khả năng code xảy ra lỗi trong bất cứ môi trường nào, bất kể là đang In QA hay Production. Vậy khi bắt đầu một sprint, code sẽ bắt đầu sinh ra từ đâu, được merge vào đâu sau mỗi phase phát triển? Và khi có phát sinh bug/issue trong phát triển và vận hành, developer cần làm như thế nào để đảm bảo các issue được fix  mà không ảnh hưởng tới các phase hoặc sprint tiếp theo.
Nhìn từ góc nhìn của developer, các công việc trải qua trong quá trình phát triển có liên quan đến git:
Phát triển feature/task
Fix bug trên môi trường QA test
Fix bug trên môi trường UAT
Fix bug trên môi trường production (hotfix)
""Về cơ bản cuộc sống của dev là tạo ra lỗi và đi sửa lỗi 🙂""
Những công việc này cần có code base để bắt đầu tiến hành code feature hoặc fixbug. Trước khi đi vào xây dựng một workflow đáp ứng các yêu cầu phát triển, ta cùng đi qua một số vấn đề liên quan đến Git mà developer có thể gặp phải.

Backward compatible
Nhìn tổng thể trong một sprint, hầu hết quá trình là tiến trình đi lên song song giữa các nhánh code và môi trường theo thứ tự đã nêu ở các cycle phát triển. Với phase implement task và integration, việc quản lý code khá thoải mái, develop có thể chủ động push code lên nhánh của cá nhân để lưu trữ hoặc request merge vào môi trường tích hợp để thực hiện dev test tính năng. Nhưng từ phase testing trở đi, code cần được quản lý chặt chẽ, vì thường các nhánh code này ánh xạ trực tiếp tới các môi trường ứng dụng đang vận hành. Việc merge code tự do sẽ làm ảnh hưởng tới downtime của ứng dụng hoặc development cycle (do môi trường đang được sử dụng bởi team khác, vd QA đang verify bug).Với happy case, mọi thứ sẽ đi lên dần theo các nhánh và môi trường. Nhưng điều gì sẽ xảy ra khi có issue xảy ra trên môi trường Testing/UAT/Production. Issue cần triển khai fix theo nguyên lý: Issue xảy ra ở môi trường nào, code fix phải đảm bảo có mặt trên môi trường đó và cả các môi trường phía trước. Điều này đảm bảo code trên các môi trường runtime luôn được cập nhật các bản vá cho các tính năng, không gây rủi ro cho các tính năng khác và hệ thống.
Lấy ví dụ nếu bug xảy ra ở UAT trên một tính năng đang phát triển, nó cần được fix và merge cả vào Integration/Testing/UAT, còn các nhánh feature của cá nhân có thể định kì pull các thay đổi từ nhánh đã base code.
Kiểm thử nhiều phiên bản code trên cùng một môi trường
Thông thường một team chỉ có 2-3 môi trường để phục vụ mục đích phát triển, điều này dẫn đến việc đôi khi developer cần thực hiện tích hợp hoặc kiểm thử tính năng khác nhau trên cùng một môi trường.
Các tính năng này có thể có những khác biệt lớn khiến chúng không thể tích hợp cùng nhau, mà cần được kiểm thử độc lập mà không làm ảnh hưởng đến member khác. Lúc này cần có kịch bản để thực hiện việc triển khai các tính năng khác nhau lên cùng một môi trường. Việc chuyển đổi qua lại giữa các nhánh tính năng khác nhau trên cùng môi trường không khó, nhưng nó thường gây ra những xung đột về mặt dữ liệu hoặc việc giao tiếp thông báo phiên bản nào đang on air không xuyên suốt giữa team dev và QA. Nhất là với hệ thống microservice được tạo thành từ nhiều dịch vụ nhỏ.
Remove một tính năng ra khỏi sprint
Không phải tất cả các sprint đều diễn ra suôn sẻ theo lý tưởng: manager cùng với member estimate một cách hợp lý, các requirement được làm rõ ràng. Tình trạng các task bị chậm tiến độ có thể đến từ rất nhiều nguyên nhân, lúc này thì team sẽ ứng xử như thế nào: Giải trình để xin giãn deadline, OT cho kịp deadline, hay giữ nguyên deadline và chấp nhập skip qua một vài task. Với các option giãn deadline và OT sẽ không ảnh hưởng tới cách vận hành mà chỉ ảnh hưởng đến vấn đề thời gian. Nhưng việc skip một tính năng đã được merge vào môi trường Test hoặc UAT sẽ cần được thực hiện theo nguyên tắc giảm đối đa sự ảnh hưởng của việc remove code ra khỏi nhánh Việc này có thể ảnh hưởng tới code của member khác và effort của QA đã test những tính năng trước đó.
Tưởng tượng môi trường Test lúc này là một mớ các feature/bugfig, việc comment code không phải là phương án đảm bảo (nếu không muốn nói là tệ . Phương án hợp lý sẽ là remove các commit chứa feature/task đó ra khỏi nhánh hiện tại.
Multiple repository or Mono Repository
Xét trên khía cạnh quản lý code của một team khi phát triển ứng dụng, với việc áp dụng kiến trúc microservice, số lượng các thành phần trong hệ thống trở lên tương đối lớn. Có thể có từ vài service đến hàng chục service chỉ xét riêng cho phần code business. Việc lưu trữ code theo hướng Multiple và Mono sẽ mang lại những ưu nhược điểm khác nhau.
Về cơ bản việc lưu trữ multiple repo có nhiều ưu điểm hơn về mặt isolation các repo, giảm độ phụ thuộc về code và cải thiện thời gian build do target vào đúng repo cần build lại. Nhưng nhược điểm sẽ đến từ việc thao tác và quản lý version code qua nhiều repo. Cần có phương án cho việc quản lý version code hợp lý để sẵn sàng cho việc rollback hoặc tracing khi cần thiết.
Mono repository thì ở chiều hướng ngược lại, nếu hệ thống không có quá nhiều member và scope phát triển không lớn, việc tập trung code vào cùng một repo sẽ giúp đơn giản hóa về mặt quản lý và thao tác. Các thay đổi giữa tất cả các sub project cũng được đồng bộ với nhau trong cùng một commit, cùng một nhánh. Lúc này một nhánh đã bao hàm tất cả các chức năng của ứng dụng.
Quản lý version code và gắn tag
Trong quá trình CICD,khi deploy một ứng dụng, sẽ có một mã đi kèm để làm định danh (name + version) cho bản artifact được build. Bản artifact này có thể được lưu trữ trực tiếp hoặc đóng gói vào trong image để phục vụ cho phase deploy. Với các bản build thông thường cho mục đích dev/test/QA thì định danh này không quá quan trọng, có thể sử dụng các mã random hoặc commit id. Nhưng với các nhánh code đã ổn định về tính năng, các bản build (artifact) hoặc image sẽ cần gắn một mã có chủ đích (version/tag) với mục đích lưu trữ lâu dài và quản lý được các phiên bản build và phiên bản code phục vụ cho rollback / track change/ specific version... Version này cũng nên được đồng bộ vào trong code, tại các file metadata của các ngôn ngữ phát triển khác nhau (pom.xml, package.json...). Lúc này khi bạn nhìn từ bất cứ view nào từ code, file build hay runtime cũng có thể biết được phiên bản hiện tại của code/ứng dụng.

Vậy nên gắn tag cho các version như thế nào? Dễ thấy việc gắn nhãn cho các phiên bản không bó buộc theo một công thức nào cả. Một cách gắn nhãn phổ biến trên các repo mà ta thấy trên github là Semantic Versioning với công thức đơn giản là một dãy 3 số đại diện cho mức độ thay đổi của mỗi phiên bản <MAJOR.MINOR.PATCH> và một số hậu tố khác đại diện cho độ ổn định.
Xem thêm về Semantic Versioning tại đây
Ngoài ra còn có một số strategy đặt tên version phổ biến khác:
Sequential numbering: Phiên bản theo thứ tự tăng dần
Date base: Gắn nhãn liên quan đến khoảng thời gian phát triển hoặc release (I like this one 😃)
Commit convention và tích hợp git với các ứng dụng CICD khác
Có một câu chuyện vui về commit code như thế này.
*Quy trình xảy ra khi có chuông báo cháy trong một công ty IT: Git add  => Git commit => Git push => Ra khỏi tòa nhà. *
Đây chỉ là trong trường hợp khẩn cấp, trên thực tế chúng ta cần commit với các message có nghĩa, ít nhất member khác có thể track lại commit đó có ý nghĩa gì mà không cần phải vọc vào code. Cách thực hiện commit này được gọi là Commit convention. Một quy ước chung của team và tổ chức nhằm thống nhất cách đặt tên để tiện quản lý và phục vụ các nhu cầu CICD tích hợp. Rất nhiều ứng dụng trong quy trình CICD sử dụng commit convention để trigger các sự kiện một cách tự động. Như việc generate change log tự động sau mỗi release, tự động chuyển trạng thái của task trên Jira sau khi commit được deploy... Tất cả những việc này đều phụ thuộc vào commit convention, mà chủ yếu nằm ở việc viết message khi commit.
Điểm qua một số commit:
git commit: Commit không có message, ngay cả người thực hiện khi nhìn lại lịch sử thay đổi cũng ko biết commit này thực hiện gì
git commit -m""Support login 2FA"": khá hơn commit trước là đã có thể biết được commit này phục vụ chức chăng Two factor authentication
git commit -m""fix: bug mail not send 2FA"": Biết thêm được đây là commit phục vụ fix bug
git commit -m""fix:[JIRA-ID] Support login 2FA, bug mail not send"": Commit fixbug theo task số <JIRA-ID>
Có thể thấy Commit convention là một quy ước quan trọng trong phát triển phần mềm. Nó thể ảnh hưởng đến performance khi thực hiện phát triển hoặc cả khi maintain cần tracing lại lịch sử các commit và tính năng, thậm chí là keypoint liên kết quá trình CICD.
Conflict do format code
Vấn đề này thường ít được để ý khi người quản lý setup workflow và convention cho team. Cùng nhìn vào một trường hợp mà mình đã gặp phải trong một dự án vài năm trước.
Team phát triển backend sử dụng Java Spring, lúc đó người thì dùng Eclipse, người thì Netbean, người thì Intellij. Lúc đầu chia mỗi người một nghiệp vụ, mọi người không code chung trong một file nên không có vấn đề gì, sau một thời gian ở một file mà người cũ đã code khoảng 2000 dòng, một member mới code cùng file đó, và rất tự nhiên, khi code xong người đó format code cả file với phím tắt và commit lên. Lúc này theo lịch sử thay đổi git không chỉ còn là phần code của người đó mới thêm vào mà là gần như toàn bộ file do khác biệt về format code.
Nếu hai người cùng checkout từ một commit, sau đó format code với hai format khác nhau trên cùng một file, lúc này việc merge lại sẽ xảy ra conflict trên phạm vi rất lớn, mặc dù thao tác đều đúng. Liệu có thể chỉ format đoạn code mình đã viết, không việc đó rất bất tiện, và cũng không thể buộc member sử dụng cùng một IDE. Vậy giải pháp nào cho việc đồng bộ format code giữa các IDE khác nhau mà thuận tiện nhất cho developer, tránh xảy ra tình trạng đã nêu ở trên.
Bản thân mình đã trải qua những issue của git khá nhiều lần. Những lần mất code làm mất thời gian trace lại lịch sử, hay việc conflict code quá nhiều khiến mất nhiều thời gian ngồi resolve conflict, hay việc phải OT để fix bug do đã lỡ merge một tính năng vào mà không biết remove nó một cách an toàn ra sao... Nên từ đó team đã họp và tìm các phương án để xác định workflow hiệu quả.
Một Git workflow hiệu quả là workflow không chỉ giảm được các thao tác của developer, giảm rủi ro do con người, cùng với đó là đảm bảo sự hợp tác giữa các role phát triển không xảy ra xung đột trong một quy trình phát triển. Nó còn cần tối ưu về mặt thao tác, productivity nhất. Cùng xem xét một số option cho việc xây dựng mô hình git workflow.
Ứng dụng một số git workflow build in
Centralized workflow: Mọi người đều làm việc xung quanh nhánh main, cũng là nhánh trung tâm duy nhất của hệ thống. Các member đều có một nhánh main trên local remote tới nhánh main trên git server. Khi có các thay đổi, mỗi member sẽ pull nhánh main về, resolve conflict và sau đó tự push code của mình lên nhánh main. Dễ thấy workflow này có nhiều nhược điểm về mặt collaboration giữa các member.
Feature Branching: Ý tưởng ở đây là mỗi feature/task nên có cho riêng mình một nhánh thay vì code tất cả mọi thứ trên cùng một nhánh. Cách này giúp các member có thể phát triển feature mà không làm ảnh hưởng đến nhánh code base hoặc code của member khác
Gitflow Workflow: Flow này được thiết kế với định hướng xoay quanh và bám sát các phase phát triển của project release: feature/fixbug/hotfix... Workflow này là một giải pháp out-of-box, có cả một extention của git (git-flow) hỗ trợ workflow này. Về cơ bản người dùng sẽ setup các nhánh của mình align với các nhánh theo concept của gitflow. Sau đó thực hiện đúng các bước khi thao tác với mỗi kiểu task (new feature, fixbug, hotfix...). Lúc này extention sẽ tối giản hóa thao tác của người dùng nhưng vẫn đảm bảo code được merge vào đúng nơi cần đến theo mỗi kiểu task.
Đọc thêm về Git Workflow tại đây
Tự define và implement cho phù hợp
Từ việc tham khảo các workflow có sẵn cộng với những tổng kết về các issue thường gặp với git trong quá trình làm việc, các bạn có thể xây dựng nên Git workflow cho team mình, bao gồm cả xây dựng quy trình và các convention làm việc cần thiết. Cho đến hiện tại tôi cảm thấy git workflow của mình khá phù hợp cho việc phát triển theo agile và giải quyết được hầu hết các issue một cách thuận tiện cho dev. Vì việc setup này khá dài nên xin hẹn các bạn ở một bài viết khác.
Đến đây chúng ta đã hiểu tầm quan trọng của một git workflow hiệu quả ảnh hưởng tới performance và quy trình phát triển ứng dụng như thế nào. Hi vọng bài viết này có thể giúp các bạn hiểu thêm về git workflow và các issue về git mà các bạn đang gặp phải. Ở bài viết sau chúng ta sẽ xây dựng một workflow đáp ứng quy trình phát triển Agile và CI/CD một cách chi tiết nhất.
Đón đọc: Setup git workflow full option in Real Life",https://viblo.asia/p/7-van-de-thuong-gap-va-giai-phap-xay-dung-git-workflow-pgjLNvydL32
36,"Design Pattern cùng Flutter. Tập 7: State - ""Siêu nhân biến hình""","Giới thiệu
Giả sử như hôm nay bạn có tiền và bắt đầu đi rút tiền ở một cây ATM, bạn sẽ thấy các trạng thái đơn giản như: chờ đút thẻ vào atm, nhập mật khẩu, thao tác rút tiền, nhận tiền và trả thẻ. Đối với mỗi trạng thái đều có thể chuyển sang trạng thái tiếp theo dựa vào trạng thái hiện tại. Đó gọi là biết mình biết ta, trăm trận toàn tạch 🤡.
Xời, với những trạng thái đơn giản như thế thì chỉ cần if else là xong nhỉ:
If cây ATM còn tiền thì cho rút tiền else thông báo là bạn nghèo rồi
If đút thẻ đúng thì cho vào else thì đá nó ra
If nhập mật khẩu đúng thì chuyển sang thao tác rút tiền ngược lại else bắt nhập lại mật khẩu
Vẫn còn chịu được, ơ thế giờ lỡ thêm 1 trạng thái else nữa thì sao nhỉ:
If nhập mật khẩu đúng thì chuyển sang rút tiền else if nhập 5 lần sai sẽ bắt nhập lại mật khẩu else đá thẻ ra và thông báo đi trả thẻ cho người khác đi =))
Lúc này thấy là rối rối rồi đúng không?  Việc tổ chức theo dạng condition thì cả mình và dev khác vào đọc sẽ tẩu hoả nhập ma mất =)))) Chưa kể đến các business logic rắc rối và phức tạp như thế nào. Vậy ra, lúc này phải cần một mẫu thiết kế thôi, điển hình cho mẫu thiết kế này được gọi tên: State - ""Siêu nhân biến hình"".
Vậy...
State là ai, địa chỉ nhà ở đâu?

State là một loại mẫu thiết kế thuộc behavioural  được ra đời để đóng gói lại các logic của các State class và thay đổi hành vi của nó tuỳ vào ngữ cảnh đang xảy ra. Tất nhiên là nhờ vậy mà việc tạo mới một State class dễ dàng và rõ ràng hơn.
Thế...
Mục tiêu là gì, tại sao nó lại tồn tại
Mẫu thiết kế State ra đời với mục tiêu giống y đúc mẫu thiết kế Strategy (thật ra là base từ Strategy), nhưng khác biệt to lớn nhất ở đây là Strategy sẽ lựa chọn thuật toán ổn định và ít sửa đổi, còn State linh hoạt hơn với sự thay đổi trạng thái, nên tuỳ vào mục đích sử dụng để có thể chọn thằng nào. Chung quy Strategy sẽ ""toàn năng"" nhưng thiếu ""biến hình"" như State 👻.
State Class Diagram

Cách tiếp cận tổng quát của mẫu thiết kế State được biểu diễn bằng sơ đồ lớp bên dưới:

Các thành phần chính:
State: Interface hoặc abstract class đại diện cho một lớp thành phần, định nghĩa ra phương thức dùng chung cho các ConcreteStates. Tuyệt đối không nên định nghĩa ra phương thức dùng chung mà tồn tại state không sử dụng nó.
ConcreteStates: Là lớp kế thừa State  sẽ biểu diễn cụ thể state. Mỗi class sẽ triển khai gắn liền với Context, tham chiếu đến Context và có thể lấy thông tin hoặc chuyển đổi trạng thái của state được khai báo trong Context (thông qua hàm setState).
Context: Duy trì một instance của ConcreteStates, nên được khai báo như một Singleton (nếu chưa biết về Singleton có thể tham khảo bài viết này) . Tham chiếu đến lớp interface State và sử dụng phụ thuộc vào các ConcreteStates. Lưu ý là Context sẽ không biết chi tiết về các ConcreteStates,  sẽ cung cấp một phương thức setState để thay đổi trạng thái hiện tại từ các lớp ConcreteStates.
Ứng dụng
Mẫu thiết kế state được sử dụng nếu thấy đối tượng đó có nhiều trạng thái và có nhiều sự thay đổi tuỳ thuộc vào trạng thái hiện tại của nó. Bằng cách đóng gói mỗi trạng thái của nó thành những class riêng biệt thì việc thêm trạng thái mới không còn khó khăn mà lại không ảnh hưởng đến các trạng thái khác, quá dữ. Ý tưởng của mẫu thiết kế này đã tuân thủ một số quy tắc trong SOLID: chữ S (Mỗi class chỉ chịu một trách nhiệm và được đóng gói trong lớp của nó) và chữ O (Dễ dàng mở rộng và hạn chế sửa đổi). Hầu như logic nào có nhiều trạng thái khác nhau và có thể tăng thêm số lượng trạng thái thì đều có thể triển khai bằng mẫu thiết kế State. Ví dụ thực tế: Khi quản lý dự án sẽ cần theo dõi trạng thái của những tasks: New, InProgress, Done, Pending, Closed,..., thực tế của thực tế hơn nữa mà ngày nào cũng gặp, bạn sẽ thấy cách hiển thị màu sắc của đèn giao thông cũng sẽ phải thông qua màu sắc hiện tại hoặc là ngay hiện tại khi mình đang viết bài viết này, nó cũng đang ở trạng thái lưu nháp, viết xong sẽ Xuất bản bài viết,..
Thực hành

Lấy đại một ví dụ ở trên thử luôn cho nóng nhé, để mình random xem như thế nào:
Tada, vậy là mình sẽ phải theo sự sắp đặt ở trên thôi (ai gato sẽ bảo mình sắp xếp kịch bản 😗) . Thế ta cùng đi vào sử dụng mẫu thiết kế State để quản lý đèn giao thông nhé.
Để phân tích bước đầu tiên dựa vào class diagram trên, ta cần một thằng ITrafficLightState đóng vai trò là một interface chứa phương thức nextTrafficLight(context) , để thêm màu mè dễ phân biệt ta sẽ thêm getColor() để get màu của một cái đèn và getDuration() để lấy ra thời gian tồn tại của cái đèn đó và next sang cái đèn tiếp theo.
abstract class ITrafficLightState {
  void nextTrafficLight(TrafficLightContext context);
  Color getColor();
  Duration getDuration();
}
Để dễ dàng tưởng tượng hơn, ta khai báo lớp context trước, ở đây là TrafficLightContext, sẽ sử dụng interface ITrafficLightState để thay đổi trạng thái bằng hàm setState(state) và có thể chuyển được sang trạng thái tiếp theo. À cũng đừng quên lấy ra currentColor và cả currentDuration nữa nhé. Tổng quan ta sẽ được như sau:
class TrafficLightContext {
  ITrafficLightState _state;

  TrafficLightContext(this._state);

  Color get currentColor => _state.getColor();
  Duration get currentDuration => _state.getDuration();

  void setState(ITrafficLightState state) {
    _state = state;
  }

  void nextTrafficLight() {
    _state.nextTrafficLight(this);
  }
}
Tiếp đến, ta xây dựng các concreteStates: đèn xanh, đèn vàng và đèn đỏ. Ở mỗi state, nó sẽ biết mình là ai và sẽ chuyển sang state nào mong muốn:
GreenState
class GreenState extends ITrafficLightState {
  @override
  void nextTrafficLight(TrafficLightContext context) {
    context.setState(YellowState());
  }

  @override
  Color getColor() {
    return Colors.green;
  }

  @override
  Duration getDuration() {
    return const Duration(seconds: 15);
  }
}
RedState
class RedState implements ITrafficLightState {
  @override
  void nextTrafficLight(TrafficLightContext context) {
    context.setState(GreenState());
  }

  @override
  Color getColor() {
    return Colors.red;
  }

  @override
  Duration getDuration() {
    return const Duration(seconds: 20);
  }
}
YellowState
class YellowState extends ITrafficLightState {
  @override
  void nextTrafficLight(TrafficLightContext context) {
    context.setState(RedState());
  }

  @override
  Color getColor() {
    return Colors.yellow;
  }

  @override
  Duration getDuration() {
    return const Duration(seconds: 3);
  }
}
Cuối cùng, là phần hiển thị: Để đơn giản hoá việc hiển thị, ta cần tạo 1 hình tròn chứa màu của đèn giao thông và hiển thị ra thời gian tồn tại của đèn hiện tại. Như mã nguồn ở trên, đèn xanh là hết 15 giây sẽ chuyển sang đèn vàng 3 giây, hết 3 giây đó sẽ chuyển sang đèn đỏ là 20 giây.
@RoutePage()
class StatePage extends StatefulWidget {
  const StatePage({super.key});

  @override
  State<StatePage> createState() => _StatePageState();
}

class _StatePageState extends State<StatePage> {
  late TrafficLightContext _trafficLight;
  late Timer _timer;
  late int _currentTime = 0;

  @override
  void initState() {
    super.initState();
    _trafficLight = TrafficLightContext(GreenState());
    _startTimer();
  }

  @override
  void dispose() {
    super.dispose();
    _timer.cancel();
  }

  void _startTimer() {
    _currentTime = _trafficLight.currentDuration.inSeconds;

    _timer = Timer.periodic(const Duration(seconds: 1), (timer) {
      setState(() {
        _currentTime--;

        if (_currentTime == 0) {
          _trafficLight.nextTrafficLight();
          timer.cancel();
          _startTimer();
        }
      });
    });
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: const PrimaryAppBar(
        title: 'State',
      ),
      body: SafeArea(
        child: Column(
          crossAxisAlignment: CrossAxisAlignment.stretch,
          mainAxisAlignment: MainAxisAlignment.center,
          children: [
            Container(
              width: 200,
              height: 200,
              decoration: BoxDecoration(
                shape: BoxShape.circle,
                color: _trafficLight.currentColor,
              ),
            ),
            const SizedBox(height: 20),
            Center(
              child: Text(
                _currentTime.toString(),
                style: const TextStyle(
                  fontSize: 30,
                ),
              ),
            ),
          ],
        ),
      ),
    );
  }
}
Thành quả

Như vậy, chúng ta đã tạo ra một cột đèn giao thông hàng ""pha ke"" rep 1:1 rồi. Hi vọng với ví dụ này bạn có thể hiểu được ý tưởng sơ qua của mẫu thiết kế State.
Về source code mình đã up lên github, mọi người có thể vào tham khảo và cho mình xin một up-vote ạ ^^
Tổng kết
Thật tuyệt vời nếu bạn đã đi được đến đây với mình. Cảm ơn các bạn và cùng mình đón chờ một mẫu thiết kế mới toanh. Design Pattern cùng Flutter. Tập 8: Facade - ""Thống nhất đất nước""",https://viblo.asia/p/design-pattern-cung-flutter-tap-7-state-sieu-nhan-bien-hinh-E1XVObbDLMz
37,Mọi người đã biết đến cursorAI chưa? 🤔,"Mình cứ tưởng đến ChatGPT hay ClaudeAI là công cụ tốt nhất cho việc hỗ trợ lập trình nhưng sau khi xem video của Phạm Huy Hoàng (toidicodedao) thì thấy có CursorAI.
CursorAI không chỉ giúp fix lỗi như ChatGPT mà còn là extention tích hợp vào VSCode có thể đọc toàn bộ source code và tự tạo các file theo prompt mà mình cho vào. Mình đã thử dùng Claude-3.5-sonnet trong CursorAI và có thể viết 1 module trong vòng vài giây từ router, view đến API và chỉ cần chỉnh lại cho đúng yêu cầu mà không cần sửa quá nhiều.

CursorAI: https://www.cursor.com/",https://viblo.asia/p/moi-nguoi-da-biet-den-cursorai-chua-2oKLnmbNJQO
38,Nhật ký học Hibernate từ số 0,"HIBERNATE
Trong mục chương trình Java tương tác với cơ sở dữ liệu, chúng ta xây dựng chương trình Java sao cho mỗi bảng trong cơ sở dữ liệu ứng với một class trong phần Models của chương trình Java. Sau đó chúng ta viết code pure Java để kết nối chương trình với cơ sở dữ liệu, qua đó thực hiện những câu lệnh SQL theo cách viết trực tiếp lệnh đó vào trong chương trình Java và truyền nó cho đối tượng Statement để thực hiện lệnh đó lên CSDL.
Như vậy tính liên kết giữa đối tượng đã xây dựng trong Models và CSDL chưa được chặt chẽ, mình đang ngộ nhận rằng chúng có liên kết với nhau vì chúng giống nhau, trong khi sự giống nhau đó lại là do mình xây dựng. Nếu chúng thực sự liên kết với nhau, mình chỉ cần tác động lên Models (thêm/sửa/xóa dựa vào Models) là đã có thể thay đổi dữ liệu trong CSDL. Không yêu cầu viết hẳn câu lệnh SQL vào trong chương trình Java, mà chỉ cần thông qua các câu lệnh dựng sẵn để thực hiện. (lúc đó tính “ánh xạ” giữa bảng trong CSDL và Objects trong Models rất rõ).

HIBERNATE
Đây là nhật ký học Hibernate với Java của một đứa chưa biết gì về Java EE, mới chỉ học qua OOP với Java. Bài viết chắc chắn có nhiều điểm sai sót và lệch lạc, xin mọi người thông cảm.
Link tham khảo mình có để ở từng phần cụ thể.
Cảm ơn mọi người.
Docs: https://docs.jboss.org/hibernate/orm/current/userguide/html_single/Hibernate_User_Guide.html
Định nghĩa
Hibernate là một framework trong java.
Cụ thể hơn, Hibernate là một framework ORM (Object-Relational Mapping) trong Java. Dịch ra là “ánh xạ các đối tượng quan hệ”, cụ thể là ánh xạ các bảng trong cơ sở dữ liệu quan hệ tới các lớp đối tượng mô phỏng (models) trong chương trình Java.
Nếu không dùng Hibernate thì mình phải làm những gì?
Nếu không sử dụng Hibernate, mọi thao tác truy xuất dữ liệu với CSDLQH (load Driver, tạo Statement, viết câu lệnh thủ công, thao tác với ResultSet, …) đều phải viết thủ công. Nếu sử dụng Hibernate, các thao tác đó sẽ được viết code bằng Java, cụ thể hơn là dưới dạng Hibernate Query Language (HQL).
Giới thiệu các thành phần

Theo sơ đồ thì phần liên quan đến hiberate sẽ xử lý cho mình phần liên kết chương trình từ Manager (Controller) tới JDBC. Qua đó cũng sẽ phát sinh thêm một vài thay đổi với phần Models và DAO so với dùng pure Java.
Giải thích cho từng phần:
Data Access Layer: phần này bao gồm những phần có dính líu tới cơ sở dữ liệu (Models và một phần Utils DAO). Phần này sẽ bị thay đổi so với bên docs1, sử dụng pure Java.
Java Persistence API (JPA): là một API phục vụ việc ánh xạ giữa các đối tượng Java (Models) với cơ sở dữ liệu quan hệ. Nó đóng vai trò như cầu nối từ Java Models tới CSDL, giữ quan hệ một một cho table – objects trong mối liên kết.
https://gpcoder.com/6282-tong-quan-ve-jpa-java-persistence-api/
Hibernate Native API: là một API nằm trong Hibernate Framework, cho phép thực thi các truy vấn SQL native trực tiếp vào cơ sở dữ liệu thay vì sử dụng Hibernate Query Language (HQL). Dùng trong trường hợp HQL hết cứu. Nhưng phải dùng cẩn thận nếu không sẽ dính SQL Injection. (nói sau đi).
=> Có thể hiểu JPA là cách tiếp cận mới như đang mong đợi trong phần Hibernate này. Nhưng đôi khi nó không thể nào thực thi hết được với những câu lệnh phức tạp (HQL hết cứu) nên phải dùng câu lệnh pure SQL để cứu. Lúc đó là lúc Hibernate Native API hoạt động.
- Hibernate: phần module này cung cấp khả năng chuyển đổi các yêu cầu nhận được từ JPA và HNA để biến đổi thành câu lệnh SQL, sau đó chuyển xuống JDBC yêu cầu thực thi.
- JDBC: như đã viết thì JDBC tạo kết nối tới CSDL, mở một Connection và thực thi câu lệnh (từ module Hibernate), sau đó đóng Connection và trả về kết quả lên các tầng cao hơn.
ORM và JPA
ORM
Như đã nói ở trên, ORM (Object-Relational Mapping - dịch ra là “ánh xạ các đối tượng quan hệ”), là một kỹ thuật để định nghĩa sự ánh xạ giữa các đối tượng (trong Models) và các table (CSDL).
Nhờ vào kỹ thuật này, mỗi table trong CSDL sẽ được xây dựng cho một đối tượng trong chương trình, và chúng được liên kết bởi các annotation và lệnh trong file cấu hình (config XML).
Nguyên tắc hoạt động:
(tìm mãi không có cái docs nào nói. Đọc source thì cũng không có gì đặc biệt)

Các class được xây dựng tương tự thông thường, rồi được thêm vào các annotations,
phần code XML thì có các lệnh cấu hình cho các class trên,
dựa vào đó Hibernate sẽ tìm được vị trí của các class Models rồi dựa vào thông tin đã có trên annotations để biết được class nào ánh xạ tới table nào.
JPA (Java Persistence API)
Mở đầu:

Như phần giới thiệu bên trên, JPA là “một API phục vụ việc ánh xạ giữa các đối tượng Java (Models) với cơ sở dữ liệu quan hệ”, là một đặc tả tiêu chuẩn của Java cho phép các ứng dụng Java quản lý dữ liệu quan hệ trong cơ sở dữ liệu theo một cách hướng đối tượng.
Cụ thể hơn, JPA là một framework bao gồm các interface và abstract class (API), được xây dựng với các thành phần các Annotations, EntityManager, EntityManagerManager,… (đừng hỏi interface gì và abstract class gì)
Với cấu trúc như vậy JPA có thể sử dụng kỹ thuật ORM như bên trên để cung cấp sự ánh xạ giữa các đối tượng Java (Models) với các table trong CSDLQH.
Tuy nhiên JPA lại là một API của Java EE (Enterprise Edition – Java bản mở rộng) chứ không có trong Java SE (Standard Edition – Java bản cơ bản). Muốn sử dụng JPA trong Java SE thì phải sử dụng từng thư viện riêng lẻ.
Như phần giới thiệu bên trên, JPA là “một API phục vụ việc ánh xạ giữa các đối tượng Java (Models) với cơ sở dữ liệu quan hệ”, là một đặc tả tiêu chuẩn của Java cho phép các ứng dụng Java quản lý dữ liệu quan hệ trong cơ sở dữ liệu theo một cách hướng đối tượng.
Cụ thể hơn, JPA là một framework bao gồm các interface và abstract class (API), được xây dựng với các thành phần các Annotations, EntityManager, EntityManagerManager,… (đừng hỏi interface gì và abstract class gì)
Với cấu trúc như vậy JPA có thể sử dụng kỹ thuật ORM như bên trên để cung cấp sự ánh xạ giữa các đối tượng Java (Models) với các table trong CSDLQH.
Tuy nhiên JPA lại là một API của Java EE (Enterprise Edition – Java bản mở rộng) chứ không có trong Java SE (Standard Edition – Java bản cơ bản). Muốn sử dụng JPA trong Java SE thì phải sử dụng từng thư viện riêng lẻ.
(Một khi code với Java EE thì bắt đầu làm quen với JAVA MAVEN).
Chi tiết:

Các thành phần trong JPA:
Các thành phần trong JPA:
Chúng ta có thể thấy trong docs cái cảnh này:


Sau khi đơn giản hóa đi thì có được sơ đồ như sau:

Nhìn vẫn rất rối. Bây giờ sẽ quan sát từ từ. Ở đây nhận thấy nơi phức tạp nhất là EntityManager (vẫn chưa biết chúng nó là gì, nhưng sẽ phân tích từ đó).

Phân biệt giữa entities, class, objects, table trong bối cảnh sử dụng JPA:
Entity: Lớp Java (Java class) được ánh xạ tới một bảng (table) trong cơ sở dữ liệu, đánh dấu bằng @Entity.

Class: Kiểu dữ liệu trong lập trình hướng đối tượng, trở thành entity nếu đánh dấu bằng @Entity.
Object: Thực thể cụ thể của một class, tương ứng với một hàng (row) trong bảng cơ sở dữ liệu.
Table: Bảng trong cơ sở dữ liệu, chứa dữ liệu dưới dạng hàng và cột, được ánh xạ từ entity class, tùy chỉnh bằng @Table.
Entity: Lớp Java (Java class) được ánh xạ tới một bảng (table) trong cơ sở dữ liệu, đánh dấu bằng @Entity.

Class: Kiểu dữ liệu trong lập trình hướng đối tượng, trở thành entity nếu đánh dấu bằng @Entity.
Object: Thực thể cụ thể của một class, tương ứng với một hàng (row) trong bảng cơ sở dữ liệu.
Table: Bảng trong cơ sở dữ liệu, chứa dữ liệu dưới dạng hàng và cột, được ánh xạ từ entity class, tùy chỉnh bằng @Table.
Entity: Lớp Java (Java class) được ánh xạ tới một bảng (table) trong cơ sở dữ liệu, đánh dấu bằng @Entity.

Class: Kiểu dữ liệu trong lập trình hướng đối tượng, trở thành entity nếu đánh dấu bằng @Entity.
Object: Thực thể cụ thể của một class, tương ứng với một hàng (row) trong bảng cơ sở dữ liệu.
Table: Bảng trong cơ sở dữ liệu, chứa dữ liệu dưới dạng hàng và cột, được ánh xạ từ entity class, tùy chỉnh bằng @Table.
Class: Kiểu dữ liệu trong lập trình hướng đối tượng, trở thành entity nếu đánh dấu bằng @Entity.
Object: Thực thể cụ thể của một class, tương ứng với một hàng (row) trong bảng cơ sở dữ liệu.
Table: Bảng trong cơ sở dữ liệu, chứa dữ liệu dưới dạng hàng và cột, được ánh xạ từ entity class, tùy chỉnh bằng @Table.
EntityManager:



Là phần quản lý đối tượng thực thể (Pesistent Objects) trong một “bối cảnh” nhất định – có thể hiểu là trong một thời điểm/ khu lưu trữ của Pesistent Objects nhất định với mỗi lần sử dụng đến nó. (Pesistence Context – dưới nói sau)

EntityManager cung cấp các phương thức CRUD với các Entity từ CSDL.
Là phần quản lý đối tượng thực thể (Pesistent Objects) trong một “bối cảnh” nhất định – có thể hiểu là trong một thời điểm/ khu lưu trữ của Pesistent Objects nhất định với mỗi lần sử dụng đến nó. (Pesistence Context – dưới nói sau)

EntityManager cung cấp các phương thức CRUD với các Entity từ CSDL.
Là phần quản lý đối tượng thực thể (Pesistent Objects) trong một “bối cảnh” nhất định – có thể hiểu là trong một thời điểm/ khu lưu trữ của Pesistent Objects nhất định với mỗi lần sử dụng đến nó. (Pesistence Context – dưới nói sau)

EntityManager cung cấp các phương thức CRUD với các Entity từ CSDL.
EntityManager cung cấp các phương thức CRUD với các Entity từ CSDL.
Đối với pure Java, chức năng này phải tự viết dựa vào JDBC. Thực tế thì để thực hiện được chức năng này EntityManager cũng dựa vào JDBC để có thể thực hiện CRUD.
Được sử dụng để thực thi các truy vấn nói trên bằng cách sử dụng các lệnh query.

Điểm đặc biệt của EntityManager là thực hiện truy vấn bằng các transaction để tăng tính toàn vẹn dữ liệu.
Được sử dụng để thực thi các truy vấn nói trên bằng cách sử dụng các lệnh query.

Điểm đặc biệt của EntityManager là thực hiện truy vấn bằng các transaction để tăng tính toàn vẹn dữ liệu.
Được sử dụng để thực thi các truy vấn nói trên bằng cách sử dụng các lệnh query.

Điểm đặc biệt của EntityManager là thực hiện truy vấn bằng các transaction để tăng tính toàn vẹn dữ liệu.
Điểm đặc biệt của EntityManager là thực hiện truy vấn bằng các transaction để tăng tính toàn vẹn dữ liệu.
Pesistence Context:



Dịch ra nghĩa đen thì nó là “Bối cảnh lưu trữ”.

Cụ thể hơn thì nó là một bộ nhớ đệm tạm thời chứa các đối tượng thực thể trước khi chúng được lưu hoặc sau khi chúng được tải từ cơ sở dữ liệu. (tác dụng như Buffer)
Pesistence Context luôn đảm bảo tính đồng nhất: đảm bảo mỗi thực thể được quản lý chỉ có một bản sao duy nhất trong một Persistence Context. (có nghĩa là nếu truy vấn một đối tượng nhiều lần trong cùng một Persistence Context, bạn sẽ luôn nhận được cùng một đối tượng, không phải các bản sao khác nhau)
Pesistence Context có tính đồng bộ hóa: sau mỗi lần transaction commit (của EntityManager) thì Pesistence Context sẽ được đồng bộ lại với database: gần như cách buffer hoạt động: đem lên một phần dữ liệu tạm lên trên chương trình, mỗi lần chương trình thực hiện xong sẽ có một hành động là “flush” xuống database.
Persistence Context quản lý vòng đời của các đối tượng thực thể (entities), từ khi chúng được tạo, cập nhật cho đến khi bị xóa.
Dịch ra nghĩa đen thì nó là “Bối cảnh lưu trữ”.

Cụ thể hơn thì nó là một bộ nhớ đệm tạm thời chứa các đối tượng thực thể trước khi chúng được lưu hoặc sau khi chúng được tải từ cơ sở dữ liệu. (tác dụng như Buffer)
Pesistence Context luôn đảm bảo tính đồng nhất: đảm bảo mỗi thực thể được quản lý chỉ có một bản sao duy nhất trong một Persistence Context. (có nghĩa là nếu truy vấn một đối tượng nhiều lần trong cùng một Persistence Context, bạn sẽ luôn nhận được cùng một đối tượng, không phải các bản sao khác nhau)
Pesistence Context có tính đồng bộ hóa: sau mỗi lần transaction commit (của EntityManager) thì Pesistence Context sẽ được đồng bộ lại với database: gần như cách buffer hoạt động: đem lên một phần dữ liệu tạm lên trên chương trình, mỗi lần chương trình thực hiện xong sẽ có một hành động là “flush” xuống database.
Persistence Context quản lý vòng đời của các đối tượng thực thể (entities), từ khi chúng được tạo, cập nhật cho đến khi bị xóa.
Dịch ra nghĩa đen thì nó là “Bối cảnh lưu trữ”.

Cụ thể hơn thì nó là một bộ nhớ đệm tạm thời chứa các đối tượng thực thể trước khi chúng được lưu hoặc sau khi chúng được tải từ cơ sở dữ liệu. (tác dụng như Buffer)
Pesistence Context luôn đảm bảo tính đồng nhất: đảm bảo mỗi thực thể được quản lý chỉ có một bản sao duy nhất trong một Persistence Context. (có nghĩa là nếu truy vấn một đối tượng nhiều lần trong cùng một Persistence Context, bạn sẽ luôn nhận được cùng một đối tượng, không phải các bản sao khác nhau)
Pesistence Context có tính đồng bộ hóa: sau mỗi lần transaction commit (của EntityManager) thì Pesistence Context sẽ được đồng bộ lại với database: gần như cách buffer hoạt động: đem lên một phần dữ liệu tạm lên trên chương trình, mỗi lần chương trình thực hiện xong sẽ có một hành động là “flush” xuống database.
Persistence Context quản lý vòng đời của các đối tượng thực thể (entities), từ khi chúng được tạo, cập nhật cho đến khi bị xóa.
Cụ thể hơn thì nó là một bộ nhớ đệm tạm thời chứa các đối tượng thực thể trước khi chúng được lưu hoặc sau khi chúng được tải từ cơ sở dữ liệu. (tác dụng như Buffer)
Pesistence Context luôn đảm bảo tính đồng nhất: đảm bảo mỗi thực thể được quản lý chỉ có một bản sao duy nhất trong một Persistence Context. (có nghĩa là nếu truy vấn một đối tượng nhiều lần trong cùng một Persistence Context, bạn sẽ luôn nhận được cùng một đối tượng, không phải các bản sao khác nhau)
Pesistence Context có tính đồng bộ hóa: sau mỗi lần transaction commit (của EntityManager) thì Pesistence Context sẽ được đồng bộ lại với database: gần như cách buffer hoạt động: đem lên một phần dữ liệu tạm lên trên chương trình, mỗi lần chương trình thực hiện xong sẽ có một hành động là “flush” xuống database.
Persistence Context quản lý vòng đời của các đối tượng thực thể (entities), từ khi chúng được tạo, cập nhật cho đến khi bị xóa.
Thực tế là một khi chạy một request của user, chỉ một EntityManager tồn tại trong thời điểm đó, nên chỉ một Persistence Context được có mặt để thực hiện request đó. Hay đơn giản hơn là 1 yêu cầu của khách hàng sẽ gọi đến 1 phần database, và lệnh gọi đó (EntityManager) đem lên một phần database cần thiết bỏ vào không gian thực thi – tạm (Persistence Context) để làm việc. Làm việc xong thì đóng luôn EntityManager đó, nên Persistence Context cũng biến mất. Và các đối tượng thực thể (entities) sẽ tồn tại và được quản lý trong phần đời của Persistence Context hiện tại.
Các trạng thái của một thực thể trong một vòng đời Persistence Context:
Các trạng thái của một thực thể trong một vòng đời Persistence Context:
Các trạng thái của một thực thể trong một vòng đời Persistence Context:
new (Transient – tạm thời): thực thể được tạo ra nhưng chưa được quản lý bởi Persistence Context, cũng chưa được lưu vào database. => Không có ID được gán từ cơ sở dữ liệu.
Managed (Persistent – bền vững): thực thể đang được Persistence Context quản lý và đồng bộ với database. => Thực thể có ID duy nhất, được gán từ database.
Detached: thực thể từng được quản lý nhưng hiện không còn nằm trong Persistence Context. => Thực thể có ID từ cơ sở dữ liệu nhưng thay đổi trên nó sẽ không được đồng bộ với database.
Removed: thực thể được đánh dấu để xóa khỏi database khi transaction kết thúc. => Thực thể vẫn được quản lý bởi Persistence Context cho đến khi transaction kết thúc.
Quan hệ và sự chuyển hóa giữa các trạng thái:
Quan hệ và sự chuyển hóa giữa các trạng thái:
Quan hệ và sự chuyển hóa giữa các trạng thái:

Trong chỗ này có 2 trạng thái được quản lý bởi Persistence Context là Managed và Removed thôi. Các quan hệ còn lại cũng được mô tả trong hình khá rõ rồi nên sẽ không nói thêm.
EntityManagerFactory



Là một interface trong JPA đại diện cho một kết nối tới cơ sở dữ liệu theo cấu hình của ứng dụng (pesistence.xml – nói sau).

EntityManagerFactory có chức năng tạo và quản lý EntityManager.
Ngoài ra EntityManagerFactory nhận cấu hình từ pesistence.xml và cung cấp nó cho những bên cần thiết như JDBC. EntityManagerFactory sử dụng JDBC để thiết lập các kết nối cơ sở dữ liệu ban đầu và tạo ra các EntityManager. Tuy nhiên, nó không trực tiếp thực hiện các truy vấn hoặc thao tác cơ sở dữ liệu.
Vòng đời của một EntityManagerFactory và quan hệ của chúng với vòng đời của app và EntityManager như sau:
Là một interface trong JPA đại diện cho một kết nối tới cơ sở dữ liệu theo cấu hình của ứng dụng (pesistence.xml – nói sau).

EntityManagerFactory có chức năng tạo và quản lý EntityManager.
Ngoài ra EntityManagerFactory nhận cấu hình từ pesistence.xml và cung cấp nó cho những bên cần thiết như JDBC. EntityManagerFactory sử dụng JDBC để thiết lập các kết nối cơ sở dữ liệu ban đầu và tạo ra các EntityManager. Tuy nhiên, nó không trực tiếp thực hiện các truy vấn hoặc thao tác cơ sở dữ liệu.
Vòng đời của một EntityManagerFactory và quan hệ của chúng với vòng đời của app và EntityManager như sau:
Là một interface trong JPA đại diện cho một kết nối tới cơ sở dữ liệu theo cấu hình của ứng dụng (pesistence.xml – nói sau).

EntityManagerFactory có chức năng tạo và quản lý EntityManager.
Ngoài ra EntityManagerFactory nhận cấu hình từ pesistence.xml và cung cấp nó cho những bên cần thiết như JDBC. EntityManagerFactory sử dụng JDBC để thiết lập các kết nối cơ sở dữ liệu ban đầu và tạo ra các EntityManager. Tuy nhiên, nó không trực tiếp thực hiện các truy vấn hoặc thao tác cơ sở dữ liệu.
Vòng đời của một EntityManagerFactory và quan hệ của chúng với vòng đời của app và EntityManager như sau:
EntityManagerFactory có chức năng tạo và quản lý EntityManager.
Ngoài ra EntityManagerFactory nhận cấu hình từ pesistence.xml và cung cấp nó cho những bên cần thiết như JDBC. EntityManagerFactory sử dụng JDBC để thiết lập các kết nối cơ sở dữ liệu ban đầu và tạo ra các EntityManager. Tuy nhiên, nó không trực tiếp thực hiện các truy vấn hoặc thao tác cơ sở dữ liệu.
Vòng đời của một EntityManagerFactory và quan hệ của chúng với vòng đời của app và EntityManager như sau:

Hình miêu tả khá rõ rồi nên không cần thêm mô tả thêm nữa. (quá lười để viết thêm nên nhờ GPT viết):
Hình minh họa trên mô tả chu trình sống của `EntityManagerFactory` và `EntityManager` trong Java Persistence API (JPA). Khi ứng dụng bắt đầu, `EntityManagerFactory` được tạo ra, đây là một đối tượng singleton và chỉ được khởi tạo một lần trong suốt thời gian chạy của ứng dụng. Từ `EntityManagerFactory`, các `EntityManager` được tạo ra để quản lý các thực thể và thực hiện các hoạt động như truy vấn, cập nhật, và xóa trên cơ sở dữ liệu. Ứng dụng gửi các yêu cầu đến `EntityManager`, và `EntityManager` sẽ xử lý các yêu cầu này và trả về phản hồi tương ứng. Sau khi hoàn thành công việc, `EntityManager` được đóng lại để giải phóng tài nguyên. Cuối cùng, khi ứng dụng dừng, `EntityManagerFactory` cũng được đóng lại để giải phóng các tài nguyên còn lại. Chu trình này phản ánh đúng cách hoạt động cơ bản của JPA.
Khác nhau giữa EntityManagerFactory và EntityManager trong khi sử dụng JDBC: EntityManagerFactory chịu trách nhiệm thiết lập và quản lý các kết nối cơ sở dữ liệu cần thiết cho toàn bộ ứng dụng. Nó được tạo ra khi ứng dụng khởi động và tồn tại trong suốt vòng đời của ứng dụng. Còn EntityManager sử dụng thông tin cấu hình từ tệp persistence.xml để thiết lập các kết nối JDBC tới cơ sở dữ liệu. EntityManagerFactory không trực tiếp thực hiện các truy vấn hay cập nhật dữ liệu; thay vào đó, nó tạo ra các EntityManager để thực hiện các thao tác này.
pesistence.xml và pesistence unit
+) pesistence.xml chứa các thông tin cần thiết để JPA có thể kết nối và làm việc với cơ sở dữ liệu. Cấu hình trong persistence.xml sẽ bao gồm các thông tin như:
. Tên đơn vị lưu trữ (Persistence Unit)
. Các thuộc tính kết nối cơ sở dữ liệu (URL, username, password, driver class)
. Các lớp entity mà JPA sẽ quản lý
. Các thuộc tính JPA (như chế độ gỡ lỗi, chiến lược tạo bảng...)
…
+) Persistence Unit là đại diện cho một tập hợp các lớp thực thể (entity classes) và các cài đặt liên quan (như các thuộc tính kết nối cơ sở dữ liệu).
Mỗi persistence unit được quản lý như một đơn vị riêng biệt.
Mỗi persistence unit phải có một tên định danh duy nhất (được xác định bằng thuộc tính name). Tên này được sử dụng trong mã nguồn Java để tham chiếu đến đơn vị lưu trữ này khi tạo EntityManagerFactory.
Khi tạo EntityManagerFactory bằng cách sử dụng tên của persistence unit, JPA sẽ đọc persistence.xml, tìm persistence unit tương ứng và khởi tạo EntityManagerFactory dựa trên các cấu hình này. EntityManagerFactory sau đó quản lý các đối tượng EntityManager, cho phép tương tác với cơ sở dữ liệu theo các cài đặt đã định nghĩa trong persistence.xml.
Một quy trình hoàn thiện cho sơ đồ trên:
Nhắc lại sơ đồ:

pesistence.xml chứa thông tin cấu hình của database và chứa pesistence unit đại diện cho một tập hợp các lớp thực thể trong database.
Với mỗi chương trình ứng dụng, EntityManagerFactory lấy thông tin cấu hình trên để kết nối với JDBC, còn lấy pesistence unit để tạo ra EntityManager, đi kèm nó là không gian bộ nhớ tạm – Bối cảnh lưu trữ (Pesistent Context).
EntityManager thường được tạo ra khi bắt đầu một request và được sử dụng lại cho toàn bộ quá trình xử lý request đó.
EntityManager sử dụng JDBC để thực thi các câu lệnh truy vấn và cập nhật các thực thể trong bối cảnh lưu trữ (Persistent Context).
Sau khi request hoàn thành, EntityManager sẽ được đóng (closed) và không sử dụng lại.
https://www.java4coding.com/contents/jpa/jpa-architecture
Hibernate Native API
Theo hình mô tả từ ban đầu thì Hibernate Native API nằm ngang hàng với JPA. Vậy chức năng chính của Hibernate Native API cũng tương tự như JPA: dựa vào JDBC để thao tác với cơ sở dữ liệu. Cụ thể hơn thì như sau:
TỔNG QUÁT
Định nghĩa: Hibernate Native API là một tập hợp các giao diện và lớp mà Hibernate cung cấp để thao tác trực tiếp với cơ sở dữ liệu mà không cần phải qua các lớp trung gian như HQL (Hibernate Query Language). Rõ ràng hơn là nó thực thi câu lệnh pure SQL lên trên CSDL từ chương trình Java.
Như vậy thì có khác gì JDBC? Mà tại sao nó lại được ngang hàng với JPA? Thực tế HNA không chỉ thực thi một cách thụ động như JDBC làm cho người dùng phải tự khởi tạo kết nối, tự quản lý Statement,.v.v... mà những thao tác đó hoàn toàn tự động. Thêm nữa, HNA không một mình thực thi pure SQL mà dựa vào JDBC để làm việc này. Vì vậy có thể coi HNA như một cấp cao hơn của JDBC.
THÀNH PHẦN

Phần này thực sự khó để tìm hiểu vì không có docs, mã nguồn cũng không. Tài liệu tìm thấy chỉ nói về cách sử dụng chứ không phân tích cấu trúc của Hibernate Native API.

- Có thể gồm các thành phần như sau (cứ viết nếu sai sẽ sửa sau)
Phần cấu trúc (có thể) cũng tương tự như bên JPA một chút: cũng có phần cấu hình, từ cấu hình tạo được Factory, từ Factory tạo ra các tượng trưng cho các kết nối, rồi từ đó thực hiện các kết nối tới database.
JDBC
Cái này quen rồi chắc nhắc lại chút cho nhớ:
JDBC (Java Database Connectivity) là một API trong Java giúp kết nối và tương tác với các cơ sở dữ liệu quan hệ. Nó cung cấp một cách để thực thi các câu lệnh SQL, truy vấn cơ sở dữ liệu và xử lý kết quả trả về từ cơ sở dữ liệu.
CÁCH THỨC HOẠT ĐỘNG
Sau khi nói về các thành phần thì sẽ tới phần hoạt động của Hibernate. Thực ra trong lúc nói về các thành phần thì cũng có đề cập qua về cách hoạt động rồi. Bây giờ sẽ nói tổng quan về cách một chương trình tổng quan sẽ hoạt động như thế nào nếu sử dụng framework Hibernate. Mà cũng không hẳn là cách Hibernate hoạt động như thế nào, nói cái đó ở bên trên hết rồi, cụ thể hơn hiện tại cần là mình cài đặt và sử dụng Hibernate như thế nào, và trong từng bước đó thì Hibernate làm gì.
1, Trước hết, nhắc lại tại sao lại là Hibernate chứ không phải pure Java với JDBC.
Đầu tiên sẽ nêu ra những vấn đề với một chương trình JDBC pure Java. Từ đó tìm ra được cách giải quyết/code tốt hơn với Hibernate.
https://www.geeksforgeeks.org/introduction-to-hibernate-framework/
Vài vấn đề của JDBC có thể nhìn thấy rõ:

Chuỗi kết nối khác nhau đối với mỗi database và loại database khác nhau:
Chuỗi kết nối khác nhau đối với mỗi database và loại database khác nhau:

Với mỗi database thì các giá trị của biến databaseName và hầu hết các giá trị kia đều phải thay đổi. Thậm chí nếu đổi loại database (từ SQL server sang MySQL chẳng hạn) thì phải đổi hoàn toàn cách kết nối.
Bắt buộc try/catch:
Bắt buộc try/catch:

Trong JDBC yêu cầu bắt buộc try/catch cho các kết nối. Tuy nó rõ ràng nhưng khá bất tiện.
Đóng mở kết nối bằng tay:
Đóng mở kết nối bằng tay:

JDBC yêu cầu đóng mở kết nối mỗi lần thực hiện Statement, làm code dài thêm.
(Boilerplate problem)
Trong khi đó, với Hibernate (theo như quảng cáo) chúng ta có thể:
Hibernate framework hỗ trợ các hoạt động DDL tự động. Trong JDBC, chúng ta phải tạo bảng theo cách thủ công và khai báo kiểu dữ liệu cho từng cột. Nhưng Hibernate có thể thực hiện các hoạt động DDL cho bạn nội bộ như tạo bảng, xóa bảng, thay đổi bảng, v.v.
(vì nó là ORM mà)
Hibernate hỗ trợ tạo khóa chính tự động . Điều đó có nghĩa là trong JDBC, chúng ta phải thiết lập khóa chính cho một bảng theo cách thủ công. Nhưng Hibernate có thể thực hiện nhiệm vụ này. (ờ chưa kiểm chứng, lát làm được thì nói sau)
Khung Hibernate độc ​​lập với Cơ sở dữ liệu vì nó hỗ trợ HQL (Ngôn ngữ truy vấn Hibernate) không dành riêng cho bất kỳ cơ sở dữ liệu nào, trong khi JDBC phụ thuộc vào cơ sở dữ liệu. (ờ chưa kiểm chứng, lát làm được thì nói sau)
Trong Hibernate, Xử lý ngoại lệ không phải là bắt buộc, trong khi xử lý ngoại lệ trong JDBC là bắt buộc. (ờ chưa kiểm chứng, lát làm được thì nói sau)
Hibernate hỗ trợ bộ nhớ đệm trong khi JDBC không hỗ trợ bộ nhớ đệm.
Hibernate hỗ trợ ánh xạ quan hệ đối tượng. Nghĩa là với JDBC, chúng ta đang xử lý những giá trị riêng biệt mà cứ ngộ nhận là chúng có liên kết. Trong khi Hibernate là thực sự kết nối chúng thành một đối tượng.
2, Rất đáng mong chờ. Vậy cấu tạo tổng của một chương trình sử dụng Hibernate như nào để mình còn sử dụng?
(Lưu ý đây là cấu tạo của Hibernate, không phải của HNA hay JPA)

Nhìn cảnh này thật quen thuộc. Hãy thử nhìn lại cấu trúc của JPA:

Có thể thấy sự tương đồng trong cấu tạo của Hibernate và JPA: cũng lấy thông tin từ file cấu hình, tạo ra một Factory, rồi từ Factory tạo ra các đại diện để thực hiện câu lệnh SQL; bằng cách này hay cách khác nhưng cấu tạo chung là như vậy.
Sau đây thì giải thích lại một chút về cấu trúc:
Configuration: Lớp chính để cấu hình Hibernate. Cung cấp các phương thức để thiết lập cấu hình và cấu hình các thuộc tính của Hibernate.
SessionFactory: Được tạo ra từ Configuration, SessionFactory là đối tượng chính để mở các Session. Nó chịu trách nhiệm quản lý các kết nối với cơ sở dữ liệu và cung cấp các Session để thực hiện các thao tác với cơ sở dữ liệu.
Session: Lớp dùng để tương tác với cơ sở dữ liệu. Session cung cấp các phương thức để lưu, cập nhật, xóa và truy vấn các thực thể.
Transaction: Đối tượng này quản lý các giao dịch cơ sở dữ liệu. Transaction được sử dụng để bắt đầu, commit hoặc rollback các giao dịch.
Query: Lớp hoặc giao diện để thực hiện các truy vấn HQL (Hibernate Query Language) hoặc SQL thuần.
Criteria: Cung cấp một API để tạo các truy vấn mà không cần phải viết HQL hoặc SQL, giúp tạo các truy vấn động và tùy chỉnh. Đơn giản hơn thì nó cung cấp các phương thức để thực thi câu lệnh ấy.
Entity: Các lớp đại diện cho các bảng trong cơ sở dữ liệu. Các lớp này được chú thích với các annotation của Hibernate để ánh xạ tới các bảng và cột.
Mapping: Cung cấp các phương thức và cấu hình để ánh xạ các lớp thực thể tới các bảng cơ sở dữ liệu, thường thông qua các file cấu hình XML hoặc annotation trong Java.
Vẽ lại workflow thì nó như thế này:

Có lẽ nhìn vào hình minh họa như thế này là đủ hiểu rồi, không cần giải thích thêm (vì nội dung cũng tương tự như nhau cả).
3, Thế code như nào để đi theo cái logic kia giờ? Bây giờ là cách code:
(sang phần mới)

Trước hết, hãy làm quen với một dự án Java Maven.
1, Java Maven là gì?
Tương tự như Java Ant, Java Maven là một công cụ quản lý dự án Java.
Java Maven là một công cụ quản lý dự án và tự động hóa build được sử dụng trong phát triển phần mềm Java. Maven giúp quản lý các thư viện phụ thuộc (dependencies), build các project một cách tự động và thống nhất, cũng như giúp dễ dàng triển khai và quản lý phiên bản của phần mềm.
Có thể hiểu đơn giản hơn là với dự án lớn và dùng nhiều framework thì nên dùng Maven, còn nếu tự build và muốn tinh chỉnh source code/cấu hình thì dùng Ant.
2, Tạo một chương trình Java Maven
Ở mục New Project, chọn Java With Maven.

VD như tôi đã tạo một Project tên là testHibernate và nó sẽ cho ra một folder như sau:

Giải thích các phần như sau:
testHibernate: Đây là tên của dự án Java mà mình đang làm việc.
Source Packages: Thư mục này chứa mã nguồn Java của dự án. Trong đó có:

com.mycompany.testhibernate: Đây là gói (package) của dự án. Các lớp Java nằm trong gói này. Mình có thể chỉnh tên của dự án khác đi, không cần mặc định là mycompany.
TestHibernate.java: Đây là một file Java chứa mã nguồn của lớp TestHibernate. Đây có thể là lớp chính của dự án hoặc một trong các lớp thành phần.
com.mycompany.testhibernate: Đây là gói (package) của dự án. Các lớp Java nằm trong gói này. Mình có thể chỉnh tên của dự án khác đi, không cần mặc định là mycompany.
TestHibernate.java: Đây là một file Java chứa mã nguồn của lớp TestHibernate. Đây có thể là lớp chính của dự án hoặc một trong các lớp thành phần.
Dependencies: Thư mục này chứa các thư viện và các phụ thuộc (dependencies) mà dự án sử dụng. Maven sẽ tự động tải và quản lý các thư viện này dựa trên cấu hình trong file pom.xml.
Java Dependencies: JDK 19 (Default): Đây là phiên bản của Java Development Kit (JDK) mà dự án đang sử dụng. Trong trường hợp này là JDK 19.
pom.xml: (Project Object Model) Đây là file cấu hình chính của dự án Maven. File này chứa thông tin về dự án, các phụ thuộc, các plugin và các chi tiết cấu hình khác. Maven sử dụng file này để quản lý và xây dựng dự án.
Thế XML là gì?
XML (eXtensible Markup Language) là một ngôn ngữ đánh dấu mở rộng được sử dụng để mô tả dữ liệu.
Trong phạm vi bài này, hãy coi nó có form giống như HTML: có các thẻ đóng/mở, và dùng để viết tệp cấu
hình trong các phần mềm.
https://docs.jboss.org/hibernate/orm/6.6/introduction/html_single/Hibernate_Introduction.html
1, Đầu tiên là thêm các thư viện của Hibernate và Database vào trong file cấu hình.
Đây là bước cung cấp thông tin Configuration.

Hãy nhớ Maven cho phép tự động tải và quản lý các thư viện/dependencies dựa vào cấu hình trong file pom.xml. Nên việc cần làm bây giờ là tìm các dependencies phù hợp để thêm vào file cấu hình pom.xml. Mà như đã giới thiệu thì chúng ta cần JDBC và Hibernate.
Bây giờ đi vào trang web https://mvnrepository.com/
Đây là trang web chứa các thư viện/dependencies trong Java Maven để mình lấy mã XML Configuration của các dự án từ đó. Nhắc lại là không phải tải, Maven tự tải.

Sau khi vào trang thì tìm kiếm Hibernate Core và MySQL Connector. Ai dùng SQL Server thì tìm SQL Server JDBC.
Sau đó nhấn chọn phiên bản. Tôi thích cái mới nhất (nhưng không phải bản thử nghiệm Alpha).

Hãy nhìn đoạn XML được làm riêng cho bản Maven kia. Copy nó lại, sau đó đi vào file pom.xml:

Lúc đầu chúng ta có một file như này. Hãy thêm vào đó một thẻ <dependencies> trong thẻ <project> để khai báo những thư viện của chúng ta trong đó nào.
Sau khi khai báo thì được như sau:

Sau đó lưu lại, và lần đầu ta thấy sự “tự động quản lý” của Maven:

Một vài thư viện mới đã được tự thêm vào phần Dependencies! Nhìn kỹ thì nó là 2 cái mình vừa thêm vào phần Dependencies.
Nhưng sao còn dấu chấm than vậy? Hãy build để hết.
Nhấn chuột phải vào tên Project và chọn build.

Sau khi build xong chúng ta thấy một loạt thư viện được tự động tải về:

Không cần quan tâm nhiều chúng là gì, chúng đơn giản là những thứ được giới thiệu ở phần CHI TIẾT CÁC THÀNH PHẦN thôi.
2, Bước tiếp theo là chuẩn bị một database
Tạo một database đơn giản như sau:

Giải thích một chút về database: quản lý nhóm, clan, nhiệm vụ của một nhóm ninja.
Về các mqh: một Teams có nhiều Ninjas,
một Clans có nhiều Ninjas,
một Teams có thể làm nhiều Missions và nhiều Teams có thể làm chung Missions.
Chúng ta cần kiểm tra xem mỗi lần đổi trưởng Clans hay trưởng Teams, trưởng Clans có trong Clans đó hay không; và trưởng Teams có trong bảng Ninjas hay không => viết 2 Trigger, mỗi lần đổi trưởng Clans mà người đó không có trong Clans đó thì báo lỗi, Teams tương tự.
Ý tưởng là như thế, nhưng vì tôi chưa thạo MySQL nên nhờ GPT viết hộ. (nó đéo viết được và báo hại t mất 1 ngày)

3, Bước tiếp là xây dựng Factory dựa trên những cấu hình vừa tạo

Sau đây là một loạt các câu lệnh với cú pháp lạ. Cần một sự ngậm ngùi chấp nhận không hề nhẹ ở đây.
Chúng ta đều rõ bước cần làm để tạo một Factory là phải có được Configuration. Lúc nãy chúng ta mới chỉ tải thư viện JDBC và Hibernate-core về chứ chưa config cái gì cả. Tiếp theo sẽ có hai cách để config (tiếp tục viết trong file XML hoặc viết code Java) nhưng docs tôi đang đọc hướng dẫn dạy tôi làm theo Java nên cứ theo cách đó đi đã.


(đưa meme này vào vì có quá nhiều thứ mới)_
Hãy tạo một class là HibernateUtils trong cùng gói đặt class Main (cho nhanh):

Đầu tiên muốn một Factory thì hãy khởi tạo và trả về nó. Logic tính sau.

FACTORY là một phần nặng và cả chương trình chỉ khởi tạo một lần. Vì vậy, xin được giới thiệu cách viết mới, làm cho khối lệnh luôn luôn chỉ được thực thi một lần trong suốt chương trình kể cả có khởi tạo đối tượng nhiều lần:

Trong đó chúng ta sẽ config và khởi tạo (build) Factory:

Nhìn xem nó yêu cầu gì thế kia: serviceRegistry. Sau khi hỏi GPT thì tôi biết nó là: class chịu trách nhiệm quản lý và cung cấp các dịch vụ cốt lõi như kết nối cơ sở dữ liệu, quản lý các phiên làm việc, các dịch vụ bộ đệm, và nhiều dịch vụ khác. Nó là một điểm trung tâm nơi tất cả các dịch vụ cần thiết cho Hibernate được đăng ký và truy cập. OK tạm chấp nhận thế chứ biết sao nữa, bản chất thì cũng có giới hạn thôi. Bây giờ tạo một serviceRegistry như nó yêu cầu thôi.
Để tạo một serviceRegistry thì chúng ta tạo từ “serviceRegistry Buider” – nơi xây dựng serviceRegistry. Cụ thể hơn là class StandardServiceRegistryBuilder.
Trong StandardServiceRegistryBuilder có một phương thức để ép thêm những cấu hình do mình tạo vào trong nơi xây dựng Factory để thành hình Factory như mình mong muốn, đó là method applySettings(Map<String,Object> settings). Có nghĩa là, sau khi dùng method này thứ chúng ta nhận được vẫn là một “bản thiết kế” nhà máy – StandardServiceRegistryBuilder - nhưng nó đã theo ý muốn chúng ta. Sau đó chỉ cần build() nữa là ra cái nhà máy.

Nhưng cái gì là mySettings? Mà nó lại là kiểu Map<> ? => trong Java có một thứ như vậy, đó là class Properties: một tập hợp các cặp khóa-giá trị, thường được sử dụng để lưu trữ các cấu hình của ứng dụng. Nó kế thừa từ lớp Hashtable. Vừa đẹp. Thế là bây giờ phải tạo thêm một Properties, rồi gán giá trị cấu hình theo cặp key-value vào đó, sau đó đem chúng đi làm “bản thiết kế”.
Theo như docs tôi xem thì có 5 tham số cần cung cấp cho “thiết kế” (nhớ vào): DIALECT, DRIVER, URL, USER, PASS. 4 cái cuối thì ok nhưng cái đầu là gì? GPT cho tôi biết: DIALECT là một lớp định nghĩa cách Hibernate sẽ chuyển đổi các câu lệnh SQL sang cú pháp SQL cụ thể của từng hệ quản trị cơ sở dữ liệu (DBMS) khác nhau. Nói cách khác, Dialect giúp Hibernate hiểu cách tương tác với từng loại DBMS cụ thể. OK chấp nhận thôi chứ biết sao giờ, chương trình người ta đã xây thế, giờ mình đi dùng lại cấm hỏi bản chất.
Các chuỗi liên kết mà chúng ta cần cung cấp cho Configuration là:
DIALECT: ""org.hibernate.dialect.MySQLDialect”
DRIVER: ""com.mysql.cj.jdbc.Driver""
URL: ""jdbc:mysql://localhost:3306/tên_database[?tham_số=giá_trị…]""
USER: ""tên_tài_khoản_database""
PASS: ""password_database""
Hãy nhớ các tham số này, và đừng hỏi vì sao nó là như thế. Nếu để ý thì 2 chuỗi DIALECT và DRIVER là địa chỉ của class Driver và Dialect bên trong gói Hibernate và JDBC. Chuỗi kết nối được mở ở cổng 3306, cổng mặc định của MySQL.
Code thôi:

Có thể thấy logic lúc nãy đang được áp dụng một cách ổn thỏa. Nhưng cái đéo gì thế kia? Code bị gạch: hết dát, đã được cập nhật. Giờ làm sao để tìm cái mới?
Tôi tra thử thì đây là nơi chứa cái đó:

org.hibernate.cfg.JdbcSettings. Lên gg tra xem nào:
https://docs.jboss.org/hibernate/orm/6.5/javadocs/org/hibernate/cfg/JdbcSettings.html

Đây rồi. Bản cũ thì thuộc tính là DRIVER còn bản mới là JAKARTA_JDBC_DRIVER. Quá tốt rồi giờ thay vào xem có chạy không nhé:

Quá tuyệt vời, không báo lỗi, quá tuyệt vời.
Bây giờ nhanh chóng test xem cái đó thực sự chạy hay chưa. Lên GPT và làm ngay một order: (cứ test đi lát nói phần này sau)

F6 hoặc build:

Trên đây tôi nêu ra tất cả những vấn đề tôi gặp phải khi cố gắng làm theo một video đã 4 năm tuổi so với lúc tôi viết cái này (31/7/2024) và cách tôi đối mặt với từng câu hỏi xuất hiện trong đầu tôi.
Lưu ý là chọn gói import đều phải từ org.hibernate mà ra. Import cái khác sai liền. (trừ Properties).
Thêm: khuyến khích dùng getProperties/setProperties của Configuration thay vì truyền thẳng nó vào applySettings().

Link video: https://www.youtube.com/watch?v=4HB6KqAdMGw

(Ai dùng SQL Server thì bỏ qua vấn đề này)
Vấn đề không chỉ dừng lại ở một số cái đơn giản như thế. Sau khi cài lại win và chạy lại thử chương trình thì tôi thấy một số vấn đề phát sinh với database (biết ngay vì sao lại đơn giản như thế được). Cụ thể:

đây là lỗi plugins bên mysql, khi mà mysql bản hiện tại không chấp nhận liên kết sử dụng mật khẩu mysql_native_password mà chỉ hỗ trợ caching_sha2_password. Cũng chưa rõ tại sao nhưng tôi đã cài lại mysql và workbench thì thấy mọi chuyện lại bình thường nên chưa đưa ra được giải pháp gì.
4, Bây giờ tạo ra class ở phần Models và thêm annotations đánh dấu.
Cùng đọc lại cách hoạt động trong phần này: 2, Rất đáng mong chờ. Vậy cấu tạo tổng của một chương trình sử dụng Hibernate như nào để mình còn sử dụng?
Tóm tắt lại những thông tin cần dùng: tạo class rồi gán @annotation vào, sau đó cài đặt liên kết, cài xong thì test một cái rồi sang bước tiếp.
Cụ thể các bước như này:
Tạo những class POJO - Plain old Java object – class Java bình thường – như cách vẫn hay tạo.

Chú ý: những class này phải có: một constructor rỗng, các thuộc tính phải đặt ở dạng private (vẫn tiêu chuẩn cũ)
Sau đó bắt đầu đánh dấu @annotation theo quy chuẩn sau:
Tạo những class POJO - Plain old Java object – class Java bình thường – như cách vẫn hay tạo.

Chú ý: những class này phải có: một constructor rỗng, các thuộc tính phải đặt ở dạng private (vẫn tiêu chuẩn cũ)
Sau đó bắt đầu đánh dấu @annotation theo quy chuẩn sau:
Chú ý: những class này phải có: một constructor rỗng, các thuộc tính phải đặt ở dạng private (vẫn tiêu chuẩn cũ)
Sau đó bắt đầu đánh dấu @annotation theo quy chuẩn sau:
QUY CHUẨN ĐÁNH ANNOTATION
Những annotation nào không phải màu đen thì đó là những cái cơ bản cần đọc ngay.
Thêm nữa là những annotation này không tự nhiên mà có, tất cả phải import (không như @Override của Object sẵn). Mà cũng không phải lo vì nó sẽ tự động import thôi.
Bây giờ đi code thử:
CODE AND CONFIG POJO CLASS
Vì database Konoha “hơi” phức tạp cho việc demo kết nối (thực ra do tôi lười viết quá) nên tôi sẽ tạo ra một database mới có một bảng duy nhất (để tránh khởi tạo mối quan hệ - cái này sẽ nói sau).

Vì thế Konoha2 ra mắt với chỉ một bảng Ninjas:
Vì database Konoha “hơi” phức tạp cho việc demo kết nối (thực ra do tôi lười viết quá) nên tôi sẽ tạo ra một database mới có một bảng duy nhất (để tránh khởi tạo mối quan hệ - cái này sẽ nói sau).

Vì thế Konoha2 ra mắt với chỉ một bảng Ninjas:
Vì thế Konoha2 ra mắt với chỉ một bảng Ninjas:

Có một câu hỏi là tại sao trong JDBC lại không có phần liên kết này trong những class trong Models? Lời giải thích tôi có thể đưa ra bây giờ là JDBC không thực thi theo đối tượng – thực thể, mà đơn giản chỉ là thực thi câu lệnh SQL thuần – không hề tạo mô hình hướng đối tượng.
Sau khi tạo lại database mới, chỉnh lại đường dẫn trong file HibernateUtils (tự làm thôi)

Tạo một nơi để chứa các class POJO này, thay vì tạo Models như JavaSE thì hãy tạo một gói mới là pojo trong gói com.mycompany:
Sau khi tạo lại database mới, chỉnh lại đường dẫn trong file HibernateUtils (tự làm thôi)

Tạo một nơi để chứa các class POJO này, thay vì tạo Models như JavaSE thì hãy tạo một gói mới là pojo trong gói com.mycompany:
Tạo một nơi để chứa các class POJO này, thay vì tạo Models như JavaSE thì hãy tạo một gói mới là pojo trong gói com.mycompany:

Trong class Ninjas.java chúng ta tạo những thuộc tính và kiểu dữ liệu tương tự như Table Ninjas bên database.
Trong class Ninjas.java chúng ta tạo những thuộc tính và kiểu dữ liệu tương tự như Table Ninjas bên database.

Sau đó bắt đầu đánh dấu bằng annotation. Hiện tại chỉ có một bảng nên chúng ta sẽ đánh dấu những nơi sau: class, id, chiến lược tự tăng (nếu có), match từng thuộc tính (nếu tên thuộc tính trong class khác tên cột trong bảng trong Database), đánh dấu mối quan hệ (nói sau).
- Code sẽ như sau:

Đã gán xong annotation. Bây giờ hãy đăng ký với Hibernate là “tôi vừa cấu hình lớp Ninjas xong, hãy chấp nhận nó và map nó với Table Ninjas trong database đi” (vào HibernateUtils và thêm vào đó dòng sau: “conf.addAnnotatedClass(Ninjas.class);”):
Đã gán xong annotation. Bây giờ hãy đăng ký với Hibernate là “tôi vừa cấu hình lớp Ninjas xong, hãy chấp nhận nó và map nó với Table Ninjas trong database đi” (vào HibernateUtils và thêm vào đó dòng sau: “conf.addAnnotatedClass(Ninjas.class);”):

Viết xong rồi. Bây giờ vào GPT và order ngay một hàm test cơ bản (viết trong TestHibernate), là lấy tất cả thông tin lên:
Viết xong rồi. Bây giờ vào GPT và order ngay một hàm test cơ bản (viết trong TestHibernate), là lấy tất cả thông tin lên:
<
Và nếu may mắn thì chúng ta được cái này sau khi run:

Nhưng nếu không may mắn thì sao
Nhưng nếu không may mắn thì sao

Lỗi do sai chính tả chỗ cấu hình, hay:

Lỗi sai chính tả chỗ câu lệnh HQL(hả HQL á?)
Thế nên thêm chú ý phần này là phải code thật ĐÚNG CHÍNH TẢ.
Thế nên thêm chú ý phần này là phải code thật ĐÚNG CHÍNH TẢ.
MỞ RỘNG QUY MÔ DATABASE
Đã hết thời gian lười. Bây giờ quay lại database Konoha với vài mqh giữa các bảng.

Mối quan hệ trong Hibernate:

Trong database chúng ta có 3 mối quan hệ phổ biến: 1:1, 1:n, n:n. Sang bên Hibernate chúng ta chuyển sang 4 mối quan hệ: @OneToOne, @OneToMany, @ManyToOne, @ManyToMany.
Trong database chúng ta có 3 mối quan hệ phổ biến: 1:1, 1:n, n:n. Sang bên Hibernate chúng ta chuyển sang 4 mối quan hệ: @OneToOne, @OneToMany, @ManyToOne, @ManyToMany.
Xem xét lại mối quan hệ trong database

Hãy cùng xem xét lại mối quan hệ trong cơ sở dữ liệu.
Hãy để ý rằng trong mối quan hệ 1:n rõ ràng có mối quan hệ chủ tớ. Nhưng các mối quan hệ còn lại thì sao? Liệu rằng mối quan hệ 1:1 và n:n có thực sự là bình đẳng?
Thử lấy ví dụ về mối quan hệ 1:1 trong thực tế: ví dụ như một người chỉ có duy nhất một CCCD. Phân tích mối quan hệ: một người chỉ có một CCCD, một CCCD cũng chỉ thuộc về duy nhất một người. Tuy nhiên khi xét kỹ hơn, một người khi chết đi, CCCD của người đó sẽ bị bỏ chứ không thể tìm một người khác vào thế chỗ cho người đó để sở hữu số CCCD đó được. Nhưng khi một thẻ CCCD mất đi thì người chủ hoàn toàn có thể đi làm lại CCCD mới (ví dụ thế chứ tôi ko rõ luật pháp chỗ này lắm, nhưng về khoa học là có thể). => có thể thấy mối quan hệ 1:1 này không hề bình đẳng.
Thế thì sẽ xuất hiện một khái niệm mới trong các mqh: bên sở hữu/quản lý và bên bị sở hữu/bị quản lý. Theo suy nghĩ tự nhiên thì chắc chắn sẽ thấy bên con người sẽ là bên sở hữu còn bên CCCD là bên bị sở hữu.
Nhưng không.
Ngại đọc thì xuống phần kết luận và nhớ luôn.
Tôi đã cố gắng tìm hiểu trên GG và GPT nhưng chẳng nơi nào nói rõ cho tôi tại sao là không. (hoặc tôi chưa tìm hết). Nhưng có một điều quan trọng mà ở đâu tôi cũng thấy nói: bên nào có khóa ngoại bên đó được coi là bên sở hữu. Trong mối quan hệ 1:1 và n:n có thể khó thấy nhưng trong mối quan hệ 1:n có thể thấy rõ ràng, bên nhiều là bên có cột khóa ngoại và theo quy tắc trên đương nhiên nó là bên sở hữu. Nó cũng suy ra một chủ nô có nhiều nô lệ thì bên nô lệ lại là bên sở hữu?! Tại sao lại ngược với tư duy thông thường vậy?

Tôi đã đem toàn bộ gạch đầu dòng trên lên GPT, và sau mấy ngày có lẽ nó cũng đang hiểu tôi nói cái gì, và cũng đưa ra cho tôi một vài ý tưởng để giải thích.

Đầu tiên, phải nắm rõ JPA sẽ hoạt động theo cách: Bên bị sở hữu không trực tiếp lưu trữ thông tin về mối quan hệ, do đó, các thao tác liên quan đến mối quan hệ cần phải tham chiếu ngược lại tới bên sở hữu để xác định các liên kết. Điều này có thể làm cho các thao tác trở nên phức tạp hơn.
Với việc truy xuất một phần tử bên nhiều (nô lệ), theo như tư duy tự nhiên chúng ta sẽ đi tìm kiếm chủ của nó và hỏi. Muốn cập nhật một nô lệ, chúng ta cũng phải đi qua chủ nô của nó, để tìm xem nó ở đâu rồi cập nhật. Đó là khi đặt bên sở hữu/quản lý ở bên chủ nô (bên 1 trong mối quan hệ 1:n). Còn nếu làm ngược lại thì sao? Nếu đặt bên sở hữu là bên nô lệ, khi cập nhật hay truy xuất, chương trình sẽ bắt đầu từ bên nô lệ.
Giờ thử tưởng tượng có 100 chủ nô và mỗi chủ nô có 1000 nô lệ (con số tượng trưng).

Trường hợp để bên sở hữu là bên chủ nô, muốn cập nhật một nô lệ A, chương trình sẽ xét tất cả chủ nô, sau đó xét từng nô lệ của từng người xem ai là A rồi cập nhật. Khá tốn thời gian, trường hợp xấu nhất có thể là 100 *1000 lần tìm. Nếu muốn cập nhật một chủ nô, thì tìm ra là xong.
Trường hợp đặt bên nô lệ là bên sở hữu, muốn cập nhật A lập tức truy xuất được A để thực hiện. Tiếp tục đến trường hợp cập nhật một chủ nô, xấu nhất là đi tìm toàn bộ 100 * 999 +1 nô lệ để ra được chủ nô cần tìm.
Những trường hợp về cập nhật thì có thể nhìn bảng sau:
Trường hợp để bên sở hữu là bên chủ nô, muốn cập nhật một nô lệ A, chương trình sẽ xét tất cả chủ nô, sau đó xét từng nô lệ của từng người xem ai là A rồi cập nhật. Khá tốn thời gian, trường hợp xấu nhất có thể là 100 *1000 lần tìm. Nếu muốn cập nhật một chủ nô, thì tìm ra là xong.
Trường hợp đặt bên nô lệ là bên sở hữu, muốn cập nhật A lập tức truy xuất được A để thực hiện. Tiếp tục đến trường hợp cập nhật một chủ nô, xấu nhất là đi tìm toàn bộ 100 * 999 +1 nô lệ để ra được chủ nô cần tìm.
Những trường hợp về cập nhật thì có thể nhìn bảng sau:

Có thể thấy nếu tính số lượng truy vấn và số lượng thực thi sẽ không khác nhau mấy. Nhưng điều làm nên sự khác biệt, hãy đọc lại cách JPA hoạt động: không có sẵn dữ liệu của bên bị quản lý. Như vậy nếu bỏ ra một ít bộ nhớ để đổi lại sự sẵn sàng cho dữ liệu là sự ưu tiên của JPA. Chúng ta cũng thấy dễ làm việc hơn nếu nắm trong tay toàn bộ dữ liệu của nô lệ, trong đó mỗi nô lệ có một ánh xạ duy nhất tới chủ nô của nó, hơn là nắm trong tay dữ liệu các chủ nô và chỉ có thông tin ánh xạ tới các nô lệ.

Thêm nữa, nếu xử lý từ phía chủ nô, phải xử lý nhiều ánh xạ phức tạp vì nó là bên 1 trong mqh 1:n – nó có quan hệ trực tiếp tới nhiều thực thể khác. Còn với nô lệ chỉ có thông tin duy nhất, có thể coi là 1:1 (một chiều) tới các chủ nô (1 nô lệ chỉ thuộc về 1 chủ nô).
Có thể thấy nếu tính số lượng truy vấn và số lượng thực thi sẽ không khác nhau mấy. Nhưng điều làm nên sự khác biệt, hãy đọc lại cách JPA hoạt động: không có sẵn dữ liệu của bên bị quản lý. Như vậy nếu bỏ ra một ít bộ nhớ để đổi lại sự sẵn sàng cho dữ liệu là sự ưu tiên của JPA. Chúng ta cũng thấy dễ làm việc hơn nếu nắm trong tay toàn bộ dữ liệu của nô lệ, trong đó mỗi nô lệ có một ánh xạ duy nhất tới chủ nô của nó, hơn là nắm trong tay dữ liệu các chủ nô và chỉ có thông tin ánh xạ tới các nô lệ.

Thêm nữa, nếu xử lý từ phía chủ nô, phải xử lý nhiều ánh xạ phức tạp vì nó là bên 1 trong mqh 1:n – nó có quan hệ trực tiếp tới nhiều thực thể khác. Còn với nô lệ chỉ có thông tin duy nhất, có thể coi là 1:1 (một chiều) tới các chủ nô (1 nô lệ chỉ thuộc về 1 chủ nô).
Thêm nữa, nếu xử lý từ phía chủ nô, phải xử lý nhiều ánh xạ phức tạp vì nó là bên 1 trong mqh 1:n – nó có quan hệ trực tiếp tới nhiều thực thể khác. Còn với nô lệ chỉ có thông tin duy nhất, có thể coi là 1:1 (một chiều) tới các chủ nô (1 nô lệ chỉ thuộc về 1 chủ nô).
Phân tích nãy giờ chỉ để rút ra kết luận rằng: bên nhiều trong mối quan hệ 1:n là bên sở hữu, bên còn lại là bên bị sở hữu (ngược với tư duy thông thường). Các mối quan hệ khác sẽ xây dựng tương tự: ngược lại với quan hệ chủ - tớ theo tư duy thông thường. Từ ngữ “sở hữu” chỉ mang tính chất minh họa cho quan hệ: sở hữu quyền quản lý quan hệ trong chương trình, chứ không phải là sở hữu trong trường hợp thực tế.
@OneToOne
Nó tương đương với mối quan hệ 1:1 trong database.
Mô tả cách tạo class POJO cho quan hệ 1:1 như sau:

Trong đó: B là chủ sở hữu (thực tế) và là bên bị sở hữu (với JPA). A thì ngược lại.
Giải thích:
Trong class A:
Annotation @OneToOne: đánh dấu mối quan hệ 1:1 đối với class B.

cascade: Định nghĩa các hành động cascade – mối quan hệ “thác nước” – cái này đổ lên đầu cái kia. Với giá trị CascadeType.ALL, tất cả các hành động (PERSIST, MERGE, REMOVE, REFRESH, DETACH) sẽ được truyền từ thực thể chủ sở hữu thực tế tới các thực thể liên kết. Điều này có nghĩa là khi lưu, cập nhật hoặc xóa một B, các hành động tương tự sẽ được áp dụng cho A liên kết.
Annotation @OneToOne: đánh dấu mối quan hệ 1:1 đối với class B.

cascade: Định nghĩa các hành động cascade – mối quan hệ “thác nước” – cái này đổ lên đầu cái kia. Với giá trị CascadeType.ALL, tất cả các hành động (PERSIST, MERGE, REMOVE, REFRESH, DETACH) sẽ được truyền từ thực thể chủ sở hữu thực tế tới các thực thể liên kết. Điều này có nghĩa là khi lưu, cập nhật hoặc xóa một B, các hành động tương tự sẽ được áp dụng cho A liên kết.
cascade: Định nghĩa các hành động cascade – mối quan hệ “thác nước” – cái này đổ lên đầu cái kia. Với giá trị CascadeType.ALL, tất cả các hành động (PERSIST, MERGE, REMOVE, REFRESH, DETACH) sẽ được truyền từ thực thể chủ sở hữu thực tế tới các thực thể liên kết. Điều này có nghĩa là khi lưu, cập nhật hoặc xóa một B, các hành động tương tự sẽ được áp dụng cho A liên kết.
những hành động này cụ thể là gì, có thể nói sau.
fetch dùng để quyết định xem có load thêm dữ liệu của B tương đương với đối tượng A được gọi đó hay không. Có 2 loại là FetchType.EAGER – có load và FetchType.LAZY – không load thêm.
fetch dùng để quyết định xem có load thêm dữ liệu của B tương đương với đối tượng A được gọi đó hay không. Có 2 loại là FetchType.EAGER – có load và FetchType.LAZY – không load thêm.
Vì là mối quan hệ 1:1 nên khi load một A chỉ có một B được load cùng – có thể để là EAGER.
orphanRemoval quyết định các thực thể con sẽ bị xóa tự động khi chúng không còn liên kết với thực thể cha. Giá trị của nó là true/false.

Annotation @JoinColumn: đánh dấu cột khóa ngoại trỏ sang bên B.
name = “B_ID”: đánh dấu tên cột ứng với thuộc tính (mà ở đây là đối tượng b trong class A)
unique = “true”: vì mqh là 1:1 nên phải giữ unique cho khóa ngoại, nghĩa là A khác nhau không thể có B giống nhau.
orphanRemoval quyết định các thực thể con sẽ bị xóa tự động khi chúng không còn liên kết với thực thể cha. Giá trị của nó là true/false.

Annotation @JoinColumn: đánh dấu cột khóa ngoại trỏ sang bên B.
name = “B_ID”: đánh dấu tên cột ứng với thuộc tính (mà ở đây là đối tượng b trong class A)
unique = “true”: vì mqh là 1:1 nên phải giữ unique cho khóa ngoại, nghĩa là A khác nhau không thể có B giống nhau.
Annotation @JoinColumn: đánh dấu cột khóa ngoại trỏ sang bên B.
name = “B_ID”: đánh dấu tên cột ứng với thuộc tính (mà ở đây là đối tượng b trong class A)
unique = “true”: vì mqh là 1:1 nên phải giữ unique cho khóa ngoại, nghĩa là A khác nhau không thể có B giống nhau.
Trong class B:
@OneToOne(mappedBy = ""b""): Chỉ định rằng B không sở hữu mối quan hệ mà mối quan hệ được ánh xạ bởi thuộc tính b trong thực thể A. Đây là bên không sở hữu, và mappedBy thường sẽ ở bên không sở hữu.

fetch: như trên.
@OneToOne(mappedBy = ""b""): Chỉ định rằng B không sở hữu mối quan hệ mà mối quan hệ được ánh xạ bởi thuộc tính b trong thực thể A. Đây là bên không sở hữu, và mappedBy thường sẽ ở bên không sở hữu.

fetch: như trên.
fetch: như trên.
Câu hỏi đặt ra là tại sao quan hệ 1:1 mà mối quan hệ lại có cách thiết lập khác nhau trong mỗi lớp như vậy? Nếu thiết lập giống nhau thì có ổn không?
Chúng thiết lập khác nhau vì có phân chia giai cấp rõ ràng: chủ - tớ chứ không hề ngang hàng. Khi thiết lập cascade ở bên A trỏ sang bên B thì khi xóa B, A tự động xóa và không có chiều ngược lại.
Thiết lập 2 bên giống nhau có ổn không? Nếu cho cả 2 bên cùng có cascade thì gây ra lặp mối quan hệ: bị vòng lặp cascade trỏ vô hạn (cái này cũng dễ hiểu).
Chỉ có cách thiết lập ĐỒNG BỘ CÙNG NHAU: nghĩa là không cần phải công bằng từ bước thiết lập mối quan hệ, nhưng sẽ thiết lập câu lệnh sao cho chúng được bình đẳng (thiết lập thêm trigger hay dùng thẳng câu lệnh, …) cái này nếu có gặp thì nói sau.
@OneToMany và @ManyToOne
Hai mối quan hệ này tương đương với mối quan hệ 1:n trong database.
Mô tả cách tạo class POJO cho quan hệ 1:n như sau:

GIải thích:

Trong bảng A có A_ID, trong bảng B có B_ID và A_ID làm khóa ngoại. Ở đây B là bên sở hữu còn A là bên bị sở hữu.
Trong bảng A có A_ID, trong bảng B có B_ID và A_ID làm khóa ngoại. Ở đây B là bên sở hữu còn A là bên bị sở hữu.
Trong class A:
Dùng annotation @OneToMany.

mappedBy dùng để trỏ tới thuộc tính khóa ngoại được đặt trong class B.
cascade: “thuộc tính thác nước đổ” – như phần trước.
orphanRemoval quyết định các thực thể con sẽ bị xóa tự động khi chúng không còn liên kết với thực thể cha. Giá trị của nó là true/false.
Dùng annotation @OneToMany.

mappedBy dùng để trỏ tới thuộc tính khóa ngoại được đặt trong class B.
cascade: “thuộc tính thác nước đổ” – như phần trước.
orphanRemoval quyết định các thực thể con sẽ bị xóa tự động khi chúng không còn liên kết với thực thể cha. Giá trị của nó là true/false.
Dùng annotation @OneToMany.

mappedBy dùng để trỏ tới thuộc tính khóa ngoại được đặt trong class B.
cascade: “thuộc tính thác nước đổ” – như phần trước.
orphanRemoval quyết định các thực thể con sẽ bị xóa tự động khi chúng không còn liên kết với thực thể cha. Giá trị của nó là true/false.
mappedBy dùng để trỏ tới thuộc tính khóa ngoại được đặt trong class B.
cascade: “thuộc tính thác nước đổ” – như phần trước.
orphanRemoval quyết định các thực thể con sẽ bị xóa tự động khi chúng không còn liên kết với thực thể cha. Giá trị của nó là true/false.
Trong class B:
Dùng annotation @ManyToOne và @JoinColumn.

fetch dùng để quyết định xem có load thêm dữ liệu của A tương đương với đối tượng B được gọi đó hay không. Có 2 loại là FetchType.EAGER – có load và FetchType.LAZY – không load thêm.
Dùng annotation @ManyToOne và @JoinColumn.

fetch dùng để quyết định xem có load thêm dữ liệu của A tương đương với đối tượng B được gọi đó hay không. Có 2 loại là FetchType.EAGER – có load và FetchType.LAZY – không load thêm.
Dùng annotation @ManyToOne và @JoinColumn.

fetch dùng để quyết định xem có load thêm dữ liệu của A tương đương với đối tượng B được gọi đó hay không. Có 2 loại là FetchType.EAGER – có load và FetchType.LAZY – không load thêm.
fetch dùng để quyết định xem có load thêm dữ liệu của A tương đương với đối tượng B được gọi đó hay không. Có 2 loại là FetchType.EAGER – có load và FetchType.LAZY – không load thêm.
Thuộc tính này cũng có trong A nhưng mặc định để là LAZY, vì A 1:n B nên nếu gọi A và gọi thêm B thì quá nhiều B xuất hiện gây chậm hệ thống. Nên tôi cũng thích để mặc định luôn.
name đây là thuộc tính thông thường của mỗi cột. Chú ý trong SQL thì cột này có kiểu dữ liệu tương tự như id bên A nhưng ở class POJO nó có kiểu dữ liệu là đối tượng A luôn. Nên thay vì là private int A_ID như các cột khác thì nó lại là private A a.
Khi xây dựng class, quy tắc là trong A chứa một List (hoặc Set) các B, còn trong B chứa một thuộc tính của A.
name đây là thuộc tính thông thường của mỗi cột. Chú ý trong SQL thì cột này có kiểu dữ liệu tương tự như id bên A nhưng ở class POJO nó có kiểu dữ liệu là đối tượng A luôn. Nên thay vì là private int A_ID như các cột khác thì nó lại là private A a.
Khi xây dựng class, quy tắc là trong A chứa một List (hoặc Set) các B, còn trong B chứa một thuộc tính của A.
name đây là thuộc tính thông thường của mỗi cột. Chú ý trong SQL thì cột này có kiểu dữ liệu tương tự như id bên A nhưng ở class POJO nó có kiểu dữ liệu là đối tượng A luôn. Nên thay vì là private int A_ID như các cột khác thì nó lại là private A a.
Khi xây dựng class, quy tắc là trong A chứa một List (hoặc Set) các B, còn trong B chứa một thuộc tính của A.
VD: ai cũng biết một Teams gồm nhiều Ninjas.
Trong class Teams:

Trong class Ninjas:

@ManyToMany
Đây là / mqh cơ bản / phức tạp nhất.
Mô tả cách tạo class POJO cho quan hệ n:n như sau:

Giải thích:
A và B có quan hệ n:n trong đó A là vật còn B là chủ, suy ra A là bên sở hữu còn B là bên bị sở hữu.
Trong class A (mang tính sở hữu):
Chứa một Set các đối tượng B ứng với mỗi đối tượng A đang xét: private Set<B> bs. (không quan trọng lắm Set hay List tùy thuộc vào người dùng).

Dùng annotation @ManyToMany để đánh dấu mqh, @JoinTable để xác định bảng quan hệ (bảng thứ 3).
Thuộc tính trong @JoinTable:

name: tên bảng thứ 3
joinColumns = { @JoinColumn(name = ""A_ID"") }: cột trong bảng thứ 3 trỏ tới bảng A, mang tính sở hữu.
inverseJoinColumns = { @JoinColumn(name = ""B_ID"") }: cột trong bảng thứ 3 trỏ tới bảng B, mang tính bị sở hữu.
Chứa một Set các đối tượng B ứng với mỗi đối tượng A đang xét: private Set<B> bs. (không quan trọng lắm Set hay List tùy thuộc vào người dùng).

Dùng annotation @ManyToMany để đánh dấu mqh, @JoinTable để xác định bảng quan hệ (bảng thứ 3).
Thuộc tính trong @JoinTable:

name: tên bảng thứ 3
joinColumns = { @JoinColumn(name = ""A_ID"") }: cột trong bảng thứ 3 trỏ tới bảng A, mang tính sở hữu.
inverseJoinColumns = { @JoinColumn(name = ""B_ID"") }: cột trong bảng thứ 3 trỏ tới bảng B, mang tính bị sở hữu.
Chứa một Set các đối tượng B ứng với mỗi đối tượng A đang xét: private Set<B> bs. (không quan trọng lắm Set hay List tùy thuộc vào người dùng).

Dùng annotation @ManyToMany để đánh dấu mqh, @JoinTable để xác định bảng quan hệ (bảng thứ 3).
Thuộc tính trong @JoinTable:

name: tên bảng thứ 3
joinColumns = { @JoinColumn(name = ""A_ID"") }: cột trong bảng thứ 3 trỏ tới bảng A, mang tính sở hữu.
inverseJoinColumns = { @JoinColumn(name = ""B_ID"") }: cột trong bảng thứ 3 trỏ tới bảng B, mang tính bị sở hữu.
Dùng annotation @ManyToMany để đánh dấu mqh, @JoinTable để xác định bảng quan hệ (bảng thứ 3).
Thuộc tính trong @JoinTable:

name: tên bảng thứ 3
joinColumns = { @JoinColumn(name = ""A_ID"") }: cột trong bảng thứ 3 trỏ tới bảng A, mang tính sở hữu.
inverseJoinColumns = { @JoinColumn(name = ""B_ID"") }: cột trong bảng thứ 3 trỏ tới bảng B, mang tính bị sở hữu.
name: tên bảng thứ 3
joinColumns = { @JoinColumn(name = ""A_ID"") }: cột trong bảng thứ 3 trỏ tới bảng A, mang tính sở hữu.
inverseJoinColumns = { @JoinColumn(name = ""B_ID"") }: cột trong bảng thứ 3 trỏ tới bảng B, mang tính bị sở hữu.
Trong class B (mang tính bị sở hữu):
Chứa một Set các đối tượng A ứng với mỗi đối tượng B đang xét: private Set<A> as.

Dùng annotation @ManyToMany để đánh dấu mqh. Ở đây KHÔNG CÓ @JoinTable.
Trong @ManyToMany chỉ chứa một thuộc tính mappedBy.
mappedBy = “bs”: trỏ tới List bs bên A, nơi mà các đối tượng B được quản lý.
Chứa một Set các đối tượng A ứng với mỗi đối tượng B đang xét: private Set<A> as.

Dùng annotation @ManyToMany để đánh dấu mqh. Ở đây KHÔNG CÓ @JoinTable.
Trong @ManyToMany chỉ chứa một thuộc tính mappedBy.
mappedBy = “bs”: trỏ tới List bs bên A, nơi mà các đối tượng B được quản lý.
Chứa một Set các đối tượng A ứng với mỗi đối tượng B đang xét: private Set<A> as.

Dùng annotation @ManyToMany để đánh dấu mqh. Ở đây KHÔNG CÓ @JoinTable.
Trong @ManyToMany chỉ chứa một thuộc tính mappedBy.
mappedBy = “bs”: trỏ tới List bs bên A, nơi mà các đối tượng B được quản lý.
Dùng annotation @ManyToMany để đánh dấu mqh. Ở đây KHÔNG CÓ @JoinTable.
Trong @ManyToMany chỉ chứa một thuộc tính mappedBy.
mappedBy = “bs”: trỏ tới List bs bên A, nơi mà các đối tượng B được quản lý.
Trong class A_B (bảng liên kết):
Đây có thể coi là bảng mà A 1:n nó và B 1:n nó.

Bảng này có 2 thuộc tính liên kết, đồng thời là 2 khóa chính của bảng.
Do đó sử dụng 2 annotation @ManyToOne, biểu diễn liên kết với 2 bảng chính nhờ 2 cột khóa ngoại (@JoinColumn) thuộc tính name = “A_ID” và name = “B_ID”.
Đây có thể coi là bảng mà A 1:n nó và B 1:n nó.

Bảng này có 2 thuộc tính liên kết, đồng thời là 2 khóa chính của bảng.
Do đó sử dụng 2 annotation @ManyToOne, biểu diễn liên kết với 2 bảng chính nhờ 2 cột khóa ngoại (@JoinColumn) thuộc tính name = “A_ID” và name = “B_ID”.
Đây có thể coi là bảng mà A 1:n nó và B 1:n nó.

Bảng này có 2 thuộc tính liên kết, đồng thời là 2 khóa chính của bảng.
Do đó sử dụng 2 annotation @ManyToOne, biểu diễn liên kết với 2 bảng chính nhờ 2 cột khóa ngoại (@JoinColumn) thuộc tính name = “A_ID” và name = “B_ID”.
Bảng này có 2 thuộc tính liên kết, đồng thời là 2 khóa chính của bảng.
Do đó sử dụng 2 annotation @ManyToOne, biểu diễn liên kết với 2 bảng chính nhờ 2 cột khóa ngoại (@JoinColumn) thuộc tính name = “A_ID” và name = “B_ID”.
VD code:
Mối quan hệ Teams n:n Missions:
Missions:

Teams:

TeamJoinInMissions:

Chưa hiểu lắm? Tại sao những annotation và thuộc tính của chúng lại được sắp xếp như vậy?
THÊM
@JoinColumn và mappedBy
Bên sở hữu và bên bị sở hữu
Hai cột @JoinColumn và mappedBy nếu để ý thì nó luôn luôn nằm khác phía nhau trong bên sở hữu/ bị sở hữu. Hãy xét mối quan hệ 1:n, để ý tiếp thì JoinColumn luôn nằm trong bên nào có cột khóa ngoại, và bên này được gọi là “bên sở hữu”.
Sắp xếp các cột
Trước hết hãy xem định nghĩa:
@JoinColumn được sử dụng để chỉ định cột khóa ngoại (foreign key) trong bảng cơ sở dữ liệu mà liên kết đến một bảng khác.
mappedBy được sử dụng trong mối quan hệ hai chiều (bidirectional relationships) để chỉ định rằng thực thể hiện tại là bên bị sở hữu (inverse side) và tên thuộc tính ở bên sở hữu (owning side) mà nó ánh xạ tới.
Vì vậy @JoinColumn luôn nằm ở bên sở hữu còn mappedBy nằm ở bên bị sở hữu. Hoặc có thể nhớ đơn giản hơn, trong thực tế thì đối tượng nào là chủ thì sẽ có thuộc tính mappedBy trong annotation, còn đối tượng nào là nô lệ thì sẽ có @JoinColumn và những thuộc tính đi kèm để điều khiển đồng bộ.
Các thuộc tính hay gặp và vị trí
5, Sau khi kết nối thành công thì đến việc học cách thao tác với dữ liệu.
(tức là viết file Test_Hibernate.java mà nãy giờ mình nhờ GPT viết).
Nhắc lại các thao tác nãy giờ đã làm:
Tạo dự án Maven
Thêm thư viện/gói phục vụ Hibernate và JDBC trong file cấu hình toàn bộ chương trình.
Xây dựng file Utils, dùng những thư viện trên để kết nối với CSDL
Xây dựng các file POJO và đánh annotation
Test chương trình
Sau khi viết hết các bước đầu, chúng ta đến bước cuối cùng là viết test chương trình.
Quy trình:

Nhìn lại vòng đời của một chương trình sử dụng JPA (hình này ở bên trên). Tất cả những gì mình cần thực hiện bây giờ là trong phần chữ “executes” kia.

Còn đây là cấu tạo chung của một chương trình Hibernate, nghĩa là phần “executes” kia sẽ tương tự cái này.
Khi một request đến server, một EntityManager được hình thành. Trong chương trình Hibernate thì nó tương đương với một Session được hình thành.
Sau đó có thể thực thi các lệnh truy vấn, transaction, … bên trong một Session.
Sau khi thực thi xong thì đóng EntityManager lại, hay đúng hơn với Hibernate là đóng Session lại.
Tuy nhiên tôi không thích dùng nhiều các lệnh linh tinh bên trong Session. Cộng thêm việc nó hỗ trợ transaction nên tôi thích cách dùng gói tất cả lệnh vào trong transaction, kể cả lệnh query không cần bảo mật.
Vậy nên quy trình trong một hàm main (sau này là một method chức năng nào đó) sẽ như sau:
Mở Session -> Mở Transaction -> CRUD -> commit/rollback transaction -> đóng Session
Code template:
Những bước cơ bản có như sau:

Trong đó:
Session mở từ Factory.
Transaction mở từ Session.
Session:
org.hibernate.Session
Session trong Hibernate là một interface đại diện cho một phiên làm việc giữa ứng dụng Java và cơ sở dữ liệu.
Nhiệm vụ chính của Session là thực hiện các thao tác tạo, đọc, và xóa trên các đối tượng của các lớp thực thể đã được ánh xạ (mapped entity classes).
Trong Session chứa transaction thực hiện tất cả câu lệnh tác động đến các thực thể, và các câu lệnh đó cũng do session thực thi. Có thể hiểu như Session mở một Transaction, rồi Session thực hiện tiếp các hành động trong đó, bao gồm việc mở Query, rồi nó đóng Transaction.
Để thực hiện các hành động đó thì Session có các method sau:
https://javadoc.io/doc/org.hibernate/hibernate-core/5.6.15.Final/org/hibernate/Session.html
(docs này chưa có ở phiên bản hiện tại đâu)
Tôi sẽ trích ra vài hàm mà (tôi nghĩ là) sẽ dùng nhiều. Từ trên xuống dưới theo thứ tự thực thi nhé:
Transaction beginTransaction(): mở transaction. Đóng bằng cách cho transaction commit() hoặc rollback().
Query<T> createQuery(String hql): tạo một truy vấn HQL. Chỉ tạo thôi chưa làm gì cả.
void persist(Object object): tạm thời dùng cái này để lưu một đối tượng mới vào CSDL, mặc dù ý định của cái này là đưa đối tượng vào trạng thái được quản lý, nhưng save() lỗi thời.
Object merge​(Object object): copy một đối tượng cũ đã detached vào một đối tượng mới persistent hiện có trong Session. Nó tương đương với UPDATE hoặc INSERT tùy trường hợp, và dùng thay update() lỗi thời.
void remove(Object object): xóa thực thể ra khỏi CSDL. Đúng hơn là chuyển đối tượng đó từ trạng thái persistent thành transient. Tương đương với một câu lệnh DELETE. Dùng thay delete() lỗi thời.
<T> get(Class<T> clazz, Serializable id): truy vấn và trả về một đối tượng dựa trên khóa chính (id). Nếu không tìm thấy, nó trả về null. Cái này cũng khá ổn.
void flush(): đẩy tất cả hành động đang chần chừ (insert update delete) thực hiện ngay.
void lock(Object entity, LockMode lockMode): Đặt một khóa trên đối tượng entity với chế độ khóa lockMode, kiểm soát việc đồng thời truy cập vào đối tượng. Chắc chắn sẽ dùng.
clear(), close(),…
Những hàm trên đây khá quan trọng và cần ghi nhớ.
Chi tiết hành động trong Transaction:
Trong Transaction chúng ta phải thực thi các hành động CRUD đối với database thông qua thao tác với Entity. Bây giờ hãy nhìn lại các trạng thái có thể có của một Entity trong vòng đời của Transaction:

ảnh này có ở phần JPA (Java Persistence API).
Ở trên có nói về phần này rồi nhưng chắc chắn là chưa rõ ràng và chưa đi sát vào chương trình (vì chưa thử code). Bây giờ sẽ nói lại phần này.
Các trạng thái của thực thể trong một Transaction:
Transient

Khi tạo mới đối tượng bằng từ khóa new, đối tượng sẽ được sinh ra và ở dưới dạng Transient.
Khi tạo mới, đối tượng chưa được liên kết với session nào, đồng nghĩa với việc nó chưa được Hibernate quản lý, chưa thể tồn tại trong database.
Nếu muốn chuyển sang trạng thái Persistent để đồng bộ hóa với database sau commit thì phải dùng lệnh: save, persist, …(tùy phiên bản Hibernate nhưng nội dung lệnh sẽ như thế)
Nếu không có hành động gì thì những đối tượng dạng này bị xóa khỏi chương trình sau commit.
Code VD:
Khi tạo mới đối tượng bằng từ khóa new, đối tượng sẽ được sinh ra và ở dưới dạng Transient.
Khi tạo mới, đối tượng chưa được liên kết với session nào, đồng nghĩa với việc nó chưa được Hibernate quản lý, chưa thể tồn tại trong database.
Nếu muốn chuyển sang trạng thái Persistent để đồng bộ hóa với database sau commit thì phải dùng lệnh: save, persist, …(tùy phiên bản Hibernate nhưng nội dung lệnh sẽ như thế)
Nếu không có hành động gì thì những đối tượng dạng này bị xóa khỏi chương trình sau commit.
Code VD:

Persistent

Khi gọi lệnh save, persist (từ đối tượng Transient) hay get (từ database), đối tượng sẽ ở dạng Persistent.
Ở trạng thái này đối tượng được Hibernate để mắt tới và được đồng bộ hóa sau khi transaction commit.
Sau khi commit, các đối tượng Persistent (sau khi đã đồng bộ hóa với database) sẽ chuyển sang trạng thái Detached – sẵn sàng bị vứt đi nếu không được cứu vớt.
Code VD:
Khi gọi lệnh save, persist (từ đối tượng Transient) hay get (từ database), đối tượng sẽ ở dạng Persistent.
Ở trạng thái này đối tượng được Hibernate để mắt tới và được đồng bộ hóa sau khi transaction commit.
Sau khi commit, các đối tượng Persistent (sau khi đã đồng bộ hóa với database) sẽ chuyển sang trạng thái Detached – sẵn sàng bị vứt đi nếu không được cứu vớt.
Code VD:
(đang đợi màn combat giữa save và persist vì chức năng tương tự nhưng :


Theo bản docs mới nhất thì save() đã chết.
https://docs.jboss.org/hibernate/orm/6.6/introduction/html_single/Hibernate_Introduction.html
vậy ở đây sẽ chỉ nói về persist và get.
persist(Object): chuyển trạng thái của đối tượng: đang từ transient (tự do) về persistent (được quản lý bởi hibernate).
persist(Object): chuyển trạng thái của đối tượng: đang từ transient (tự do) về persistent (được quản lý bởi hibernate).
persist(Object): chuyển trạng thái của đối tượng: đang từ transient (tự do) về persistent (được quản lý bởi hibernate).
VD:

get(Class, id): lấy đối tượng lên từ database theo id của nó trong database.
get(Class, id): lấy đối tượng lên từ database theo id của nó trong database.
get(Class, id): lấy đối tượng lên từ database theo id của nó trong database.

Còn rất rất nhiều method, hãy tự tìm hiểu/tự tra nếu cần thiết hoặc nếu rảnh.
Detached

Khi đóng session, đối tượng hiện tại đang được xử lý sẽ chuyển từ Persistent sang dạng Detached.
Một đối tượng ở dạng Detached sẽ sẵn sàng cho việc bị xóa khỏi vòng đời của request đó, trừ khi sau khi session1 đóng thì có session2 mở ra và dùng update, merge hoặc saveOrUpdate.
Nghĩa là session1 đã loại bỏ nó và nó sẽ bị vứt đi nếu không có session khác cứu nó.
Nói kỹ hơn chút về 3 method trên:

update(): Khi có một đối tượng Detached và muốn gắn lại nó với session khác để tiếp tục quản lý.
Khi đóng session, đối tượng hiện tại đang được xử lý sẽ chuyển từ Persistent sang dạng Detached.
Một đối tượng ở dạng Detached sẽ sẵn sàng cho việc bị xóa khỏi vòng đời của request đó, trừ khi sau khi session1 đóng thì có session2 mở ra và dùng update, merge hoặc saveOrUpdate.
Nghĩa là session1 đã loại bỏ nó và nó sẽ bị vứt đi nếu không có session khác cứu nó.
Nói kỹ hơn chút về 3 method trên:

update(): Khi có một đối tượng Detached và muốn gắn lại nó với session khác để tiếp tục quản lý.
update(): Khi có một đối tượng Detached và muốn gắn lại nó với session khác để tiếp tục quản lý.
Nếu đối tượng này đã tồn tại trong session hiện tại hoặc có một đối tượng khác cùng ID trong session, sẽ xảy ra lỗi NonUniqueObjectException.
merge(): Hibernate tạo ra một bản sao của đối tượng Detached và bản sao này trở thành Persistent. Bản gốc vẫn là Detached.
merge(): Hibernate tạo ra một bản sao của đối tượng Detached và bản sao này trở thành Persistent. Bản gốc vẫn là Detached.
merge(): Hibernate tạo ra một bản sao của đối tượng Detached và bản sao này trở thành Persistent. Bản gốc vẫn là Detached.
Không yêu cầu đối tượng phải có ID hợp lệ. Nếu ID null, Hibernate sẽ lưu đối tượng như một bản ghi mới.
saveOrUpdate(): Nếu đối tượng đã tồn tại trong cơ sở dữ liệu (Detached), nó sẽ được cập nhật (update()). Nếu đối tượng chưa tồn tại (Transient), nó sẽ được lưu mới (save()). (Chắc tôi dùng method này suốt)
Code VD:
saveOrUpdate(): Nếu đối tượng đã tồn tại trong cơ sở dữ liệu (Detached), nó sẽ được cập nhật (update()). Nếu đối tượng chưa tồn tại (Transient), nó sẽ được lưu mới (save()). (Chắc tôi dùng method này suốt)
Code VD:
saveOrUpdate(): Nếu đối tượng đã tồn tại trong cơ sở dữ liệu (Detached), nó sẽ được cập nhật (update()). Nếu đối tượng chưa tồn tại (Transient), nó sẽ được lưu mới (save()). (Chắc tôi dùng method này suốt)
Code VD:
Thôi để sau đi, cái này hiểu lý thuyết đã.
HQL:
https://docs.jboss.org/hibernate/orm/6.6/querylanguage/html_single/Hibernate_Query_Language.html
Có rất nhiều thứ phải học ở đây nhưng có lẽ ở đây chỉ viết được một chút.

GIỚI THIỆU: NGÔN NGỮ TRUY VẤN HIBERNATE – HIBERNATE QUERY LANGUAGE
Định nghĩa và tổng quan
- Theo GPT thì: HQL là một ngôn ngữ truy vấn cấp cao được thiết kế để truy vấn các đối tượng trong Hibernate. Thay vì làm việc với các bảng và cột trong cơ sở dữ liệu, HQL làm việc với các đối tượng và thuộc tính của chúng. Điều này giúp bạn giữ cho mã của mình gần gũi với mô hình dữ liệu của ứng dụng và độc lập hơn với cơ sở dữ liệu cụ thể.
- Trong cách kết nối với JDBC thì SQL được viết trực tiếp vào chương trình Java dưới dạng chuỗi:

Thì HQL cũng thế, cũng được đưa vào một String để thực thi như một câu truy vấn SQL.
Cụ thể hơn thì HQL được dùng ở các class DAO, vv (vì chưa biết nó ở đâu thêm nữa)
Cú pháp của HQL: chắc sẽ không nói nhiều về cái này vì nó gần gần SQL. Khác biệt lớn nhất là thay vì làm việc trực tiếp với các table của database thì nó làm việc với các entity của hibernate.
Viết mấy cái này với mặc định người đọc đã biết SQL một chút.
Cú pháp
1, Khi nào dùng select và không
Khi dùng HQL cơ bản có 2 trường hợp truy vấn là dùng “select” hoặc không. VD:
Khi dùng HQL cơ bản có 2 trường hợp truy vấn là dùng “select” hoặc không. VD:

Hoặc là:

Hãy tạm bỏ qua phần hàm bị lỗi thời, chú ý đến câu lệnh HQL. Khi dùng “select” thì lấy ra thuộc tính chỉ định còn khi không dùng “select”, viết mỗi ”from” thì lấy ra tất cả dữ liệu của nhóm đối tượng đó.
Trong HQL không hỗ trợ câu lệnh “select *”:

Lý do có thể như sau: HQL làm việc với các đối tượng chứ không phải các bảng. Với SQL, khi truy vấn đến bảng, câu lệnh “from” chỉ trỏ tới tên bảng chứ không đi vào từng thuộc tính. Với HQL khi làm việc với các đối tượng, “from” nghĩa là đã gọi đối tượng đó ra, và trong đối tượng có tất cả thông tin của bảng đó.
2, Interface Query<T>
org.hibernate.query.Query
Không liên quan đến cú pháp nhưng nêu trước cho các phần VD sau còn hiểu được.
Trong docs định nghĩa đơn giản: là một interface để quản lý các truy vấn.
Cụ thể hơn, một Query được sinh ra khi gọi hàm sinh từ Session (createQuery…), sau đó nó có chức năng: thiết lập tham số, thực thi câu lệnh HQL, xử lý xuất kết quả. Sau đó nó bị xóa nếu Session kết thúc.
VD:

Trong docs có định nghĩa nhiều method nhưng có một số method cơ bản (dựa vào chức năng trên) như sau:

int executeUpdate(): thực thi câu lệnh update hoặc delete, trả về số thực thể bị ảnh hưởng (~ row affected)
Query<T> setParameter(): thiết lập tham số cho câu lệnh HQL được truyền vào đối tượng Query đó. Có vẻ chức năng này giống ở bên PreparedStatement để tránh inject trực tiếp vào câu lệnh HQL. Điều này nghiễm nhiên tránh “HQLi” (mà có không ấy nhỉ)
List list(): thực thi câu lệnh HQL có SELECT và trả về một List.
int executeUpdate(): thực thi câu lệnh update hoặc delete, trả về số thực thể bị ảnh hưởng (~ row affected)
Query<T> setParameter(): thiết lập tham số cho câu lệnh HQL được truyền vào đối tượng Query đó. Có vẻ chức năng này giống ở bên PreparedStatement để tránh inject trực tiếp vào câu lệnh HQL. Điều này nghiễm nhiên tránh “HQLi” (mà có không ấy nhỉ)
List list(): thực thi câu lệnh HQL có SELECT và trả về một List.
Vv và vv. Còn nhiều lắm. Nhưng hầu hết đã lỗi thời, còn cơ bản để dùng chắc có 3 cái trên (đoán là) hay được dùng nhất. Đọc thêm ở đây: https://docs.jboss.org/hibernate/orm/5.6/javadocs/org/hibernate/query/Query.html
(tôi thấy hối hận vì xây dựng DB phức tạp không demo nổi, vì tôi quên hết rồi)
(bonus)
Tôi chắc là sẽ xây lại database cho thật đơn giản để demo cho nhanh.
DROP SCHEMA IF EXISTS mf;
CREATE SCHEMA mf;
USE mf;
CREATE TABLE Classes (
classID nvarchar(10) primary key,
className nvarchar(20),
classType nvarchar(5) -- c/cc
);
CREATE TABLE Students (
studentID int primary key auto_increment,
studentName nvarchar(50),
studentGender nvarchar(10),
studentDetails nvarchar(200),
classID nvarchar(10),
foreign key (classID) references Classes(ClassID)
)
3, Tham số trong HQL
Tham số trong HQL là đại diện cho giá trị trong câu lệnh HQL (chứ không phải cho tên cột hay tên bảng, …) vì HQL không muốn bị injection. Nhớ kỹ cái này.
Có hai cách để sử dụng tham số trong HQL:
Tham số vị trí (Positional Parameters): Sử dụng dấu ? và thứ tự chỉ số (bắt đầu từ 1).
Tham số tên (Named Parameters): Sử dụng dấu : và tên tham số.
Mỗi loại tham số có một cách truyền khác nhau. Xem VD tự hiểu:

Và

4, Join trong HQL
Mỗi khi viết câu lệnh HQL nào cũng nên nhớ nó dành cho các đối tượng. Và JOIN cũng vậy.
Có 5 loại join trong SQL: inner, left, right, fullouter, cross. HQL cũng có nhưng chia ra thành phần khác một chút:
Join thường: các loại hay gặp trong SQL như trên. Với cú pháp hơi khác:
SELECT a.property1, b.property2
FROM A a
[INNER | LEFT | RIGHT] JOIN a.B b
Join Fetch: tải trước (eager fetching) các thực thể liên quan, giảm thiểu số lượng truy vấn cần thực hiện. Chưa rõ nó có liên quan đến phần lựa chọn (fetch=”FetchType.EAGER”) mà trước đó có nói hay không, nhưng sẽ tìm hiểu cái này sau.
SELECT a
FROM A a
JOIN FETCH a.B b
Implicit join: Join tự động mà không cần dùng từ khóa “Join” trong câu truy vấn. Trong mỗi đối tượng nếu bảng tương ứng có cột khóa ngoại liên kết thì đối tượng đó cũng có thuộc tính liên kết, và dựa vào đó làm thành các câu truy vấn. Loại này thay thế cho Inner Join.
SELECT a.property1, a.B.property2
FROM A a
Cơ bản những thứ khác biệt thông thường là thế. Nếu muốn hãy tìm hiểu thêm: https://docs.jboss.org/hibernate/orm/6.6/querylanguage/html_single/Hibernate_Query_Language.html
Tuy còn phần từ session đến Query nhưng phần này không quan trọng đến thế, có thể xem VD.
Cơ bản đã hết. Sau đây là ví dụ code đơn giản cho phần CRUD:
Vì chưa tìm được mô hình nào tốt hơn nên tôi tạm lấy theo mô hình MVC.
Theo đó tôi sẽ tạo một class DAO dành cho Students để demo:

(Tạm thời không viết Interface các kiểu nữa mà đi thẳng vào DAO luôn)
Nhớ là SessionFactory chỉ được tạo một lần trong cả vòng đời chương trình, nên DAO không khởi tạo nó, chỉ nhận nó. Thêm nữa vì đây là demo nên không thử tất cả các ĐK, cái đó tự xây dựng trong từng chương trình.
Create:

Read:

Update: (với details, các thuộc tính hay cách khác tương tự)

Delete:

Tôi quá lười để viết thêm, thực ra cấu trúc nó cần như thế này:

Hết phần I Hibernate. Cảm ơn ai đọc đến tận đây:))))",https://viblo.asia/p/nhat-ky-hoc-hibernate-tu-so-0-7ymJXGQa4kq
39,Chia sẻ một số câu hỏi phỏng vấn về SQL Database đã gặp,"Trong quá trình đi làm phỏng vấn ở một vài nơi, phỏng vấn với khách hàng mình gặp một số câu hỏi về SQL database. Bài viết này sẽ chia sẻ các câu hỏi mình đã gặp về SQL database, hy vọng sẽ được thảo luận cùng anh em về câu trả lời. Hoặc anh em nào có câu hỏi nào hay có thể thảo luận ở phần comment.

I. Ôn phỏng vấn gì về database (SQL Server)?
4 tính chất ACID, chuẩn hóa dữ liệu (Data Normalization)
Cơ sở dữ liệu, cách tạo database, table, column, type, schema, view, temporary table, primary key, foregin key, constrain.
Các câu lệnh cơ bản: select, insert, update, delete.
Làm việc với index, store procedure, function, trigger, Permission, users, roles.
Backup, restore.
Tối ưu performance về query, database
Xử lý lỗi khi viết câu query, trong store procedure, function
Transaction
Bulk insert, bulk delete
Partitioning, Sharding
Các tool sử dụng: sql profiler, SQL Server Integration Services, Azure SQL Analytics, Query Performance Insight, sql execution plan
II. Một số câu hỏi phỏng vấn mình đã gặp
Có rất nhiều câu hỏi nhưng ở đây mình chỉ liệt kê ra các câu mình đã gặp. Nếu anh em có các câu hỏi hay có thể thêm vào phần bình luận.
Các cách tối ưu sql query?
Bạn có thiết kế database cho dự án không? cần lưu ý gì khi thiết kế 1 database mới cho dự án.
Index là gì? có mấy loại index? tại sao phải đánh index? thuật toán sử dụng để đánh index ra sao?
Trigger là gì?
Nếu table có 10 cột đều cần search, trừ cột ID, mình đánh index có tất các cột khác được không? có nên làm vậy không?
Store procedure là gì? lợi ích khi dùng store procedure?
Cách debug khi làm việc với store procedure?
Quản lý exception, lỗi khi làm việc với store procedure, function ra sao?
Cách sử dụng transaction trong sql như thế nào?
Khác nhau giữa Partitioning và Sharding ?
Muốn insert data cỡ 1000 records thì làm cách nào?
Full-Text Search là gì?
Khác nhau giữa Union và Join?
ETL là gì? ELT là gì? khác nhau như thế nào?
Cần làm gì khi migrate DB?
If you find duplicated record in the source, what will you do?
How do you define when or which situation we need to convert from LinQ to raw query/procedure?
III. Các tools mình xài để làm việc với SQL Server
Sql profiler: sử dụng để giám sát, phân tích và chẩn đoán hiệu suất của cơ sở dữ liệu. Ví dụ ta có thể sử dụng sql profiler chọn các cột như Completed, Performance, Duration, CPU ... để đo hiệu suất truy vấn của query.

Sql execution plan: là một bản đồ hoặc hướng dẫn chi tiết mà SQL Server sử dụng để thực thi một câu lệnh SQL. Có các thành phần như Operators, Data Access Methods (Phương pháp Truy cập Dữ liệu), Join Methods, Estimates vs. Actuals, Costs.
Sql execution plan: là một bản đồ hoặc hướng dẫn chi tiết mà SQL Server sử dụng để thực thi một câu lệnh SQL. Có các thành phần như Operators, Data Access Methods (Phương pháp Truy cập Dữ liệu), Join Methods, Estimates vs. Actuals, Costs.
Query performance insight: giúp quản trị viên cơ sở dữ liệu và nhà phát triển theo dõi, phân tích và cải thiện hiệu suất của các truy vấn SQL. Ví dụ ta có thể xem top queries by CPU
Query performance insight: giúp quản trị viên cơ sở dữ liệu và nhà phát triển theo dõi, phân tích và cải thiện hiệu suất của các truy vấn SQL. Ví dụ ta có thể xem top queries by CPU

SQL Server Integration Services: Là một công cụ ETL (Extract, Transform, Load) của Microsoft SQL Server, được thiết kế để hỗ trợ các nhiệm vụ tích hợp dữ liệu, chuyển đổi dữ liệu và di chuyển dữ liệu giữa các nguồn khác nhau
SQL Server Integration Services: Là một công cụ ETL (Extract, Transform, Load) của Microsoft SQL Server, được thiết kế để hỗ trợ các nhiệm vụ tích hợp dữ liệu, chuyển đổi dữ liệu và di chuyển dữ liệu giữa các nguồn khác nhau
Cảm ơn mọi người đã xem bài viết. Chúc anh em một ngày làm việc hiệu quả và đầy năng lượng. Nếu có thắc mắc về các phần trong bài này mọi người có thể inbox qua facebook:https://www.facebook.com/FriendsCode-108096425243996 Mình sẽ giải đáp thắc mắc trong tầm hiểu biết. Cảm ơn mọi người!
Cảm ơn mọi người đã xem bài viết. Chúc anh em một ngày làm việc hiệu quả và đầy năng lượng. Nếu có thắc mắc về các phần trong bài này mọi người có thể inbox qua facebook:https://www.facebook.com/FriendsCode-108096425243996 Mình sẽ giải đáp thắc mắc trong tầm hiểu biết. Cảm ơn mọi người!
Hoặc liên hệ mình qua facebook cá nhân: https://www.facebook.com/Flamesofwars/
Hoặc liên hệ mình qua facebook cá nhân: https://www.facebook.com/Flamesofwars/
P/S: Team mình có nhận làm freelancer nên bạn nào cần thì liên hệ nha. (Tech-stack: .NET, Nodejs, ReactJs, Vue)",https://viblo.asia/p/chia-se-mot-so-cau-hoi-phong-van-ve-sql-database-da-gap-0gdJz6mjJz5
40,Paper reading | Masked Motion Encoding for Self-Supervised Video Representation Learning,"Bài báo đóng góp một phương pháp để có thể học biểu diễn video (Video representation learning) một cách hiệu quả. Do lượng video data được gán nhãn còn hạn chế nên việc có một phương pháp có thể tận dụng nguồn data không gán nhãn này là một điều cần thiết.Những năm gần đây trong các bài toán computer vision, có rất nhiều nghiên cứu ứng dụng self-supervised learning để học biểu diễn trong các dữ liệu không được gán nhãn. Ví dụ đối với ảnh, ta sẽ mask (che) các patch của ảnh theo một tỉ lệ nào đó và thực hiện train để học cách tái tạo (reconstruct) lại phần bị mask này (ví dụ trong model MAE). Dựa vào ý tưởng này, nhiều nhóm nghiên cứu áp dụng cho video, tuy nhiên vẫn tồn tại 2 điểm hạn chế chính:
Trong hình ảnh, ta có thể mask các patch của ảnh và tái tạo lại một cách dễ dàng. Điều này tương tự với video do bản chất là sự kết hợp giữa các frame, ta có thể tái tạo các patch của các frame. Tuy nhiên, việc học cách tái tạo này không giúp ta có được những thông tin về thời gian.
Các nghiên cứu trước đây thường áp dụng cách sample frame là sử dụng stride cố định (sau mỗi stride thì lấy frame 1 lần) sau đó mask một vùng nào đó trên các frame này, điều này làm mất đi những frame chứa thông tin quan trọng. Đặc biệt, với bài toán phân loại hành động thì những chi tiết chuyển động nhỏ ở từng frame là vô cùng cần thiết, là dấu hiệu để phân biệt với các hành động khác.
Từ những hạn chế trên, bài báo đưa ra phương pháp giải quyết như sau:
Đề xuất mô hình Masked Motion Encoding (MME) giải quyết những hạn chế về vấn đề thông tin thời gian bằng cách yêu cầu mô hình reconstruct lại quỹ đạo chuyển động (motion trajectory).
Đề xuất cơ chế nội suy chuyển động nhận đầu vào là video đã masking sau đó dự đoán chuỗi chuyển động ở cả chiều không gian và thời gian. Điều này cho phép mô hình nắm bắt các thông tin chuyển động dài hạn và chi tiết từ đầu vào là video ""thưa"".
Tổng quan mô hình được thể hiện trong hình dưới

Nhắc lại Masked Video Modeling
Cho một video clip được cắt từ một video, mục tiêu của self-supervised video representation learning là training một feature encoder fenc (⋅)f_{\text {enc }}(\cdot)fenc ​(⋅) ánh xạ video clip thành feature tương ứng. Các model video masking trước đây thường theo một ý tưởng chung như sau:
Chia clip đầu vào thành các 3D patch không chồng nhau.
Thực hiện mask ngẫu nhiên các patch sau đó đưa các patch không bị mask vào một feature encoder.
Sử dụng decoder fdec (⋅)f_{\text {dec }}(\cdot)fdec ​(⋅) để reconstruct lại thông tin từ các patch bị mask. Một số model có ý tưởng reconstruct thông tin khác nhau, ví dụ như model VideoMAE thì reconstruct luôn các pixel, MaskFeat thì reconstruct HOG.
Cách trên tuy có thể học tốt các thông tin không gian nhưng lại hạn chế học được các thông tin về thời gian, đây là yếu tố rất quan trọng trong biểu diễn video.
Kiến trúc tổng quát của MME
Để học các thông tin về thời gian tốt hơn trong video, model MME sẽ không học cách reconstruct thông tin không gian như các model trước đó mà học reconstruct thông tin chuyển động của object bao gồm vị trí và sự thay đổi hình dạng của object.

Như hình trên, các bước thực hiện của model như sau:
Chia video thành các 3D patch không chồng nhau có kích thước t×h×wt \times h \times wt×h×w tương ứng với chiều time, height, width.
Sử dụng chiến lược tube masking (giống VideoMAE, bạn có thể đọc thêm về model này trên Viblo 😄) để mask cho một số patch.
Các patch không bị mask sẽ được đưa vào encoder.
Biểu diễn đầu ra sau khi đi qua encoder cùng với các learnable [MASK] token được đưa vào encoder để reconstruct motion trajectory z\mathbf{z}z trong các patch bị mask.
[MASK]
Training loss cho MME như sau:
trong đó z^\hat{\mathbf{z}}z^ là giá trị dự đoán motion trajectory và tập các index I\mathcal{I}I của các motion trajectories trong tất cả các patch bị mask.
Motion Trajectory trong MME
Được truyền cảm hứng bởi thực tế là con người chúng ta nhận ra các hành động bằng cách cảm nhận sự thay đổi vị trí và thay đổi hình dạng của các vật thể đang chuyển động, nhóm tác giả tận dụng hai loại thông tin này để biểu diễn thông tin motion trajectory.
Sự chuyển động của các object có thể biểu diễn bằng nhiều cách khác nhau như:
Optical flow (Luồng quang): Optical flow là một khái niệm trong lĩnh vực thị giác máy tính và xử lý hình ảnh, nó liên quan đến sự di chuyển của các đối tượng trong một cảnh quan. Khi một đối tượng di chuyển trong một khung hình, các pixel tương ứng trên các khung hình liên tiếp sẽ thay đổi vị trí của chúng. Optical flow sử dụng các phương pháp tính toán để ước lượng vector chuyển động của mỗi pixel từ khung hình hiện tại sang khung hình tiếp theo. Optical flow là một công cụ quan trọng để phân tích và hiểu sự chuyển động của các đối tượng trong các chuỗi video. Bằng cách tính toán optical flow, ta có thể xác định hướng và tốc độ di chuyển của các đối tượng trong cảnh quan.
Histograms of optical flow (HOF) là một phương pháp trong xử lý hình ảnh và phân loại đối tượng dựa trên optical flow. HOF sử dụng thông tin về hướng và tốc độ di chuyển của các pixel trong một chuỗi video để tạo ra các biểu đồ histogram mô tả các mẫu chuyển động trong cảnh quan. HOF thường được sử dụng như các đặc trưng (features) trong các hệ thống nhận dạng hoặc phân loại đối tượng. Các biểu đồ histogram của optical flow cung cấp thông tin về mẫu chuyển động trong video và có thể được sử dụng để phân loại các hành động, nhận dạng các hoạt động như đi bộ, đạp xe, chạy,...
Motion boundary histograms (MBH) là một phần mở rộng của Histograms of Optical Flow (HOF) và tập trung vào việc mô hình hóa biên giới chuyển động trong cảnh quan. MBH sử dụng thông tin về hướng và tốc độ di chuyển của các pixel trong video để tạo ra các biểu đồ histogram mô tả phân bố các biên giới chuyển động. Thay vì chỉ xem xét hướng di chuyển, MBH cũng xem xét biên giới chuyển động, tức là các vùng trên các đối tượng trong video mà có sự thay đổi mạnh về di chuyển. MBH cung cấp thông tin chi tiết về các biên giới chuyển động trong video, giúp nhận biết các cạnh và biên giới quan trọng trong quá trình di chuyển. Điều này có thể được sử dụng trong các ứng dụng như nhận dạng hành động, phân loại đối tượng di động, nhận diện vật thể và xử lý video.
Tuy nhiên các cách trên chỉ biểu diễn các chuyển động ngắn hạn giữa các frame kề nhau. Thứ ta mong muốn bây giờ là motion trajectory có thể biểu diễn được các chuyển động dài hạn hơn. Đầu tiên, ta sẽ track chuyển động của object trong LLL frame để thu được chuyển động dài hạn, kết quả là một quỹ đạo T\mathbf{T}T như sau:

trong đó pt=(xt,yt)\mathbf{p}_t = (x_t, y_t)pt​=(xt​,yt​) biểu diễn một điểm tại vị trí (xt,yt)(x_t, y_t)(xt​,yt​) của frame ttt và (⋅,⋅)(\cdot, \cdot)(⋅,⋅) là thao tác concat. Xét trong quỹ đạo này, ta sẽ tìm các position feature zp\mathbf{z}^pzp và shape feature zs\mathbf{z}^szs của object để tạo thành một quỹ đạo chuyển động z\mathbf{z}z:

Các position feature được biểu diễn bởi sự dịch chuyển vị trí tương đối với time step trước đó, trong khi đó các shape feature được coi là các HOG descriptor của object được track trong các những time step khác nhau.
Một điểm mới trong bài báo đó là track object bằng cách sử dụng một quỹ đạo dày đặc (dense) theo cả không gian và thời gian. Cụ thể, nhóm tác giả thực hiện chọn KKK điểm trong một patch (có kích thước t×h×wt \times h \times wt×h×w) bị mask, trong đó mỗi điểm là một phần của một object. Với mỗi điểm, ta thực hiện track qua LLL frame liên tiếp, kết quả là ta thu được KKK quỹ đạo. Bạn có thể xem hình dưới để hiểu rõ hơn.

Bằng cách này, model có thể nắm bắt được thông tin chuyển động về mặt không gian và thời gian tốt hơn (vẫn theo cơ chế là mask và predict như các model masking).
Các nghiên cứu trước đây thường sample video ""thưa"" làm đầu vào, tức là có stride giữa các frame. Tuy model được đề xuất cũng nhận video ""thưa"" làm đầu vào nhưng điểm khác là model thực hiện nội suy quỹ đạo chuyển động. Việc thực hiện nội suy này không làm tăng chi phí tính toán của encoder mà còn giúp cho model học được nhiều thông tin chuyển động chi tiết cho dù đầu vào là video thưa.
Cho một quỹ đạo T\mathbf{T}T chứa vị trí của object được track tại mỗi frame, ta sẽ quan tâm đến chuyển động của các object thay vì vị trí chính xác của chúng. Do đó, ta sẽ biểu diễn các position feature là chuyển động giữa 2 điểm kề nhau Δpt=pt+1−pt\Delta \mathbf{p}_t=\mathbf{p}_{t+1}-\mathbf{p}_tΔpt​=pt+1​−pt​ như sau:

trong đó zp\mathbf{z}^pzp là một feature L×2L \times 2L×2 chiều. Vì mỗi patch có KKK position feature nên ta sẽ concat và chuẩn hóa  chúng và chuyển thành các position feature.
Bên cạnh việc thực hiện embed chuyển động của object, model cũng cần học được sự thay đổi hình dạng của object để nhận biết được các hành động. Trong model, nhóm tác giả sử dụng HOG (histograms of oriented gradients) với 9 bin để mô tả hình dạng của object. Do hình dạng của object thường thay đổi qua các frame nên ta cần tính toán trajectory-aligned HOG để track được những sự thay đổi hình dạng của object, cụ thể:

trong đó HOG(⋅)HOG(\cdot)HOG(⋅) là HOG descriptor và zs\mathbf{z}^szs là feature L×9L \times 9L×9 chiều. Vì mỗi patch gồm KKK quỹ đạo, ta cần concat KKK trajectory-aligned HOG feature và chuẩn hóa chúng về phân phối chuẩn.
Bảng dưới là so sánh của các model SOTA trên tập dữ liệu Something-Something V2. Model MME có hiệu suất tốt hơn so với các model trước đó.

Tương tự, trên tập dữ liệu Kinetics-400. MME cũng cho kết quả ấn tượng.

Với 2 bộ dữ liệu UCF101 và HMDB51, MME cũng cho kết quả tốt hơn với các model trước đó.

Bài báo cũng trình bày ảnh hưởng của các loại reconstruct khác nhau lên hiệu suất của mô hình. Đầu tiên, để kiểm tra xem việc tái tạo hình ảnh có được thực hiện độc lập trong từng frame hay không. Nhóm tác giả thực hiện trộn ngẫu nhiên các frame trong video để hạn chế thông tin về thời gian, điều này có nghĩa là model sẽ khó tận dụng được thông tin từ những frame khác để reconstruct frame hiện tại thông qua sự tương quan về mặt thời gian. Nhóm tác giả cũng thực hiện mask 90% các frame trong video như model VideoMAE.

Từ hình trên ta nhận thấy rằng reconstruction loss (là L2 error giữa pixel dự đoán và pixel ground truth) hội tụ về giá trị thấp. Nhóm tác giả cũng tiến hành thực nghiệm sử dụng video thô ban đầu mà không trộn (shuffling) các frame. Model trải qua một quá trình hội tụ tương tự. Điều này chứng tỏ rằng các pixel bị mask được reconstruct tốt mà không cần thông tin thời gian.
Để tìm hiểu xem model được pretrain trên task reconstruct hình ảnh có thể nắm bắt được những thông tin thời gian quan trọng hay không? Nhóm tác giả thực hiện transfer 2 model VideoMAE (1 model được pretrain trên shuffled video và 1 model được pretrained trên video thô) thành downstream task là actio recognition. Như hình trên ta có thể nhận thấy rằng, 2 model có hiệu suất na ná nhau 😄 Điều này chứng tỏ rằng việc xóa thông tin về thời gian không làm ảnh hưởng nhiều đến việc học biểu diễn video trong model VideoMAE. Điều này là hợp lý và dễ hiểu vì model VideoMAE ít tập trung và thông tin thời gian hơn là thông tin về không gian. Ngược lại, model MME lại tốt hơn nhiều VideoMAE khoản này 😄 khi mà thông tin thời gian được cung cấp cho việc pretraining (64.1% và 61.1%). Khi mà thực hiện shuffle video ta thấy hiệu suất của 2 model không khác nhau nhiều (60.7% và 60.9%). Điều này chứng tỏ model MME có khả năng nắm bắt và tận dụng được thông tin thời gian trong quá trình pretraining.

Trong 2 bảng trên, nhóm tác giả thực hiện so sánh ảnh hưởng của các phương pháp reconstruct khác nhau lên hiệu suất model.

6 bảng trên là ảnh hưởng của các yếu tố khác lên hiệu suất model.
[1] Masked Motion Encoding for Self-Supervised Video Representation Learning
[2] https://github.com/XinyuSun/MME
[3] https://viblo.asia/p/tim-hieu-ve-phuong-phap-mo-ta-dac-trung-hog-histogram-of-oriented-gradients-V3m5WAwxZO7",https://viblo.asia/p/paper-reading-masked-motion-encoding-for-self-supervised-video-representation-learning-5OXLAg7w4Gr
41,Các Vấn Đề Thực Tế Khi Hệ Thống Bị Quá Tải Mà Ít Ai Chia Sẻ,"Source: https://www.tuanh.net/blog/devops/the-behavior-of-a-service-under-heavy-load
Một sự tăng đột biến lưu lượng đột ngột có thể gây ra nhiều vấn đề cho một dịch vụ, từ suy giảm hiệu suất và trầm trọng nhất là gây quá tải hệ thống. Bằng cách hiểu rõ các kịch bản này, bạn có thể chuẩn bị tốt hơn và triển khai các giải pháp để duy trì một dịch vụ ổn định. Bài viết này sẽ khám phá các nguyên nhân, hậu quả tiềm ẩn của lưu lượng truy cập tăng đột biến và một số gợi ý để xử lý chúng.
Mary là một nhân viên trẻ, nhiệt huyết, luôn đặt toàn bộ tâm huyết vào công việc. Cô sẵn sàng làm việc chăm chỉ từ 8 giờ sáng đến 8 giờ tối, không ngừng nỗ lực để hoàn thành nhiệm vụ và vượt quá kỳ vọng của sếp. Mặc dù đôi lúc cảm thấy mệt mỏi, nhưng hình ảnh về một tương lai giàu có và hạnh phúc luôn thúc đẩy cô tiếp tục tiến về phía trước. (Có lẽ cô chưa đọc cuốn ""Cha giàu cha nghèo"" nên chưa thực sự hiểu tầm quan trọng của cân bằng giữa công việc và cuộc sống.)
Tuy nhiên, sếp của Mary là một người hà khắc và mang đậm dấu ấn của một tư bản sư, ông ta nhìn thấy cô ấy như một người quá dễ bảo và háo hức muốn làm hài lòng. Ông ta thường xuyên giao thêm dự án cho cô ấy, mặc dù khối lượng công việc của cô ấy đã gấp ba lần so với một nhân viên bình thường
Cuối cùng, sau vô số ngày làm việc không ngừng nghỉ, Mary kiệt sức và ngã bệnh nặng gần một tháng. Thay vì nhận được sự hỗ trợ, công ty quyết định chấm dứt hợp đồng, khiến cô không chỉ kiệt quệ về thể chất mà còn vô cùng thất vọng và tổn thương.
Chắc các bạn háo hức lắm phải không? Để tôi giải thích:
Câu chuyện của Mary tương đồng với những thách thức mà một dịch vụ phải đối mặt khi bị quá tải. Giống như sự nhiệt tình ban đầu của Mary dần bị thử thách bởi khối lượng công việc ngày càng tăng, một dịch vụ vốn mạnh mẽ cũng bắt đầu căng thẳng dưới áp lực của lưu lượng truy cập quá mức. Cả Mary và dịch vụ, trong khả năng tương ứng của họ, đều được yêu cầu hoạt động vượt quá giới hạn thiết kế.
Sếp của Mary, người liên tục giao thêm việc cho cô ấy, giống như một vòi nước chảy không ngừng. Cũng như một dịch vụ không thể xử lý được nhu cầu tăng lên, càng nhiều nhiệm vụ được giao thêm, Mary càng bị quá tải và cuối cùng, hiệu suất công việc của cô ấy bị ảnh hưởng.
Khi Mary phải đối mặt với khối lượng công việc quá tải, cô quyết định đa nhiệm để đáp ứng các deadline của dự án. Điều này yêu cầu Mary phải tập trung cao độ. Nếu chỉ làm một dự án, cô có thể hoàn thành trong 4 giờ. Tuy nhiên, khi phải cân bằng hai dự án cùng lúc, thời gian hoàn thành kéo dài từ 6 đến 7 giờ.
Tình huống này tương tự như một dịch vụ phải xử lý quá nhiều yêu cầu đồng thời. Khi có quá nhiều yêu cầu, các tài nguyên hệ thống như CPU và bộ nhớ phải được chia sẻ giữa chúng, dẫn đến mỗi yêu cầu nhận được ít tài nguyên hơn. Kết quả là thời gian xử lý cho mỗi yêu cầu tăng lên, và điều này có thể dẫn đến các yêu cầu mới không nhận đủ tài nguyên hoặc phải chờ đợi lâu hơn.
Sự phân bổ tài nguyên không hiệu quả này tạo ra một vòng luẩn quẩn, trong đó các yêu cầu tiếp theo ngày càng khó đáp ứng và hệ thống trở nên kém hiệu quả dần dần.
Nói cách khác, càng nhiều dự án mà Mary phải xử lý cùng một lúc, cô càng lầm vào tình cảnh không thể hoàn thành tất cả công việc trước deadline.
Điều này giống như một request bị connection timeout: khi có quá nhiều dữ liệu được xử lý cùng lúc dẫn đến một số yêu cầu bị bỏ lỡ. Với quá nhiều dự án trên tay, Mary có nguy cơ không thể dành đủ sự chú ý cho từng dự án, dẫn đến chậm trễ.
Tương tự như tình trạng kiệt sức dẫn đến việc mất việc của Mary, các dịch vụ có thể gặp phải những gián đoạn nghiêm trọng, ngừng hoạt động hay bị khởi động lại khiến chúng không thể hoạt động trơn tru lúc website đang có lưu lượng truy cập tăng cao.
Giả sử bạn có một máy ảo mô phỏng cấu hình não bộ của Mary với 1 CPU (có 2 luồng ảo (vitual thread) trên mỗi CPU) và 8 GB RAM. Dưới đây là cách CPU và các worker-thread khi Mary phải xử lý nhiều tác vụ:

Biểu đồ này là ví dụ minh họa.
Trường hợp 1: 2 tác vụ
Cấu hình CPU: 1 CPU với 2 luồng ảo.
Số lượng tác vụ: 2.
Khi Mary nhận được 2 tác vụ và có 2 luồng ảo để xử lý, mỗi luồng ảo sẽ xử lý 1 tác vụ. Do đó, mỗi tác vụ sử dụng 1/2 CPU của Mary. Trong trường hợp này, Mary có thể xử lý hai tác vụ đồng thời mà không gặp vấn đề về hiệu suất, vì CPU có đủ luồng ảo để xử lý chúng cùng một lúc.
Trường hợp 2: 4 tác vụ
Cấu hình CPU: 1 CPU với 2 luồng ảo.
Số lượng tác vụ: 4.
Khi Mary có 4 tác vụ nhưng chỉ có 2 luồng ảo để xử lý, các tác vụ sẽ phải được xử lý tuần tự. Vì vậy, mỗi luồng ảo sẽ lần lượt thực hiện 2 tác vụ. Do đó, mỗi tác vụ chỉ sử dụng 1/4 CPU của Mary. Trong trường hợp này, do các tác vụ được thực hiện tuần tự trên các luồng ảo nên thời gian xử lý cho mỗi tác vụ sẽ dài hơn khi chỉ có 2 tác vụ.
Trường hợp 3: 6 tác vụ
Cấu hình CPU: 1 CPU với 2 luồng ảo.
Số lượng tác vụ: 6.
Khi có 6 tác vụ nhưng vẫn chỉ có 2 luồng ảo, các tác vụ sẽ tiếp tục được thực hiện tuần tự thông qua 2 luồng ảo. Mỗi tác vụ chỉ sử dụng 1/6 CPU của Mary. Chia sẻ tài nguyên CPU cho nhiều tác vụ hơn sẽ dẫn đến thời gian xử lý dài hơn và có thể giảm hiệu suất tổng thể.
Nếu bạn muốn biết thêm về Đồng bộ và Đồng thời, bạn có thể kiểm tra nó ở đây. Trong bài viết này, tôi đã nói về nó là gì, nó hoạt động như thế nào và đưa ra một số ví dụ.
Tôi đã thử nghiệm tải trọng cho GCS. Đúng vậy, đó chính là GCS (Google Cloud Storage). Tôi đã thực hiện điều này bằng cách tạo một module và sử dụng module đó để gọi một yêu cầu nhằm tải lên một tệp vào GCS.
Nếu bạn muốn xem cách tôi tải bài kiểm tra bằng Locust, bạn có thể xem tại đây.
Đây là cấu hình dịch vụ của tôi tại thời điểm đó. Chúng chỉ được thực hiện vào những thời điểm khác nhau.
Pod: 
    Request:
        Memory: 10G
        CPU: 100m
    Limit:
        Memory: 10G
        CPU: 15
Tổng cộng có 10 pod và kích thước tệp là 3KB.
4.1 Lần đầu

Biểu đồ này mô tả các yêu cầu thất bại trong quá trình loadtest. Tôi cũng sẽ thống kê các luồng worker được sinh ra bởi mỗi pod. Điều quan trọng cần lưu ý là có một mối quan hệ một-một giữa các yêu cầu và worker-thread: mỗi yêu cầu đến một pod kích hoạt việc tạo ra một worker-thread duy nhất.

Bằng cách tìm kiếm cụm từ ""default task-"" trong văn bản, tôi có thể xác định được số worker-thread đang được tạo ra là bao nhiêu.

Trong giai đoạn này, sau khi tiến hành vertical scaling, các pod có xấp xỉ 21 CPU.
4.2 Lần thứ 2
Chúng ta làm tương tự lần 1




4.3 Lần thứ 3




4.4 Tổng kết

Qua 3 lần đo trên, số lượng worker-thread không nhất thiết phải cao để đạt được hiệu suất tốt. Trường hợp 1, với ít worker-thread nhất nhưng hiệu suất tốt nhất, chứng minh tầm quan trọng của việc tối ưu hóa số lượng worker-threadvà phân bổ tài nguyên CPU.
Tỷ lệ CPU/worker-thread cao hơn có thể tương quan với hiệu suất tốt hơn, miễn là số lượng worker-thread không quá thấp đến mức không thể xử lý đủ yêu cầu.
Đôi khi, không thể kiểm soát số lượng worker-thread có thể là dấu chấm hết cho hệ thống của bạn.
Thoughput của 3 lần đo mà khác nhau, mặc dù chúng đều được đo trên cùng một k8s configuration, nguyên nhân gốc rễ của vấn đề này nằm ở khả năng mở rộng của CPU không nhanh chóng, dẫn đến sự tích tụ các request do tắc nghẽn xử lý. Trong trường hợp 2, mặc dù có hơn 300 worker-thread, chúng tôi chỉ có thể xử lý 94k yêu cầu và gặp 258 lỗi. Sử dụng CPU cho mỗi yêu cầu khá thấp, khoảng 0,61.
Số lượng worker-thread nhiều đến vậy là do, khi request vào pod, nó được tạo 1 worker-thread, nhưng do thời gian xử lý lâu, nên các request khác vào thì nó vẫn tiếp tục tạo ra worker-thread, trong khi những worker-thread hiện thời thì vẫn chưa được giải phóng tài nguyên.
Tôi đề xuất các giải pháp sau:
Thứ nhất, áp dụng fault tolerance để giải quyết vấn đề nhiều yêu cầu gây tắc nghẽn. Bạn có thể tham khảo một trong những bài viết của tôi về Bulkhead
Thứ hai, áp dụng tính năng tự động mở rộng thông minh. Đôi khi, việc mở rộng pod dựa trên RAM và CPU là không đủ. Chúng ta có thể sử dụng Chỉ số Tùy chỉnh (Custom Metrics) để giải quyết vấn đề này. Để biết thêm thông tin, vui lòng tham khảo: https://cloud.google.com/monitoring/custom-metrics
Thứ ba, tối ưu hóa mã nguồn để giảm thời gian thực thi.
Tối ưu hóa hiệu suất hệ thống là một quá trình toàn diện, từ điều chỉnh các worker-thread đến quản lý tài nguyên CPU và bộ nhớ. Bằng cách triển khai các giải pháp cụ thể, bạn có thể cải thiện hiệu quả khả năng xử lý và giảm thiểu lỗi.
Cảm ơn bạn đã đọc bài viết này, chúng tôi rất mong được gặp lại bạn trong các bài viết tiếp theo!",https://viblo.asia/p/cac-van-de-thuc-te-khi-he-thong-bi-qua-tai-ma-it-ai-chia-se-Ny0VGaw5JPA
42,Kotlin Flow cheat sheet phần 1: Channel,"Sau khi làm việc với Kotlin Flows một thời gian, có thể bạn đã quen với các khái niệm cơ bản. Nhưng nếu chưa bao giờ sử dụng Channel, bạn sẽ không biết sự khác nhau giữa merge, combine và zip, hoặc có thể bạn chưa hiểu rõ SharedFlow và StateFlow cũng như cách sử dụng chúng.
Channel
merge
combine
zip
SharedFlow
StateFlow
Cheat sheet này hệ thống lại những kiến thức quan trọng mà mình đã góp nhặt được trong quá trình làm việc với Kotlin Flow. Nó được thiết kế để trở thành một tài liệu tham khảo hữu ích, giúp bạn giải quyết các tình huống phức tạp.
Trong phần 1, chúng ta sẽ tìm hiểu chi tiết về Channel, cách thức hoạt động, và cách ứng dụng trong các trường hợp thực tế để giao tiếp giữa các coroutine một cách an toàn và hiệu quả.
Bạn có thể đọc toàn bộ serie tại đây:
Kotlin Coroutines cheat sheet nâng cao dành cho Android Engineer
Kotlin Flow cheat sheet phần 1: Channel
Kotlin Flow cheat sheet phần 2: Flow
Kotlin Flow cheat sheet phần 3: SharedFlow và StateFlow
Hot streams
Ví dụ: channel, Collections (List, Set… ).
channel
Collections
List
Set
Bắt đầu ngay lập tức: bắt đầu emit các giá trị bất kể có subscriber hay không.
Lưu các phần tử: chúng không cần phải tính toán lại và tất cả subscriber đều nhận được cùng một chuỗi giá trị.
Cold streams
Ví dụ: Sequence, Flow
Sequence
Flow
Bắt đầu theo yêu cầu: cold streams chỉ bắt đầu emit các giá trị khi subscriber chủ động đăng ký stream đó. Nguồn dữ liệu là lazy.
Phát độc lập: mỗi subscriber nhận được chuỗi giá trị độc lập của riêng mình. Không có phần tử nào được lưu trữ.
Nguyên tắc chính
Là một hot stream.
Đảm bảo không có xung đột (không có vấn đề với trạng thái chia sẻ) và công bằng nên rất hữu ích khi các coroutine khác nhau cần liên lạc với nhau.
Hỗ trợ bất kỳ số lượng sender và receiver.
Mỗi giá trị gửi tới channel chỉ được nhận một lần.
Nếu có nhiều receiver subscribe cùng lúc, các phần tử sẽ được phân bổ công bằng giữa các receiver. (Hàng đợi FIFO của receiver).
Channel có 3 receiver, subscribe theo thứ tự:
Receiver1, Receiver2, Receiver3.
Tất cả receiver đã subscribe channel.
Channel emit ra 4 giá trị: ""A"", ""B"", ""C"" rồi ""D"".
Receiver1 nhận được ""A"" và ""D""
Receiver2 nhận được ""B""
Receiver3 nhận được ""C""
Chúng có 2 suspend function là send và receive.
send
receive
receive bị suspend nếu không có phần tử nào trong channel và sẽ đợi một phần tử sẵn sàng để tiếp tục.
receive
send bị suspend nếu channel đạt đến capacity.
send
Chúng ta cũng có thể sử dụng phiên bản không bị suspend là trySend và tryReceive, chúng trả về một ChannelResult (cho chúng ta biết thao tác có thành công hay không).
trySend
tryReceive
ChannelResult
Chúng cần được close thủ công sau khi chúng ta gửi xong dữ liệu hoặc khi xảy ra Exception: myChannel.close(). Nếu không, receive sẽ đợi các phần tử mãi mãi.
myChannel.close()
Các loại channel capacity
val myChannel = Channel<Int>(capacity = 3)

// hoặc

val myChannel = produce(capacity = 3) {
    // emit các giá trị ở đây
}
Channel.UNLIMITED: buffer không giới hạn và send không bao giờ bị suspend.
Channel.UNLIMITED
send
Channel.BUFFERED: buffer capacity là 64. Giá trị mặc định này có thể được override bằng thuộc tính hệ thống kotlinx.coroutines.channels.defaultBuffer trong JVM.
Channel.BUFFERED
kotlinx.coroutines.channels.defaultBuffer
Channel.RENDEZVOUS: (behavior mặc định) buffer capacity là 0. Receiver sẽ chỉ nhận được dữ liệu nếu nó đã subscribe với sender khi dữ liệu được emit.
Channel.RENDEZVOUS
Channel.CONFLATED: buffer capacity là 1. Mỗi phần tử mới sẽ thay thế phần tử trước đó.
Channel.CONFLATED
Giá trị intbất kỳ: buffer sẽ có capacity bằng giá trị được set.
int
Xử lý lỗi tràn buffer
Các channel có một tham số onBufferOverflow kiểm soát những gì xảy ra khi buffer đầy. Có 3 lựa chọn:
onBufferOverflow
BufferOverflow.SUSPEND: (behavior mặc định) tạm dừng phương thức send khi buffer đầy.
BufferOverflow.SUSPEND
send
BufferOverflow.DROP_OLDEST: loại bỏ phần tử cũ nhất khi buffer đầy.
BufferOverflow.DROP_OLDEST
BufferOverflow.DROP_LATEST: loại bỏ phần tử mới nhất khi buffer đầy.
BufferOverflow.DROP_LATEST
Tạo Channel tự động close
Coroutine builder produce sẽ close channel bất cứ khi nào builder coroutine kết thúc (finish, stop hoặc cancel).
produce
suspend fun myFunction() = coroutineScope {
    val channel = produce {
        // emit các giá trị ở đây và không cần gọi close() khi kết thúc
    }
}
Tự động dọn dẹp nếu một phần tử không thể xử lý
Nếu channel đã bị close, cancel hoặc khi send, receive, hastNext có lỗi
send
receive
hastNext
val myChannel = Channel(
    capacity,
    onUnderliveredElement = { /* các tác vụ dọn dẹp ở đây */ }
)
Use case: trigger một refresh
Trong Android, trường hợp sử dụng phổ biến cho các channel là trigger khi một screen được refresh (pull to refresh hoặc button retry). Đoạn code bên dưới trình bày cách fetch data từ API khi chúng ta subscribe flow lần đầu tiên hoặc khi trigger một refresh.
Rất nhiều người sử dụng SharedFlow để trigger refresh và nó hoạt động ổn, nhưng đó không phải là giải pháp tốt nhất vì SharedFlow được thiết kế để có nhiều receiver.
SharedFlow
SharedFlow
// Đây là phiên bản đơn giản hóa để minh họa cách chúng ta có thể sử dụng channel. 
// Trong trường hợp sử dụng thực tế, chúng ta sẽ yêu cầu một số logic bổ sung để tránh 
// làm mới nếu dữ liệu đã được tải chẳng hạn. 

interface ApiService {
    suspend fun fetchData(): List<String>
}

class FetchDataUseCase @Inject constructor (
    private val apiService: ApiService
) {
    // tạo một channel có buffer là 1 và sẽ loại bỏ dữ liệu mới nhất
    // vì vậy nếu chúng ta trigger refresh nhiều lần liên tiếp
    // chúng ta sẽ chỉ giữ phần tử đầu tiên.
    private val refreshChannel = Channel<Unit>(
        capacity = 1,
        onBufferOverflow = BufferOverflow.DROP_LATEST
    )

    // viewModel có thể receive flow này để build UI state
    val dataState: Flow<FetchDataState> =
        refreshChannel
            // convert channel thành flow
            .consumeAsFlow()
            // emit một phần tử khi bắt đầu fetch data ngay khi chúng ta subscribe flow
            .onStart { emit(Unit) }
            .map { fetchData() }

    fun refresh() {
        // Chúng ta sử dụng trySend ở đây để không phải tạo suspend function
        // và vì vậy chúng ta không cần scope để gọi nó.
        // Phương thức này có thể được gọi từ viewModel để trigger refresh
        refreshChannel.trySend(Unit)
    }
    private suspend fun fetchData(): FetchDataState =
        try {
            val data = apiService.fetchData()
            FetchDataState.Success(data)
        } catch (e: Exception) {
            FetchDataState.Error(e.message ?: ""An error occurred"")
        }

    sealed interface FetchDataState {
        data object Loading : FetchDataState
        data class Success(val data: List<String>) : FetchDataState
        data class Error(val message: String) : FetchDataState
    }
}
Cảm ơn các bạn đã đọc đến đây, cùng chờ đón những phần tiếp theo nhé.
https://medium.com/@galou.minisini/advanced-kotlin-flow-cheat-sheet-for-android-engineer-cb8157d4f848
🔔 Blog: henrytechie.xyz
☕️ Facebook: Henry Techie
☁️ TikTok: @henrytechie",https://viblo.asia/p/kotlin-flow-cheat-sheet-phan-1-channel-2oKLnno1LQO
43,Sự khác biệt giữa Kubernetes Controller và Kubernetes Operator là gì?,"Kubernetes, hay K8s, là một trong những hệ thống điều phối container nguồn mở mạnh mẽ nhất — đặc biệt là khả năng tự động triển khai trạng thái mong muốn. Nói cách khác, với tư cách là quản trị viên, bạn có thể chỉ định cách bạn muốn ứng dụng và cụm của mình hoạt động, sau đó hệ thống điều khiển sẽ thực hiện điều đó cho bạn bằng cách sử dụng các feedback loop. Thật tuyệt vời phải không nào?
Hai phương pháp chính cho phép quản lý tác vụ hoạt động phức tạp trong Kubernetes đó chính là Controller và Operator. Cả hai đều đảm bảo rằng tài nguyên cụm tuân thủ trạng thái mong muốn, nhưng chúng đóng vai trò khác nhau trong hệ sinh thái Kubernetes. Nếu sự phân biệt này có vẻ khó hiểu, thì đó là vì sự khác biệt giữa Controller và Operator có thể khá phức tạp! Đặc biệt là vì Operator là một subcategory của Controller.
Đọc tiếp để xem xét kỹ hơn cách hoạt động của Kubernetes Controller và Operator. Tôi sẽ định nghĩa các tính năng chính và liệt kê các trường hợp sử dụng của chúng trong bài viết ngay sau đây, cùng chú ý theo dõi nhé!!!
Kubernetes Controller là gì?
Kubernetes Controller là vòng điều khiển theo dõi các cụm tài nguyên của bạn và thay đổi chúng để phù hợp với trạng thái mong muốn trong một chu kỳ liên tục. Ví dụ, nếu bạn cần nướng gà ở nhiệt độ 200 độ C, vòng điều khiển trong trường hợp này sẽ theo dõi nhiệt độ lò nướng để giữ nhiệt độ gần với nhiệt độ mong muốn của bạn nhất có thể. Nếu nhiệt độ xuống dưới hoặc trên 200, vòng điều khiển sẽ tự động điều chỉnh các nút điều khiển lò nướng để sửa lỗi nhằm đưa nhiệt độ về đúng chính xác như mong muốn.
Đây chính xác là cách mà Kubernetes Controller hoạt động — chúng giám sát các tài nguyên và điều kiện mà quản trị viên yêu cầu ở trạng thái mong muốn và tự động triển khai chúng. Controller đảm bảo rằng trạng thái mong muốn và trạng thái thực tế luôn được căn chỉnh liên tục.
1. Các hàm sử dụng Kubernetes Controller
Kubernetes Controller thực hiện nhiều chức năng khác nhau để quan sát, duy trì và thực thi trạng thái mong muốn của tài nguyên trong cụm Kubernetes. Các chức năng chính của Controller bao gồm:
Quan sát cụm — Controller liên tục theo dõi các thay đổi trạng thái trong cụm tài nguyên, cho phép quản trị viên hoặc nhóm cơ sở hạ tầng phản hồi kịp thời.
Thực thi trạng thái mong muốn — Controller so sánh các điều kiện của trạng thái mong muốn của quản trị viên với trạng thái hiện tại của cụm tài nguyên và tự động thực hiện các thay đổi để thực thi trạng thái mong muốn.
Thực thi giới hạn tài nguyên — Controller thực thi giới hạn đối với tài nguyên được sử dụng trong không gian tên để ngăn ngừa việc sử dụng quá mức.
Lịch trình được xác định — Controller chạy Công việc theo lịch trình do người dùng xác định.
Xử lý sự kiện — Controller xử lý các sự kiện vòng đời và kích hoạt phản hồi, chẳng hạn như tự động khởi động lại pod khi gặp lỗi.
Bảo trì tình trạng cụm — Controller phát hiện và quản lý các nút không hoạt động tốt, trong khi Controller điểm cuối duy trì các đối tượng điểm cuối được cập nhật để khám phá dịch vụ.
Triển khai và khôi phục — Controller triển khai cung cấp khả năng quản lý triển khai và khôi phục về phiên bản trước đó.
Kubernetes Controller tự động hóa các hoạt động cụm thường lệ, thực thi chính sách, phản ứng với các lỗi và nỗ lực liên tục điều chỉnh trạng thái thực tế để phù hợp với cấu hình mong muốn của người dùng.
2. Các trường hợp sử dụng Kubernetes Controller
Kubernetes Controller có một số trường hợp sử dụng chính để quản lý và vận hành các ứng dụng và tài nguyên trong một cụm:
Mở rộng ứng dụng web front-end — Đảm bảo bạn luôn có đủ bản sao pod đang chạy để có thể xử lý lưu lượng người dùng tăng lên đến ứng dụng web. Các bộ điều khiển như ReplicaSet sẽ tự động tạo các pod mới khi cần.
Chạy cơ sở dữ liệu được quản lý — Triển khai và quản lý cơ sở dữ liệu có trạng thái và duy trì ID mạng ổn định khi các pod được khởi động lại hoặc di chuyển bằng Controller như StatefulSet.
Xử lý dữ liệu hàng loạt — Chạy các pod đến khi hoàn tất cho các tác vụ hàng loạt và đảm bảo rằng các pod bị lỗi được chạy lại. Trường hợp sử dụng này hoạt động đặc biệt tốt trong thiết lập đường ống phân tích hoặc đào tạo máy học.
Duy trì một tập hợp các pod — Đảm bảo rằng các tập hợp gồm nhiều pod giống hệt nhau đang chạy tại bất kỳ thời điểm nào.
Lên lịch tác vụ — Chạy các nhóm theo lịch trình do người dùng xác định với các tác vụ cụ thể bằng bộ điều khiển như CronJob.
Triển khai tự động mở rộng — Mở rộng triển khai và bộ bản sao dựa trên các số liệu khác nhau và tự động điều chỉnh bản sao pod bằng bộ điều khiển HorizontalPodAutoscaler.
Giới hạn mức sử dụng tài nguyên — Giới hạn mức sử dụng tài nguyên và ngăn ngừa quá tải bằng Controller như ResourceQuota.
Về cơ bản, Kubernetes Controller tự động hóa các hoạt động để triển khai, mở rộng quy mô, phục hồi và thực thi các chính sách cho các ứng dụng trong khi tối đa hóa việc sử dụng cụm. Điều này làm giảm nỗ lực thủ công cho các nhóm nền tảng và bổ sung khả năng tự quản lý cho Kubernetes.
3. Lợi ích của việc sử dụng Kubernetes Controller
Tôi đã đề cập đến các chức năng và trường hợp sử dụng ở trên, nhưng Kubernetes Controller còn có thể cung cấp những lợi ích nào khác nữa?
Tự động hóa — Controller giảm thiểu các thao tác thủ công nhờ khả năng tự động điều chỉnh quy mô và thực thi trạng thái.
Độ tin cậy — Controller giúp ứng dụng đáng tin cậy hơn với các tính năng như lập lịch, tạo pod và quản lý bản sao.
Tính khả dụng — Bằng cách tự động xử lý các sự kiện lỗi, Controller đảm bảo tính khả dụng cao của các ứng dụng.
Hiệu quả — Controller cung cấp khả năng quản lý tài nguyên cụm hiệu quả với chức năng quản lý báo giá và tự động mở rộng quy mô.
Tính linh hoạt — Với các tính năng như cập nhật liên tục, Controller giúp triển khai ứng dụng nhanh hơn và quản lý vòng đời.
Tính linh hoạt — Controller có sẵn cho nhiều nhu cầu khác nhau, như tác vụ hàng loạt, dịch vụ daemon, triển khai, v.v.
Khả năng quan sát — Controller cung cấp khả năng hiển thị rõ ràng về các thay đổi trạng thái tài nguyên cụm, sự kiện và tình trạng hoạt động thông qua API K8s.
Kubernetes Operator là gì?
Kubernetes operators là một phân loại con của controller sử dụng API mở rộng — hoặc tài nguyên tùy chỉnh — để hoàn thành nhiệm vụ. Operators thường được xây dựng như một tập hợp các controller độc lập, mỗi controller chịu trách nhiệm cho tập hợp con các task và resource liên quan đến ứng dụng được quản lý.
Trong khi operator chia sẻ các chức năng tương tự với controller, nó chỉ sử dụng các tài nguyên tùy chỉnh và tập trung vào một miền.
Mặt khác, controller hoạt động mà không cần tài nguyên tùy chỉnh hoặc phần mở rộng API và không cần kết nối với một miền cụ thể. Các operator vì thế mà rất phù hợp để đáp ứng nhu cầu hoạt động cho một ứng dụng hoặc nền tảng cụ thể, nhưng chúng không đáp ứng được trạng thái cụm tài nguyên chung cũng như controller.
1. Các hàm điều khiển Kubernetes Operator
Kubernetes operators là controller được xây dựng tùy chỉnh mở rộng chức năng Kubernetes để tự động triển khai, quản lý và vận hành các ứng dụng hoặc dịch vụ cụ thể. Một số chức năng chính mà nó có thể cung cấp bao gồm:
Quản lý vòng đời — Operator quản lý toàn bộ vòng đời của các ứng dụng phức tạp có trạng thái như cơ sở dữ liệu thông qua triển khai, nâng cấp và mở rộng quy mô. Họ cũng có thể áp dụng logic ứng dụng để tự động hóa các hoạt động như sao lưu và khôi phục.
Quản lý khai báo — Bằng cách tận dụng các khái niệm trừu tượng của K8 như CRD, các operator cung cấp khả năng quản lý khai báo đơn giản cho các ứng dụng thông qua các tài nguyên tùy chỉnh.
Bảo mật — Operator tích hợp các biện pháp bảo mật hàng đầu, chẳng hạn như mã hóa, quản lý bí mật và kiểm soát truy cập, để bảo vệ các ứng dụng. Các biện pháp này có thể tùy chỉnh từ ứng dụng này sang ứng dụng khác.
Lên lịch nâng cao — Operator cho phép tùy chỉnh chính sách lên lịch cho tài nguyên ứng dụng bằng cách tích hợp với trình lập lịch K8s.
Phát hiện và điều chỉnh lỗi — Operator phát hiện lỗi và bất thường về tài nguyên như bế tắc thông qua kiểm tra tình trạng cụ thể của ứng dụng và thực hiện hành động để sửa lỗi. Họ cũng có thể tự động điều chỉnh ứng dụng bằng cách phân tích số liệu và điều kiện bằng kiến thức về miền.
Di chuyển - Operator có thể tự động di chuyển ứng dụng trên các cụm tài nguyên và phiên bản Kubernetes với sự gián đoạn tối thiểu.
Phục hồi sau thảm họa — Sử dụng logic lấy ứng dụng làm trung tâm, operator tạo và thực hiện quy trình sao lưu và phục hồi sau thảm họa cho các ứng dụng có trạng thái.
Bằng cách mã hóa kiến thức vận hành này vào controller tự động, Kubernetes Operator sẽ giảm bớt công việc thủ công và lỗi của con người đồng thời đảm bảo các ứng dụng chạy đáng tin cậy trên các cụm Kubernetes theo các thông lệ tốt nhất.
2. Các trường hợp nào nên sử dụng Kubernetes Operator?
Kubernetes Operator có một số trường hợp sử dụng chính để quản lý và tự động hóa các ứng dụng và dịch vụ trong một cụm:
Quản lý cơ sở dữ liệu — Triển khai, nâng cấp, sao lưu và khôi phục cơ sở dữ liệu bằng cách sử dụng trình điều hành như MySQL hoặc Postgres.
Cung cấp lưu trữ — Cung cấp và quản lý các hoạt động lưu trữ cho các ứng dụng có trạng thái dựa trên nhu cầu cụ thể của chúng.
Ghi nhật ký và giám sát — Đơn giản hóa việc phân tích và thu thập số liệu bằng cách triển khai các công cụ ghi nhật ký và giám sát như Elasticsearch hoặc Prometheus.
Triển khai các công cụ CI/CD — Tự động hóa việc triển khai, cấu hình và mở rộng quy mô các đường ống CI/CD bằng các công cụ như Jenkins trong hệ điều hành của bạn.
Tích hợp hệ thống nhắn tin — Tự động thực hiện mở rộng cụm, nâng cấp và kiểm tra tình trạng với operator nhắn tin có thể quản lý hệ thống, như RabbitMQ.
Sử dụng công cụ lưới dịch vụ — Cài đặt các công cụ lưới dịch vụ như Istio để quản lý lưu lượng ứng dụng, tự động triển khai canary, v.v.
Sao lưu và khôi phục — Tự động sao lưu ứng dụng của bạn và sử dụng hệ thống phục hồi sau thảm họa cho các ứng dụng cụ thể.
Nhìn chung, các Kubernetes Operator mang đến một cách tiếp cận tập trung vào ứng dụng để quản lý các dịch vụ trên Kubernetes. Họ cho phép các hoạt động hoàn toàn tự động trong khi cải thiện độ tin cậy, khả năng quan sát và tối ưu hóa — tất cả đều tuân theo các thông lệ tốt nhất.
3. Lợi ích của việc sử dụng Kubernetes operators
Các Kubernetes Operator cung cấp một số lợi ích chính:
Quản lý ứng dụng đơn giản — Operator quản lý các hoạt động ứng dụng phức tạp và cung cấp khả năng quản lý khai báo thông qua các tài nguyên tùy chỉnh.
Tăng năng suất — Operator tự động hóa khối lượng công việc nặng thường cần thiết để chạy và bảo trì các ứng dụng phức tạp, cho phép các nhà phát triển tập trung nhiều hơn vào mã.
Tính nhất quán khi triển khai — Các Operator cho phép các ứng dụng có trạng thái phức tạp chạy nhất quán trên các môi trường đám mây tại chỗ, kết hợp hoặc công cộng.
Khả năng mở rộng — Bằng cách mở rộng API K8s với CRD, Operator cho phép quản trị viên hỗ trợ nhiều loại ứng dụng khác nhau mà không cần tăng thêm khối lượng công việc.
Quản lý hệ sinh thái — Với hệ sinh thái Operator phong phú, các nhóm cơ sở hạ tầng có thể dễ dàng quản lý nhiều ứng dụng phức tạp trên K8s.
Tính mô-đun — Các Operator có tính mô-đun vì họ thường tập trung vào việc tự động hóa các tác vụ cho một ứng dụng hoặc nền tảng duy nhất.
Chuyển đổi hệ thống cũ dễ dàng hơn — Operator đơn giản hóa đáng kể quá trình chuyển đổi sang K8 cho các tổ chức cũ có ứng dụng phức tạp.
Lợi ích chính của việc sử dụng Operator là mã hóa chuyên môn vận hành và cho phép Kubernetes tự động hóa hoàn toàn việc triển khai và vận hành các ứng dụng hoặc dịch vụ phức tạp.
Controller so với Operator - Bạn nên sử dụng phương pháp nào?
Bây giờ bạn đã biết các chức năng, trường hợp sử dụng và lợi ích của Kubernetes Controller và Operator rồi — nhưng làm thế nào để bạn quyết định phương pháp nào phù hợp với doanh nghiệp của mình? Hãy để tôi cung cấp một số điều sau đây để giúp bạn hiểu rõ hơn nhé.
Kubernetes Controller thường được sử dụng cho các ứng dụng không trạng thái với các trường hợp sử dụng phổ biến, như giao diện người dùng. Như đã đề cập trước đó, controller chứa các trạng thái cụm tài nguyên chung, do đó chúng hoạt động tốt nhất cho các ứng dụng đơn giản hơn với các yêu cầu tự động hóa cơ bản cho các chức năng như mở rộng quy mô và sao lưu. Controller cũng sử dụng các trừu tượng Kubernetes mặc định, nghĩa là chúng kết hợp tốt với các ứng dụng đám mây gốc trong các môi trường đơn lẻ có nhu cầu hoạt động tối thiểu. Nếu mục tiêu chính của bạn là mở rộng và kiểm soát Kubernetes ở cấp tài nguyên API, thì lựa chọn tốt nhất của bạn có thể là controller có thể đáp ứng các mục đích chung không bị ràng buộc với một miền cụ thể.
Mặt khác, các Kubernetes Operator hữu ích cho việc triển khai và quản lý các ứng dụng có trạng thái như cơ sở dữ liệu. Các operator tự động hóa các tác vụ phức tạp cho các ứng dụng và nền tảng cụ thể bằng cách sử dụng kiến thức vận hành sâu sắc. Chúng cho phép quản trị viên tạo các tài nguyên tùy chỉnh và đạt được khả năng tự động hóa nâng cao các tác vụ quản lý vòng đời như mở rộng quy mô, nâng cấp và lưu trữ. Ngoài ra, các Operator là lựa chọn tốt nhất cho các tổ chức muốn chuyển đổi từ các ứng dụng cũ hoặc triển khai các ứng dụng một cách nhất quán trên nhiều môi trường. Nếu bạn cần làm việc ở cấp độ chi tiết trong một ứng dụng phức tạp và giảm chi phí vận hành, Kubernetes Operator sẽ là lựa chọn phù hợp.
Nhìn chung, Kubernetes Controller nên sử dụng ở cấp độ tài nguyên, trong khi các Operator tập trung vào việc tự động hóa các ứng dụng cụ thể, phức tạp. Bạn nên đánh giá các đặc điểm ứng dụng, môi trường, kiến thức vận hành và nhu cầu tự động hóa để đưa ra quyết định cuối cùng giữa controller hoặc Operator.
Phần kết luận
Kubernetes Controller và Operator chắc chắn sẽ đưa quản lý tác vụ tài nguyên cụm của bạn lên một tầm cao mới. Chúng tôi hy vọng hướng dẫn này có thể đóng vai trò như một lộ trình giúp bạn đánh giá kiến thức và nhu cầu ứng dụng cụ thể của mình, để bạn có thể bắt đầu hành trình Kubernetes của mình sớm nhất có thể. Cảm ơn các bạn đã theo dõi bài viết này.",https://viblo.asia/p/su-khac-biet-giua-kubernetes-controller-va-kubernetes-operator-la-gi-zOQJwoPyJMP
44,Tối ưu hóa việc lọc dữ liệu với Java 8 Predicates,"Java 8 đã giới thiệu cho chúng ta biết đến một giao diện chức năng mạnh mẽ được gọi là Predicate, sự ra đời của Predicate đã cách mạng hóa việc lọc dữ liệu, khiến mọi thứ trở nên dễ dàng hơn. Bằng cách cung cấp một cách ngắn gọn và diễn đạt để xác định tiêu chí bộ lọc, các Predicates đã dần cải thiện đáng kể khả năng đọc và bảo trì mã. Tuy nhiên, việc sử dụng hiệu quả các Predicates đòi hỏi phải cân nhắc cẩn thận để tối ưu hóa hiệu suất và tránh những sai lầm phổ biến có thể gặp phải.
Bài viết này sẽ đi sâu vào sự phức tạp của tối ưu hóa thuật ngữ, khám phá các phương pháp hay nhất, cân nhắc về hiệu suất và các kỹ thuật tiên tiến để tối đa hóa hiệu quả của hoạt động lọc dữ liệu của bạn.
Hiểu rõ hơn về Predicate
1. Khái niệm cốt lõi
Predicate trong Java 8 là một giao diện chức năng biểu diễn một hàm có giá trị boolean của một đối số. Nói một cách đơn giản hơn, đó là một điều kiện trả về true hoặc false khi áp dụng cho một đối tượng. Predicate chủ yếu được sử dụng để lọc dữ liệu trong các collections hoặc các luồng.
2. Các phương pháp Predicate phổ biến
test(T t): Đây là phương thức cốt lõi của Predicate, phương thức này lấy một đối số và trả về một giá trị boolean cho biết liệu đối số có thỏa mãn điều kiện của predicate hay không.
and(Predicate<? super T> other): Kết hợp hai vị ngữ bằng hàm AND.
or(Predicate<? super T> other): Kết hợp hai vị ngữ bằng hàm OR.
negate(): Phủ định predicate hiện tại, trả về một predicate khác nhằm biểu diễn phép phủ định logic của predicate này.
3. Ví dụ thực tế khi sử dụng Predicate
import java.util.function.Predicate;
 
public class PredicateExample {
    public static void main(String[] args) {
        Predicate<Integer>   
 isEven = number -> number % 2 == 0;
        Predicate<Integer>   
 isPositive = number -> number > 0;
 
        // Filtering with a single predicate
        List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5, 6);
        List<Integer> evenNumbers = numbers.stream()
                                          .filter(isEven)
                                          .collect(Collectors.toList());
 
        // Combining predicates
        Predicate<Integer> isEvenAndPositive = isEven.and(isPositive);
        List<Integer> evenPositiveNumbers = numbers.stream()
                                                   .filter(isEvenAndPositive)
                                                   .collect(Collectors.toList());
    }
}
Ví dụ này minh họa cách tạo và sử dụng Predicates để lọc dữ liệu trong danh sách.
Tối ưu hóa hiệu suất của Predicate
1. Các yếu tố ảnh hưởng đến hiệu suất của predicate
Một số yếu tố có thể ảnh hưởng đáng kể đến hiệu suất của các hoạt động lọc dựa trên thuật ngữ:
Kích thước tập dữ liệu: Tập dữ liệu lớn hơn thường đòi hỏi nhiều thời gian xử lý hơn.
Độ phức tạp của predicate: Predicate càng phức tạp có thể dẫn đến việc đánh giá bị chậm hơn.
Cấu trúc dữ liệu cơ bản: Hiệu quả của cấu trúc dữ liệu được sử dụng để lưu trữ dữ liệu có thể ảnh hưởng đến hiệu suất lọc.
Tối ưu hóa JVM: Khả năng tối ưu hóa biểu thức predicate của JVM có thể ảnh hưởng đến tốc độ thực thi.
2. Tránh các hoạt động tốn kém khi dùng Predicate
Để cải thiện hiệu suất của predicate, điều quan trọng là phải biết cách giảm thiểu các hoạt động tốn kém trong test phương pháp của predicate. Hãy xem xét các hướng dẫn sau:
Tránh các tính toán không cần thiết: Thực hiện các phép tính phức tạp bên ngoài predicate nếu có thể.
Sử dụng bộ nhớ đệm: Lưu trữ bộ nhớ đệm các kết quả trung gian để tránh tính toán trùng lặp.
Phân tích điểm nghẽn hiệu suất: Xác định điểm nóng về hiệu suất bằng cách sử dụng các công cụ phân tích để tối ưu hóa các khu vực cụ thể.
3. Sử dụng chỉ mục hiệu quả
Nếu bạn đang làm việc với các tập dữ liệu lớn, các chỉ mục có thể cải thiện đáng kể hiệu suất lọc. Hãy cân nhắc việc tạo các chỉ mục phù hợp cho các thuật ngữ thường dùng.
4. Lưu trữ kết quả dự đoán
Đối với các predicate được đánh giá nhiều lần với cùng một đối số, việc lưu trữ đệm kết quả có thể tối ưu hóa hiệu suất. Tuy nhiên, hãy sử dụng lưu trữ đệm một cách thận trọng để tránh tiêu thụ bộ nhớ quá mức.
Kỹ thuật Predicate nâng cao
1. Triển khai predicate tùy chỉnh
Trong khi giao diện Predicate tích hợp cung cấp nền tảng vững chắc, bạn có thể cần tạo các predicate tùy chỉnh cho các trường hợp sử dụng cụ thể. Điều này có thể đạt được bằng cách triển khai giao diện Predicate hoặc sử dụng biểu thức lambda.
import java.util.function.Predicate;
 
public class CustomPredicate implements Predicate<String> {
    @Override
    public boolean test(String   
 s) {
        // Custom logic for filtering strings
        return s.startsWith(""A"") && s.length() > 5;
    }
}
2. Chuỗi và sự kết hợp các predicate
Java 8 cung cấp các phương thức như and(), or(), và negate() để kết hợp nhiều predicate với nhau. Điều này cho phép xây dựng các điều kiện lọc phức tạp.
Predicate<Integer> isEven = number -> number % 2 == 0;
Predicate<Integer> isPositive = number -> number > 0;
 
Predicate<Integer> isEvenAndPositive = isEven.and(isPositive);
3. Sử dụng các predicate với các luồng hoạt động khác
Các predicate thường được sử dụng kết hợp với các luồng hoạt động khác như map, flatMap, và reduce. Điều này cho phép chuyển đổi dữ liệu và tổng hợp mạnh mẽ hơn.
List<String> names = Arrays.asList(""Alice"", ""Bob"", ""Charlie"");
List<String> upperCaseNames = names.stream()
                                    .filter(name -> name.length() > 4)
                                    .map(String::toUpperCase)
                                    .collect(Collectors.toList());
Các trường hợp sử dụng thực tế của Predicate
Các predicate được ứng dụng rộng rãi trong nhiều lĩnh vực khác nhau. Sau đây là một số trường hợp sử dụng phổ biến:
1. Lọc collections
Các nhà bán lẻ trực tuyến thường sử dụng các thuật ngữ để lọc danh mục sản phẩm dựa trên nhiều tiêu chí khác nhau như phạm vi giá, danh mục, thương hiệu hoặc sở thích của khách hàng. Ví dụ, chức năng tìm kiếm của Amazon phụ thuộc rất nhiều vào bộ lọc dựa trên thuật ngữ để hiển thị các sản phẩm có liên quan cho khách hàng.
Trích xuất các phần tử cụ thể từ danh sách dựa trên tiêu chí (ví dụ: tìm tất cả các số chẵn, lọc sản phẩm theo phạm vi giá).
Xóa các mục trùng lặp khỏi collections.
2. Truy vấn cơ sở dữ liệu
Xây dựng mệnh đề WHERE động trong truy vấn cơ sở dữ liệu.
Triển khai lọc trong bộ nhớ trước khi truy cập cơ sở dữ liệu.
3. Xác thực dữ liệu
Kiểm tra xem dữ liệu đầu vào có đáp ứng các yêu cầu cụ thể hay không (ví dụ: xác thực email, độ mạnh của mật khẩu).
4. Logic kinh doanh
Đóng gói các quy tắc kinh doanh phức tạp dưới dạng các thuật ngữ (ví dụ: xác định xem khách hàng có đủ điều kiện để được giảm giá hay không).
Kết luận
Java 8 Predicates cung cấp một cơ chế mạnh mẽ để lọc dữ liệu một cách hiệu quả nhất. Bằng cách hiểu rõ các khái niệm cốt lõi, kỹ thuật tối ưu hóa và các ứng dụng thực tế, các lập trình viên có thể cải thiện đáng kể khả năng đọc, khả năng bảo trì và hiệu suất của mã. Trong khi lựa chọn giữa các vòng lặp truyền thống và các hoạt động dựa trên luồng phụ thuộc vào các trường hợp sử dụng cụ thể, thì việc thành thạo các predicates là điều cần thiết cho quá trình phát triển Java hiện đại. Bằng cách kết hợp các predicates với các tính năng lập trình chức năng khác, bạn có thể mở khóa toàn bộ tiềm năng của Java 8 và tạo ra các giải pháp nhanh gọn và hiệu quả hơn. Cảm ơn các bạn đã theo dõi bài viết này.",https://viblo.asia/p/toi-uu-hoa-viec-loc-du-lieu-voi-java-8-predicates-pgjLN2APL32
45,Laravel VS Symfony: Which PHP Frameworks Did You Choose For Your Framework?,"Symfony and Laravel Development have been struggling to gain a better position in the war. Both PHP frameworks help faster web development and meet business requirements if you have the proper knowledge and expertise.
A brief overview of Laravel Development:
It is primarily used for custom web app development by using PHP development. Laravel Development is a server-side framework that focuses on data manipulation. It uses the Model-View-Controller structure that helps in keeping the development process structured and effortless.
Some unique features of the Laravel framework are:
* Template Engine
Laravel helps web developers by giving them ease of development and lightweight through the Blade template engine. This framework helps to build dynamic content with the help of CSS and JS files in the templates.
* Artisan
A Command Line Interface which handles the repetitive tasks during Laravel development. It reduces the app development time and effort with a cleaner coding interface. Artisan can be used to create MVC files from the CLI and manage them easily.
Eloquent ORM
This tool helps web developers to engage with the database using SQL programming language. Instead of the PHP syntax, the ORM is faster.
Libraries
There are pre-installed, object-oriented, third-party, and other libraries. Laravel’s libraries help to add functionalities, build modular, and develop effective web applications.
* Migration System
In Laravel Development, you have less risk of losing data. The application’s database structure does not need recreation every time the developers introduce changes in the structure.
A brief overview of Symfony Development:
The web developers get a robust set of tools that help in developing complex web apps effortlessly. Some key features of this framework are:
* Bundles
It is a collection of organized files. Developers use these to build and reconfigure apps. This helps web developers reduce costs and development time.
* Components
There are 30 components that help in the development process. Web developers have the freedom to build their modules for a better user experience.
* Customization
It has a vast collection of functionalities and features to support custom web app development. This ensures that the platform matches customer expectations and trends by daily tribute.
* Security Component
It is a powerful system of Symfony. Web developers use this solution to enhance the security of web apps. There are various tools to authorize and authenticate, encode passwords, and many others.
What are the pros and cons of Symfony vs. Laravel Development?
Pros of Laravel:
It provides regular updates
Integrating apps with popular email services using API
Highly compatible with third-party libraries and platforms
Additional tools with a large ecosystem
Cons of Laravel:
There are chances that some applications might be heavier and affect the loading speed
Some updates might not be compatible with the latest versions
Pros of Symfony:
It has the support of a wide community
Support platforms like Drupal and Magento
Keeps the developers updated with the recent requirements
Cons of Symfony:
It has a difficult learning curve when compared to another PHP framework
There are a lack of origin elements
It needs to be prebuilt code, which means more time consumption and a slower development process
What are the popular apps made using Symfony and Laravel Development?
Symfony Development
Spotify is a highly customized platform to listen to songs and podcasts
BlaBlaCar uses Symfony as backend, which helps to access the database quickly
Course Hero is used for task management, emails, and enhance user performance
Trivago helps to compare and search hotels across devices
Vogue is used to create a database in French and Dutch for the readers
Laravel Development
Laracasts is an e-learning website. It provides courses and serves solutions to online queries
Barchart is a financial portal. It helps to gain information related to the global market, currency exchange, gold price, and more
Asgard CMS is a modular multilingual CMS that provides ready-to-use modules.
World Walking is a charity-led healthcare platform. The main aim is to motivate people to walk more by offering them rewards
Neighborhood lenders use this platform as backend support. It acts as a mortgage calculator with great financial functionalities.",https://viblo.asia/p/laravel-vs-symfony-which-php-frameworks-did-you-choose-for-your-framework-zOQJwoKNJMP
46,NextCloud là gì? Biến VPS thành Cloud Storage với NextCloud,"Các dịch vụ đám mây đang ngày càng trở nên phổ biến, nhất là đối với việc lưu trữ dữ liệu. Google Drive hay Dropbox chắc hẳn là những cái tên đã quá quen thuộc với người dùng và bên cạnh đó, NextCloud cũng là một giải pháp lưu trữ vô cùng tiện ích. Hãy cùng tìm hiểu thêm về NextCloud trong bài viết này của TotHost nhé!
1. Giới thiệu về NextCloud
1.1. NextCloud là gì?
NextCloud là phần mềm mã nguồn mở cung cấp giải pháp lưu trữ đám mây cho cá nhân và doanh nghiệp. Cũng giống như các nền tảng lưu trữ đám mây khác, NextCloud chi phép người dùng lưu trữ, truy cập và chia sẻ dữ liệu từ bất kì thiết bị ở bất cứ đâu.
Các nhà phát triển đã phát triển NextCloud dựa trên ownCloud vào tháng 6, 2016. Nó hỗ trợ một số nền tảng phổ biến như Android, Linux, Windows, iOS, MacOS.
Khi sử dụng NextCloud, người dùng hoàn toàn kiểm soát tập tin của họ, lựa chọn nơi lưu trữ mà không phụ thuộc vào hạ tầng của bên thứ ba và cấp quyền truy cập cho người khác. Bạn có thể sử dụng máy chủ riêng ảo để có thể sử dụng giải pháp Cloud Storage này một cách dễ dàng.
1.2. Tính năng nổi bật của NextCloud
Từ chia sẻ dữ liệu linh hoạt và lưu trữ trung tâm trên đám mây đến các công cụ làm việc nhóm thực tế, Nextcloud cung cấp một bộ tính năng đầy đủ. Các tính năng này bao gồm:
Tải lên/tải xuống: Giống như Google Drive và các lựa chọn đám mây khác, Nextcloud cho phép bạn tải lên và tải xuống các tập tin cá nhân hoặc toàn bộ thư mục. Mở chúng không phụ thuộc vào thiết bị và vị trí.
Nextcloud Files: Cho phép chia sẻ tập tin cho các cá nhân cụ thể hoặc thực hiện hành động trên cùng một tập tin. Điều này giúp bạn làm việc trên một tài liệu nhóm và sử dụng các tính năng như chức năng bình luận, cộng tác trong thời gian thực hoặc tích hợp tài liệu Microsoft Office.
Nextcloud Office: Giải pháp đám mây đi kèm với bộ công cụ văn phòng trực tuyến riêng bao gồm các công cụ cộng tác và hỗ trợ định dạng tệp lớn.
Nextcloud Hub: Giao diện người dùng trung tâm cho phép bạn tích hợp, kiểm soát và quản lý bốn chức năng Nextcloud quan trọng nhất: Files, Talk, Office và Groupware.
Nextcloud Talk: Cung cấp các công cụ thuận tiện cho các cuộc gọi video, hội nghị, chia sẻ màn hình hoặc trò chuyện cho các nhóm làm việc và đội ngũ.
Nextcloud Groupware: Một công cụ lịch và quản lý hữu ích, Groupware bao gồm email, lịch biểu, danh sách liên hệ và nhiều hơn giúp công việc hoàn thành đúng hạn và năng suất cao hơn.
Nextcloud sync Client: Tính năng đồng bộ hóa dữ liệu cho phép bạn tạo WebDAV và hiển thị NextCloud như một ổ đĩa ngay trên PC/laptop của mình.
1.3. NextCloud có miễn phí không?
Câu trả lời là miễn phí nhưng không hoàn toàn. Bạn có thể tải xuống sử dụng miễn phí NextCloud và những tính năng cơ bản được tích hợp của phần mềm lưu trữ đám mây sẽ không tốn bất cứ chi phí nào. Nhưng đối với một số tính năng nâng cao, người dùng phải cài đặt thêm và trả phí. Hơn nữa, phiên bản miễn phí thường có tính bảo mật thấp hơn so với phiên bản trả phí. Do đó, nếu phải lưu trữ những dữ liệu quan trọng, bạn hãy cân nhắc sử dụng phiên bản trả phí để có thể tránh được những rủi ro về bảo mật.
2. Dùng NextCloud tạo Cloud Storage với VPS TotHost
Chỉ với VPS, bạn có thể tự cài đặt NextCloud và có một đám mây lưu trữ của riêng mình theo hướng dẫn và video sau:

snap install nextcloud
snap install nextcloud
Hệ thống sẽ bắt đầu cài đặt NextCloud. Sau vài phút, thông báo ""NextCloud Installed"" xuất hiện nghĩa là việc cài đặt đã thành công.
Để sử dụng, hãy mở trình duyệt và sử dụng thông tin sau để truy cập vào Cloud Storage của bạn:
Link truy cập: Địa chỉ IP của VPS vừa cài NextCloud
Thông tin đăng nhập: Nhập thông tin cài đặt tài khoản admin của VPS
Sau khi đăng nhập thành công, NextCloud sẽ gợi ý một số ứng dụng có thể cài thêm. Nếu bạn không muốn, có thể trực tiếp chọn ""Skip"" để bỏ qua.
Chọn ""Syllabus"" và bạn bạn sẽ được đưa tới ổ lưu trữ của bạn để tải file lên như các nền tảng lưu trữ đám mây thông thường khác.
Nếu muốn chia sẻ đám mây của mình với ngươi khác, bạn có thể thêm một hoặc nhiều user khác. Để tùy chỉnh người dùng, bạn cần vào giao diện quản trị User:
Click vào avatar Admin ở góc trên bên phải màn hình và chọn mục Users;
Thêm nhóm người dùng: Chọn tiếp mục + New Group.
Thêm người dùng mới: Chọn mục New User và điền các thông tin như Display Name, Password, Group, Role cũng như cài đặt dung lượng lưu trữ user này được phép sử dụng.
Sau khi up file lên đây, bạn có thể cài đặt chia sẻ với những User và Group đã tạo ở trên.
Trên đây là hướng dẫn cơ bản về cách sử dụng NextCloud để tạo đám mây lưu trữ riêng bằng VPS TotHost. Thử thực hành ngay bạn nhé.
Nguồn: tothost.vn",https://viblo.asia/p/nextcloud-la-gi-bien-vps-thanh-cloud-storage-voi-nextcloud-MG24BkdeJz3
47,Tự động tắt bàn phím laptop khi gắn bàn phím rời trên Linux,"
Có ai như mình không, đặc biệt rất thích mang laptop ra cafe ngồi gõ, không những thế mà lại còn thích dùng cả phím cơ nữa chứ. Thế nhưng đời không như là mơ, có những quán cà phê với bàn và diện tích rất nhỏ, thế nên việc để phím cơ lên laptop sẽ gây ra một tình trạng muôn thuở đó là...
Câu chuyện cấn phím 😵‍💫
Và bài viết này, với một trick nhỏ sẽ giúp mọi người dứt điểm được tình trạng đó, bằng cách chạy script với duy nhất 1 logic đơn giản là:
Tắt phím internal của laptop khi phát hiện keyboard cần tìm được gắn vào
Bật lại phím internal của laptop khi keyboard cần tìm bị rút ra
1. Tìm tên bàn phím
Trước tiên thì gõ lệnh đơn giản sau để tìm tên bàn phím của các bạn trước đã:
xinput list
Đây là mẫu output của laptop mình:

Okay bây giờ thì ta sẽ chỉ cần quan tâm đến cái nhánh có tên là Virtual core keyboard thôi nha:
Thằng XTEST thì không cần quan tâm tới
Và bàn phím internal của laptop sẽ có tên kiểu kiểu như đây: (thường là giống y chang)
AT Translated Set 2 keyboard
Tiếp đến, là bàn phím rời thường ở dưới cùng của mục, như của mình là Evision AKKO Keyboard, nhưng của các bạn sẽ là một cái tên khác, thường thì ta sẽ nhận qua brand bàn phím là nhanh nhất, như của mình là kit AKKO thì thấy có AKKO là xúc thôi.
Nếu không chắc chắn về tên phím rời của mình lắm thì các bạn có thể làm cái trò là: ngắt kết nối với phím rời, gõ xinput list lần 1, sau đó kết nối rồi gõ xinput list lần thứ 2 để check sự khác biệt ở hai output
xinput list
xinput list
Chúng ta note những cái tên này lại nha 😀
2. Tạo script thôi
Đầu tiên thì mình tạo file script tên tùy thích:
touch disable_laptop_keyboard.sh
chmod +x disable_laptop_keyboard.sh # để cấp quyền execute cho em nó
Thêm nội dung sau vào file disable_laptop_keyboard.sh:
disable_laptop_keyboard.sh
Thay đổi giá trị tên bàn phím rời đã note ở bước 1 trong function check_external_keyboard. Check lại xem của AT_KEYBOARD_ID có trùng khớp với giá trị bên trên không, không thì mình chủ động đổi nhé.
check_external_keyboard
AT_KEYBOARD_ID
#!/bin/bash

# check xem ban phim roi co duoc ket noi khong
check_external_keyboard() {
    xinput list | grep -q 'Evision AKKO Keyboard'
}

# id cua ban phim internal trong laptop
AT_KEYBOARD_ID='AT Translated Set 2 keyboard'

while true; do
        if check_external_keyboard; then
                xinput disable ""$AT_KEYBOARD_ID""
        else
                xinput enable ""$AT_KEYBOARD_ID""
        fi
        sleep 1
done
Giải thích từng bước:
Đầu tiên mình check tên phím đã noted ở bước 1 có tồn tại trong xinput list không bằng cách đơn giản sau:

Mình đóng function cho lệnh này để check xem output có là empty hay không
xinput list
Mình đóng function cho lệnh này để check xem output có là empty hay không
xinput list | grep -q 'Evision AKKO Keyboard'
Gán biến internal laptop vào trong script
Thường thì chỉ khi giá trị mà bạn đã check khác thì mới cần thay đổi bước này
AT_KEYBOARD_ID='AT Translated Set 2 keyboard'
Sau đó thì ta tiến hành thực hiện logic đã đề cập ở phần dẫn nhập:
Tắt phím internal của laptop khi phát hiện keyboard cần tìm được gắn vào, tức là if check_external_keyboard; then thực hiện tắt phím bằng lệnh sau:
if check_external_keyboard; then
if check_external_keyboard; then
   xinput disable ""$AT_KEYBOARD_ID""
Bật lại phím internal của laptop khi keyboard cần tìm bị rút ra, tức là else thực hiện bật phím bằng lệnh sau:
else
else
    xinput enable ""$AT_KEYBOARD_ID""
Thực hiện việc check như này liên tục mỗi giây 1 lần, hoặc realtime thì bạn bỏ sleep 1 trong script nhé.
sleep 1
3. Thêm vào startup script
Ubuntu, Debian
Mở Startup Applications qua Activities (nhấn Super). Hoặc đơn giản hơn là nhấn Alt+F2 và gõ lệnh gnome-session-properties và nhấn Enter.

Nhấp vào Add và nhập đường dẫn của file script vừa tạo tại mục Command, đặt tên tùy thích, sau đó nhấn Save.

Khởi động lại là xong.
Các distro khác
Move file script đã tạo vào /etc/init.d/:
/etc/init.d/
sudo mv ./disable_internal_keyboard.sh /etc/init.d/disable_internal_keyboard
Lưu ý là khi move vào mình bỏ cái đuôi sh cho tiện gọi nha
Update cho script này chạy startup (dùng cái tên script đã bỏ đuôi .sh ban nãy:
.sh
sudo update-rc.d disable_internal_keyboard defaults 100
Nếu cần thay đổi, thì con số 100 tức là thứ tự chạy script startup của mình thôi, mình hay dùng 100, 101, 102,....
100, 101, 102,...
Reboot lại test thử thôi
sudo reboot",https://viblo.asia/p/tu-dong-tat-ban-phim-laptop-khi-gan-ban-phim-roi-tren-linux-yZjJYKKlVOE
48,Quy trình phát triển và kiểm thử phần mềm,"Ở bài viết này tôi sẽ giới thiệu tổng quan và 1 cách đầy đủ kiến thức về quy trình phát triển phần mềm và các nguyên tắc kiểm thử phần mềm

Bước 1: Lập kế hoạch và phân tích yêu cầu ( nhiệm vụ BA)
Trước khi bắt đầu xây dựng phần mềm, BA sẽ tiến hành thu thập thông tin yêu cầu khách hàng, sau đó tiền hành nghiên cứu thị trường để xác định những chức năng mà phần mềm cần có.
Sau đó, BA sẽ làm việc với khách hàng để đưa ra các thông số kỹ thuật và yêu cầu chi tiết về sản phẩm phần mềm. Tất cả thông tin này sẽ được tổng hợp thành một tài liệu được gọi là tài liệu đặc tả yêu cầu phần mềm - Software Requirement Specication. Tài liệu gồm các yêu cầu về chức năng, giao diện hiệu suất,... Ngoài ra, còn có cả bản phác thảo về thành phần,...
Ở giai đoạn này, người quản lý và các nhà phát triển phần mềm sẽ thống nhất việc lựa chọn kiểu mô hình phát triển phần mềm nào.
Bước 2: Thiết kế phần mềm( Deginer, Developer)
Kết quả của giai đoạn này là các đặc điểm kỹ thuật thiết kế, bao gồm các chie định về thiết kế kiến trúc, yêu cầu hệ thống cũn như đại diện code,...cho phép cả nhóm phát triển theo dõi toàn bộ quá trình phát triển phần mềm
Bước 3: Phát triển phần mềm( Developer)
Bắt đầu viết code và triển khai các thông số thiết kế đã đưa ra
Bước 4: Kiểm thử phần mềm( Tester)
Các tester sẽ tào ra các tình huoongs kiểm thử( test case và tiến hành kiểm thử.
Sau khi kiểm thử, tester sẽ cập nhập các lỗi cào công cụ quản ký và thông báo các bug cho Developer.
Bước 5: Triển khai và phát hành sản phẩm
Triển khai trên Production environment: môi trường thật, dữ liệu thật, chạy với người dùng thật và cung cấp hoàn thiện sản phẩm cho khách hàng.
Bước 6: Bảo trì

Bước 1:  Phân tích yêu cầu ( Requirement Analysis) :
Các tester sẽ phân tích tài liệu Prorotype: tài liệu đặc tả yêu cầu
Yêu cầu được chia làm 2 dạng:
Functional( chức năng): mô tả tính năng
Non-Functional: ( phi chức năng): mô tả hiệu năng
Bước 2: Lập kế hoạch kiểm thử
Lập kế hoạch kiểm thử để kiểm tra xem phần mềm có đáp ứng yêu cầu hay không. Kế hoạch kiểm thử dự án bao gồm:
Phạm vi kiểm thử, hướng tiếp cận, quy trình kiểm thử, tài nguyên và nhân lực test.
Chức năng/ module cần được kiểm tra: các công cut và môi trường kiểm thử cần có
Ai test chức năng nào? Khi nào bắt đầu thực hiện viết và hoàn thành testcase? Khi nào kết thúc?
Bước 3: Phát triển kịch bản kiểm thử
Xây dựng Testcase dựa trên yêu cầu phầ mềm.
Template của Testcase thường có 5 mục đích: ID, mục đích kiểm thử, các bước thực hiện, kết quả mong đợi và kết quả thực tế
Các Tester trong cùng 1 team sẽ Review cheos testcase của nhau tránh bỏ sót những trường hợp test quan trọng.
Bước 4: Thiết lập môi trường  kiểm thử
Thiết lập môi trường  kiểm thử là 1 hoạt động độc lập và có thể được bắt đầu cùng với các giai đoạn phát triển kịch bản kiểm thử. Môi trường kiểm thử sẽ do developer tạo ra để deploy sản phẩm đã được hoàn thiện về lập trình.
Bước 5: Thực hiện kiểm thử
Khi Developer đã code và đưa ra sản phẩm lên môi trường kiểm thử -> Tester sẽ thực hiện dựa trên Test Case đã viết.
Trong quá trình test, nếu phát hiện ra bug( lỗi) thì tester sẽ log(vieets0 lên các tool quản lý lỗi. Bug của lập trình viên nào sẽ giao lại cho người đấy xử lý. Khi nào developer fixbug xong, tester sẽ nhận lại và tiến hành kiểm thử.
Bước 6: Đóng kiểm thử
Tester chuẩn bị báo cáo kết thúc kiểm thử, tổng hợp lại các chỉ số trong quá trình test. Cả team phát triển sẽ ngồi họp để đánh giá toàn bộ các tiêu trí xác định kiểm thử đủ hay chưa
Những tiêu chí này khác nhau tùy theo từng dự án, thường bao gồm:
Số lượng test case tối đa được thực thi Pased
tỷ lệ lỗi giảm xuống dưới mức nhất định
Dealine được chốt từ giai đoạn lmf kế hoạch kiểm thử.
Quy trình kiểm thử phần mềm thường chỉ được kết thúc khi sản phẩm được bàn giao cho khách hàng. Ngoài ra, hoạt động kiểm có thể kế thúc trong các trường hợp sau:
Khi 2 dự án bị hủy bỏ
Khi các mục tiêu chính đã hoàn toàn
Khi việc bảo trì hoặc cập nhập đã hoàn thành.
Ở bài này tôi chỉ giới thiệu các bạn 7 nguyên tắc , ở các bài sau tôi sẽ nói rõ hơn về 7 nguyên tắc này và áp dụng như nào trong kiểm thử.
7 nguyên tắc cơ bản của kiểm thử phần mềm sau:
Kiểm thử chứng minh sự hiện diện của lỗi
Kiểm thử chứng minh sự hiện diện của lỗi
Kiểm thử toàn bộ là không thể
Kiểm thử toàn bộ là không thể
Kiểm thử càng sớm càng tốt
Kiểm thử càng sớm càng tốt
Lỗi thường được phân bố tập trung
Lỗi thường được phân bố tập trung
Nghịch lý thuốc trừ sâu
Nghịch lý thuốc trừ sâu
Kiểm thử phụ thuộc vào ngữ cảnh
Kiểm thử phụ thuộc vào ngữ cảnh
Quan niệm sai lầm về việc “hết lỗi”
Quan niệm sai lầm về việc “hết lỗi”
Tham khảo:
7 Nguyên tắc cơ bản của Kiểm Thử Phần Mềm
Trung tâm Anh  IT",https://viblo.asia/p/quy-trinh-phat-trien-va-kiem-thu-phan-mem-EoW4oXx7Jml
49,Mysql - Index optimization check list,"
Vì sao sử dụng index có thể cải thiện tốc độ truy vấn?
Làm cách nào để xác định khi nào cần set index cho một column?
Bài viết này sẽ giúp anh em hiểu và tự tin set index chuẩn như quân đội.
Lấy ví dụ tình huống sau.
Anh quản lý và đôi chân sưng tấy
Anh A mới được bổ nhiệm làm quản lý 100 giường bệnh của một bệnh viện.
Mỗi bệnh nhân nằm ở một phòng riêng biệt, phân biệt các bệnh nhân bằng mã bệnh nhân. Nhiệm vụ của A là quản lý danh sách bệnh nhân ở các phòng và hỗ trợ bác sĩ thăm khám.
Tìm kiếm bệnh nhân - Cuộc sống không có index
Trong ngày, các bác sĩ sẽ đến khám riêng cho từng bênh nhân hoặc một nhóm bệnh nhân.
Bác sĩ cho A một danh sách các mã bệnh nhân và yêu cầu A cho họ biết các bệnh nhân này ở phòng nào.
A đi hết 100 phòng, hỏi từng bệnh nhân, đối chiếu với mã bệnh nhân của bác sĩ để lấy được danh sách hoàn chỉnh.
Hơi vất vả nhưng A vẫn hài long với công việc hiện tại. Bắp chân A to lên, rắn chắc, A thầm cảm ơn những ngày làm việc vất vả chạy qua chạy lại 100 phòng đến hơn chục lượt.

Index - cuốn sổ cái
Nhưng một ngày A bệnh, A lết mãi mới được nửa vòng, A quyết định lần này là lần cuối, A đi hết 100 phòng, mỗi phòng A dừng lại và mapping thông tin bệnh nhân và số phòng vào một cuốn sổ cái.
Những lần tiếp theo, khi có yêu cầu, A chỉ cần giở cuốn sổ ra và trích ra danh sách phòng của từng bệnh nhân theo yêu cầu của bác sĩ.
Khi có bệnh nhân xuất viện hoặc nhập viện, A cập nhật lại trên cuốn sổ của mình, đảm bảo rằng thông tin trong cuốn sổ luôn chính xác.
Chỉ với một cuốn sổ nhỏ và một chút tỉ mỉ, công việc quản lý của A đã nhàn đi nhiều.

Rảnh rang được một thời gian, A được cất nhắc lên tuyến trên quản lý 10.000 giường bệnh.
Rút kinh nghiệm, A chấp nhận đau một lần rồi thôi, A cũng lại đi thống kê toàn bộ vào cuốn sổ cái của mình, nhưng cuốn sổ của A chằng chịt toàn chữ, dày lên trông thấy.
Mỗi lần bác sĩ đến khám, A dò 10.000 dòng trong cuốn sổ cái của mình. Chân A không còn đau nhưng mắt A bắt đầu nhoè dần sau 1 tuần làm việc.
Chia để trị
A thử chia cuốn sổ cái của mình thành 1.000 cuốn sổ nhỏ, mỗi cuốn 100 dòng, rồi chia theo dạng cây cân bằng vào giá sách có nhiều ngăn.
Các ngăn lớn ngoài cùng dán nhãn mã từ 1 -> 10.000, rồi tiếp tục từ 10.001 -> 20.000, tổng cộng có 10 ngăn.
Trong mỗi ngăn lớn lại chia thành 10 ngăn nhỏ, mã từ 1 -> 1.000, 1.001 -> 2.000, như vậy một ngăn chỉ còn có 10 cuốn sổ nhỏ mỗi cuốn 100 bệnh nhân.

Giả sử cần tìm bệnh nhân mã số = 1.890
Ở ngăn lớn ngoài cùng, A so sánh 1 < 1.890 < 10.000, do đó A biết cần tìm trong ngăn này
A tìm trong ngăn 1 -> 1.000, không thấy. Chuyển sang tìm trong ngăn có giá trị > 1.000, phát hiện ngăn con thoả mãn 1001 < 1890 < 2000.
Ngăn trong cùng này có 10 cuốn mỗi cuốn 1.000 dòng, cuốn sổ cái thứ 9 lưu mã từ 1.801 -> 1.900 sẽ là cuốn sổ mà A cần tìm.

Chỉ cần 3 bước tìm kiếm, A tìm ra được phòng bệnh nhân mong muốn. Cuốn sổ cái trong ví dụ trên tương tự với khái niệm index trong các hệ cơ sở dữ liệu. Sử dụng một vùng nhớ nhỏ để lưu giá trị của column có tính đại diên, từ đó lấy được record tương ứng.
Indexes are used to find rows with specific column values quickly. Without an index, MySQL must begin with the first row and then read through the entire table to find the relevant rows.
The larger the table, the more this costs. If the table has an index for the columns in question, MySQL can quickly determine the position to seek to in the middle of the data file without having to look at all the data.
This is much faster than reading every row sequentially.
Index là một cấu trúc dữ liệu giúp tăng tốc độ truy vấn và thao tác dữ liệu trong các bảng. Index được tạo trên một hoặc nhiều column và hoạt động như một cuốn sổ cái để tìm kiếm và truy xuất dữ liệu mà không cần quét toàn bộ bảng.
Dữ liệu càng lớn thì vai trò của index càng trở nên rõ ràng hơn.
1. Phân loại
Mysql hỗ trợ nhiều loại index đáp ứng các cách đánh chỉ mục khác nhau.
PRIMARY KEY
Primary key đảm bảo có thể xác định một record duy nhất.
Non-nullable: không cho phép giá trị NULL
NULL
Uniqueness: không chấp nhận giá trị trùng lặp
Column được khai báo làm PRIMARY KEY sẽ được tự động đánh index
PRIMARY KEY
Không thể khai báo 2 PRIMARY KEY trong một table.
PRIMARY KEY
UNIQUE
Tương tự như PRIMARY KEY, UNIQUE đảm bảo các giá trị trong index là duy nhất.
Điều thú vị là UNIQUE index cho phép NULL và còn cho phép nhiều NULL values cùng tồn tại.
PRIMARY KEY
UNIQUE
UNIQUE
NULL
NULL
INDEX
Thoáng hơn cả 2 anh trên, loại index này cho phép null, cho phép giá trị trùng lặp. Không có ràng buộc gì đặt biệt.
FULLTEXT
Chỉ hỗ trợ các kiểu dữ liệu dạng chuỗi như CHAR, VARCHAR và TEXT, được sử dụng để thực hiện các truy vấn theo ngôn ngữ tự nhiên WHERE MATCH ... AGAINST.
Duy trì các index dạng này tốn nhiều tài nguyên hơn, đặc biệt với các bảng nhiều record và có index kích thước lớn. Trong một số trường hợp có thể áp dụng prefix index để giới hạn kích thước của index dạng chuỗi.
CHAR
VARCHAR
TEXT
WHERE MATCH ... AGAINST
prefix index
SPATIAL
Được thiết kế để hỗ trợ truy vấn cho dữ liệu không gian. Thực tế tôi cũng chưa được kinh qua món này nên chỉ để đây và không nói gì.
Multi-column index (Composite index)
Khác với các loại trên, index loại này cho phép đánh chỉ mục trên nhiều column. Giả sử tôi có 1 multi-column index (col1, col2, col3), index này cũng tương đương với index(col1), index(col1, col2), index(col1, col2, col3).
Ngoài ra, multi-column index còn có thể lấy giúp cải thiện hiệu suất của các câu lệnh ORDER BY, GROUP BY, MAX, MIN.
Covering index
Là một loại multi-column index khi toàn bộ các column trong câu lệnh SELECT cũng được đánh index cùng với các column dùng để tìm kiếm.
2. B-tree (Balanced Tree)
B-tree còn được gọi là cây tự cân bằng.
Cấu trúc dữ liệu này giải quyết giới hạn của Binary search tree (Cây tìm kiếm nhị phân), một node của B-tree có thể lưu nhiều giá trị cùng lúc rút ngắn chiều dài của cây.
Hầu hết các loại index của Mysql (PRIMARY KEY, UNIQUE, INDEX, FULLTEXT) đều sử dung B-tree.
Các đặc tính của B-tree bao gồm:
PRIMARY KEY
UNIQUE
INDEX
FULLTEXT
Các node lá (leaf) luôn cùng cấp với nhau.
Quy định giá trị minimum degree (T), là số giá trị tối thiểu của một node khi quyết định có sinh node mới hay không.
Tất cả các node ngoại trừ root phải có ít nhất T-1 giá trị.
T-1
Tất cả các node bao gồm root node chỉ có thể có nhiều nhất 2T - 1 giá trị.
2T - 1
Số node con của một node cha bất kỳ bằng số giá trị của node + 1.
Tất cả các giá tị của node được sắp xếp theo thứ tự tăng dần. Con của k1 và k2 luôn có giá trị nằm trong khoảng k1 < child node < k2.
Độ phức tạo của các thao tác cơ bản như INSERT, DELETE, SELECT đều bằng nhau và bằng O(log(n))
INSERT
DELETE
SELECT
Nên set index ở đâu
Để để trả lời câu hỏi này, anh em cần thống kê ra một danh sách các query đánh giá theo 2 tiêu chí tuần suất sử dụng và độ phức tạp của query. Trong danh sách này, lọc ra các column được sử dụng cho câu lệnh điều kiện.
Sau khi xem xét các điều kiện trên anh em sẽ có 1 danh sách các column làm ứng viên để đánh chỉ mục. Việc tiếp theo là cân nhắc trong danh sách này column nào thực sự cần đánh index.
Một số tiêu chí sau có thể giúp anh em lựa chọn index sao cho phù hợp.
Số lượng record phải đủ lớn
Độ hiệu quả của index tỉ lệ thuận với kích thước của bảng. Bảng có càng nhiều record thì càng cần phải cân nhắc set index cho các column thường được sử dụng cho câu lệnh điều kiện.
Lựa chọn column có tỉ lệ trùng lặp thấp
Nếu đang cân nhắc giữa nhiều column, hãy chọn column có tỉ lệ giá trị trùng lặp thấp. Thử tưởng tượng chỉ với 1 câu lệnh WHERE trên index column,anh em có thể lọc xuống còn 10 records trên tổng số 1 triệu record.
Ngược lại, nếu anh em set index cho column kiểu boolean (TINYINT(1)) chỉ chấp nhận 2 giá trị TRUE/FALSE trên hàng triệu record, index sẽ không tạo ra nhiều khác biệt. Lí tưởng nhất là tạo các index type = UNIQUE, 1 index tương đương với 1 record.
WHERE
TINYINT(1)
TRUE/FALSE
UNIQUE
Lưu ý với column có tần suất thay đổi cao
Anh em biết rằng đánh index cho column có thể hiểu là tạo một vùng nhớ khác phục vụ việc mapping giữa column và record, điều này đồng nghĩa là Mysql phải quản lý vùng nhớ này khi có bất kì thao tác ghi liên quan (INSERT/UPDATE/DELETE).
Nếu column có tần suất thay đổi càng cao thì gánh nặng quản lý index càng tăng. Ví dụ nên đánh index trên những bảng theo cơ chế append only như bảng history với số lượng record nhiều và ít có thao tác DELETE/UPDATE một khi được khởi tạo.
INSERT
UPDATE
DELETE
DELETE
UPDATE
Sử dụng prefix index cho các column dạng chuỗi
Thực tế việc set index cho các column chuỗi kích thước lớn không được khuyến khích, nhưng nếu cần phải set index cho trường hợp này anh em có thể áp dụng prefix index.
Thay vì set index cho nguyên column (đặc biệt là dạng chuỗi không có kích thước cố định như VARCHAR/TEXT) , anh em có thể set index cho n ký tự đầu tiên của chuỗi để cải thiện performance.
prefix index
VARCHAR/TEXT
n
CREATE INDEX idx_prefix ON MY_TABLE(col1(3)); -- prefix index được set trên 3 ký tự đầu tiên của column col1

SELECT * FROM my_table WHERE col1 LIKE 'val%'; -- có sử dụng prefix
SELECT * FROM my_table WHERE col1 LIKE '%val'; -- không sử dụng prefix vì search like 3 phần tử cuối
SELECT * FROM my_table WHERE col1 = 'xx'; -- có sử dụng index vì 'xx' chỉ có 2 ký tự <= 3 ký tự của index
Sử dụng multi-column index
Như đã chia sẻ ở trên, 1 multi column index tương đương với nhiều index con nên hãy tận dụng để gộp các index đơn lẻ nếu nó cũng là một phần của các query search trên nhiều column.
Ngoài các column được sử dụng làm điều kiện cho các câu lệnh WHERE, JOIN; query còn được hưởng lợi từ việc set multi-column index trong câu lệnh ORDER BY, GROUP BY, MIN, MAX.
WHERE
JOIN
ORDER BY
GROUP BY
MIN
MAX
CREATE INDEX col123 ON my_table(col1, col2, col3);

SELECT * FROM my_table WHERE col1 = 1; -- Sử dụng index của col123 tương đương index(col1)
SELECT * FROM my_table WHERE col1 = 1 AND col2 = 'a'; -- Vẫn có thể sử dụng index của col123 tương đương index(col1, col2)
SELECT * FROM my_table WHERE col1 = 1 AND col2 = 'a' col3 = 'b'; -- Sử dụng index của col123
SELECT col1, col2 FROM my_table WHERE col1 = 1 ORDER BY col2; -- Sử dụng index của col123
Sử dụng covering index
Trong một số trường hợp, các column trong SELECT query cũng có thể được cân nhắc nếu có thể ứng dụng covering index.
Lúc này query truy vấn lấy dữ liệu trực tiếp từ vùng nhớ của index mà không cần ánh xạ sang record của bảng gốc.
SELECT
CREATE INDEX idx_col1_col2 ON MY_TABLE(col1,col2); -- composite index được set trên column col1 và col2
SELECT col2 FROM my_table WHERE col1 = val; -- chỉ SELECT các giá trị có trong composite index
Trên đây là một số gạch đầu dòng giúp anh em lập trình viên có một check list để set index sao cho hợp lý. Sau khi apply index mới, anh em nên tích cực theo dõi để đánh giá hiệu quả và phát hiện ngăn chặn sớm các side effect nếu có.
Kỹ thuật đánh chỉ mục chỉ là một trong những cách để giúp tầng database có được performance tốt, còn rất nhiều phương pháp để tối ưu và mở rộng database tôi sẽ cùng anh em tìm hiểu thêm ở các bài sau.
""Use the right tool for the right job""
Nếu anh em cảm thấy bài viết hữu ích đừng ngần ngại click upvote cho bài viết, hoặc phát hiện ý nào chưa hợp lý hoặc cần giải thích thêm hãy comment cho tôi biết để cùng trao đổi nhé.
Anh em có thể tham khảo các bài viết khác của tôi tại Blog cái nhân hoặc kết nối với tôi qua Linkedin",https://viblo.asia/p/mysql-index-optimization-check-list-WR5JRZvdJGv
50,Snowflake là gì và cách kết nối với Python,"Hello mn, hôm nay chúng ta sẽ cùng nhau tìm hiểu Snowflake nhé ^^.
Trong kỷ nguyên số, dữ liệu là tài sản quý giá nhất của các doanh nghiệp. Việc quản lý, phân tích và bảo mật dữ liệu trở thành nhiệm vụ tối quan trọng. Snowflake, một nền tảng dữ liệu đám mây, đã xuất hiện như một giải pháp đột phá, cung cấp khả năng lưu trữ, xử lý và phân tích dữ liệu một cách hiệu quả và linh hoạt.
Snowflake là một nền tảng dữ liệu đám mây được thiết kế để xử lý khối lượng dữ liệu lớn một cách nhanh chóng và dễ dàng. Được xây dựng trên cơ sở hạ tầng đám mây của các nhà cung cấp lớn như Amazon Web Services (AWS), Microsoft Azure và Google Cloud Platform (GCP), Snowflake cung cấp khả năng lưu trữ và xử lý dữ liệu mạnh mẽ mà không cần phải đầu tư vào phần cứng hoặc quản lý hệ thống phức tạp.
Khả năng mở rộng vô hạn: Snowflake có khả năng mở rộng linh hoạt, cho phép người dùng tăng hoặc giảm tài nguyên dựa trên nhu cầu thực tế mà không gây gián đoạn dịch vụ.
Hiệu suất cao: Snowflake tối ưu hóa việc xử lý truy vấn thông qua kiến trúc đa cụm, giúp cải thiện tốc độ và hiệu suất phân tích dữ liệu.
Bảo mật và tuân thủ: Snowflake tích hợp các cơ chế bảo mật tiên tiến như mã hóa dữ liệu, kiểm soát truy cập và tuân thủ các quy định bảo mật quốc tế.
Dễ dàng sử dụng: Giao diện thân thiện và trực quan của Snowflake giúp người dùng dễ dàng thao tác và quản lý dữ liệu mà không cần nhiều kiến thức chuyên sâu về kỹ thuật.
Tiết kiệm chi phí: Không cần đầu tư vào cơ sở hạ tầng phần cứng đắt đỏ, người dùng chỉ phải trả chi phí dựa trên tài nguyên thực tế sử dụng.
Tăng cường khả năng phân tích: Khả năng xử lý dữ liệu nhanh chóng và hiệu quả giúp các doanh nghiệp dễ dàng khai thác thông tin từ dữ liệu để đưa ra quyết định chiến lược.
Linh hoạt và tích hợp: Snowflake hỗ trợ tích hợp dễ dàng với nhiều công cụ BI và ETL phổ biến, giúp tạo ra một hệ sinh thái dữ liệu hoàn chỉnh.
Bước 1: Cài đặt Snowflake Connector cho Python
Snowflake cung cấp một gói đặc biệt để dễ dàng kết nối với Python. Connector có sẵn trên các hệ điều hành Linux, Windows và macOS.
Đầu tiên,  cài đặt snowflake:
pip install snowflake-connector-python
Bước 2: Tạo Kết Nối
Tạo một tệp Python mới trong IDE yêu thích của bạn. Đầu tiên, chúng ta cần import gói Snowflake connector vừa cài đặt:
import snowflake.connector
Bây giờ gói đã được import, chúng ta có thể tạo kết nối. Chúng ta sử dụng hàm connect() của gói để kết nối với Snowflake. Hàm này yêu cầu định danh tài khoản Snowflake cùng với thông tin xác thực dựa trên loại xác thực được sử dụng.
Định danh tài khoản là phân đoạn đầu tiên của tên miền URL Snowflake của bạn cùng với nhà cung cấp dịch vụ đám mây và khu vực. Ví dụ:
URL Snowflake: http://abc123.us-east-1.aws.snowflakecomputing.com
Định danh tài khoản: abc123.us-east-1.aws
Ba loại xác thực được phép: Mặc định (Default), Đăng nhập một lần (Single Sign On - SSO), hoặc Khóa cặp (Key Pair).
Việc trích xuất dữ liệu từ Snowflake ra file CSV có thể được thực hiện dễ dàng bằng  cách: sử dụng code Python
Trích xuất file CSV bằng Python
Để trích xuất dữ liệu từ Snowflake ra file CSV bằng Python, bạn cần sử dụng thư viện snowflake-connector-python. Dưới đây là đoạn mã mẫu:
import snowflake.connector
import csv

# Kết nối đến Snowflake
conn = snowflake.connector.connect(
    user='your_username',
    password='your_password',
    account='your_account'
)

# Thực hiện truy vấn
cursor = conn.cursor()
cursor.execute(""SELECT * FROM your_table"")

# Lấy dữ liệu
rows = cursor.fetchall()

# Ghi dữ liệu vào file CSV
with open('output.csv', mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow([desc[0] for desc in cursor.description])  # Ghi tiêu đề cột
    writer.writerows(rows)

# Đóng kết nối
cursor.close()
conn.close()
Snowflake đang cách mạng hóa cách các doanh nghiệp quản lý và phân tích dữ liệu. Với các tính năng nổi bật, hiệu suất cao và khả năng mở rộng linh hoạt, Snowflake mang lại những lợi ích to lớn, giúp các tổ chức tận dụng tối đa sức mạnh của dữ liệu để phát triển và cạnh tranh trong thị trường hiện đại. Trong tương lai, Snowflake hứa hẹn sẽ tiếp tục là một trong những nền tảng dữ liệu đám mây hàng đầu, hỗ trợ các doanh nghiệp trong việc chuyển đổi số và tối ưu hóa hoạt động kinh doanh.
https://docs.snowflake.com/en/developer-guide/python-connector/python-connector",https://viblo.asia/p/snowflake-la-gi-va-cach-ket-noi-voi-python-aAY4q7veLPw
51,SOLID - Đóng/ Mở (P1),"Tôi có được nghe hai lần câu chuyện ở một công ty lớn ở Việt Nam, một Solution Architect đã từ chối một pull request của đồng nghiệp với lý do các thay đổi không tuân theo nguyên tắc Đóng/Mở. Tôi nghĩ rằng đã có một yêu cầu từ khách hàng và lập trình viên đã sửa đổi một lớp sẵn có để thực hiện yêu cầu ấy. SA đã từ chối vì theo SA, không được sửa class sẵn có mà phải tìm cách mở rộng nó. Tôi không chắc đây là một câu chuyện có thật, nhưng việc nghe được nó hai lần từ hai người khác nhau, tôi nghĩ cũng có khả năng. Tại sao tôi lại không tin vào câu chuyện này? Nó có những lý do sau:
Khá là kì quặc khi từ chối một PR bằng lý do nó không tuân theo SOLID. Chúng ta là những kĩ sư, và chúng ta nói chuyện bằng giải pháp, ví dụ. Dùng một luận điểm có tính lý thuyết để bác bỏ một PR là thiếu thuyết phục.
Việc có thể mở rộng một chức năng mà không sửa đổi code cũ hay không phụ thuộc vào nhiều yếu tố. Chúng ta vẫn thường sửa đổi code là chính. Đa số các mã nguồn chúng ta tiếp xúc trong thực tế không hề tốt, không có sẵn các phương án để mở rộng.
Nguyên tắc Đóng/Mở không đứng một mình. Trừ Đơn nhiệm, các nguyên tắc khác phụ thuộc vào nhau và vào Đơn nhiệm. Bạn sẽ đạt được cả 5 nguyên tắc SOLID hoặc không đạt được gì. Dẫn chứng chỉ một nguyên tắc một lần nữa rất thiếu thuyết phục.
Tôi muốn các bạn hiểu một điều:
Đóng/Mở là nguyên tắc về thiết kế hơn là nguyên tắc viết mã
Tức là bạn đạt được Đóng/Mở phải ngay khi bạn thiết kế mã, không phải lúc bạn có một yêu cầu từ khách hàng, và với kĩ năng đầy mình bạn sẽ mở rộng code cũ để thỏa mãn yêu cầu mới. Không hề có chuyện đó. Nếu bạn có thể mở rộng code cũ thì không phải do bạn, mà là do người đã thiết kế nên code cũ cho phép bạn làm điều ấy. Không phải bạn đạt được Đóng/Mở mà là người trước bạn đã đạt được Đóng/Mở. Tất nhiên không phải không có trường hợp những code khá đóng nhưng lập trình viên vẫn tìm ra cách để mở rộng. Những thành tựu ấy không mang tính hệ thống mà thường là trick, áp dụng cho trường hợp cụ thể, và chính trick ấy cũng trở nên khó mở rộng.
Một mã nguồn được thiết kế tốt thường thể hiện sẵn các khả năng và các giải pháp để mở rộng. Hoặc nếu chưa có sẵn giải pháp để mở rộng thì nó cũng dễ dàng cài đặt giải pháp để mở rộng, bởi nó đã đạt được SLID trong SOLID. Nhưng trong đa số trường hợp, giải pháp đã có sẵn cho bạn.
Một điều nữa mà có thể chúng ta nhầm lẫn về thay đổi mã nguồn. Không phải thay đổi nào cũng là mở rộng vì:
Các thay đổi đối với mã nguồn cũng phải tuân thủ theo Đơn nhiệm
Chúng ta cần xác định yêu cầu thay đổi có Đơn nhiệm không, tức là nó có độc lập với mã nguồn sẵn có hay không. Nếu nó độc lập - đơn nhiệm thì bạn có thể mở rộng. Nếu không chúng ta vẫn phải sửa đổi mã nguồn. Cùng xét quy trình xử lý đơn hàng:
Đơn hàng => Tính tổng giá => Nhập thông tin khách => Chọn phương thức thanh toán => Thanh toán => Giao hàng => Đóng.
Chúng ta có hai yêu cầu mới:
Tính giảm giá/ khuyến mãi.
Chọn phương thức giao hàng.
Yêu cầu Chọn phương thức giao hàng rõ ràng là đơn nhiệm, ta thêm nó vào sau bước Nhập thông tin khách.
Yêu cầu Tính giảm giá/ khuyến mãi lại không như thế. Đặt trước hay sau Tính tổng giá đều không hợp lý. Có những khuyến mãi dựa trên một sản phẩm, ta có thể tính trước. Có những khuyến mãi lại dựa trên tổng giá trị đơn hàng, ta lại phải tính sau. Chưa nói về nguyên tắc tính thuế nếu giá chưa bao gồm thuế. Chúng ta khó mà tách nó ra khỏi bước tính tổng giá. Khả năng cao nó sẽ là sửa đổi chứ không thể mở rộng và giữ nguyên mã nguồn cũ (Việc thiết kế để có thể mở rộng ở đây sẽ rất tinh vi và có thể là over-design so với sửa đổi).
Mặc dù trong thực tế, chúng ta không mấy quan tâm đến Đóng/Mở đâu, nhưng nếu bạn có để ý đến nó thì cũng không cần phải áp lực quá vì nếu có thể mở rộng thì chúng ta đã mở rộng được rồi, và nếu không thể mở rộng thì giải pháp cũng chỉ là sửa đổi cái có sẵn. Tôi nói thế không có nghĩa khuyến khích các bạn vô tư đi. Việc hiểu rằng chúng ta không thể mở rộng khác với việc cứ code bừa đi cho xong. Tôi chỉ mong lúc các bạn lăn tăn, các bạn sẽ có những lời này để thoát khỏi vướng mắc ""Không được sửa đổi"".
Hóa ra tôi lại đang bảo vệ việc sửa đổi khi nói về Đóng/Mở =))",https://viblo.asia/p/solid-dong-mo-p1-pgjLN2d1L32
52,[JavaNet p3] Blocking Server,"Bài viết được lấy từ https://truongphuoc.wordpress.com/2024/07/18/javanet-p3-blocking-server/
Bài này gồm hơn 2000 chữ và rất nhiều code, tin mình đi bạn không muốn đọc trên điện thoại đâu. Vì thế nếu đang đọc trên điện thoại thì bạn nên note lại, sau đó mở laptop hay ipad lên đọc sau nha.
Trong bài Java Networking, ta đã tìm hiểu về cách hai máy tính tạo kết nối TCP với nhau sử dụng Socket và ServerSocket.
Trong bài Java IO, ta đã tìm hiểu về Stream, Buffer, Reader và Writer. Ta cũng đã biết Reader, Writer có thể kết hợp được với Stream.
Note: Nếu chưa đọc hai bài trước thì các bạn đọc lại rồi hãy quay lại đây, việc bỏ qua có thể khiến việc đọc bài này trở thành ác mộng đó 😇
Trong bài viết này, ta sẽ sử dụng Java IO để đọc, ghi dữ liệu thông qua Socket.

Socket cung cấp cho chúng ta InputStream và OutputStream để có thể thao tác với dữ liệu. Chúng ta có thể sử dụng mọi thứ trong Java IO để đọc ghi dữ liệu dựa trên InputStream và OuptutStream này
Socket socket;
// socket = new Socket();
// socket = serverSocket.accpet();

InputStream in = socket.getInputStream();
OutputStream out = socket.getOutputStream();

BufferedReader reader = new BufferedReader(new InputStreamReader(in));
PrintWriter writer = new PrintWriter(out);

writer.println(""Hello"");
writer.flush();
Đấy, chỉ thế thôi. Nếu bạn đã hiểu về Java Networking và Java IO thì tới đây mọi thứ đều quen thuộc rồi. Nếu bạn chưa thấy quen thuộc thì đọc và làm bài tập hai bài trước nha, có ích lắm đó.
Học phải đi đôi với hành phải không nào, giờ chúng ta hãy cùng làm một chương trình Console đơn giản nhé. Chương trình như sau:
Client sẽ cho người dùng nhập một số, Server sẽ trả về bình phương của số đó.
import java.io.*;
import java.net.*;

public class Client {
    public static void main(String[] args) throws IOException {
        Socket socket = null;
        try {
            socket = new Socket();
            InetAddress localhost = InetAddress.getByName(""localhost"");
            InetSocketAddress inetSocketAddress = new InetSocketAddress(localhost, 8080);

            socket.connect(inetSocketAddress);
            System.out.println(""Client info: "" + socket);
            // Client info: Socket[addr=localhost/127.0.0.1,port=8080,localport=57884]

            InputStream in = socket.getInputStream();
            OutputStream out = socket.getOutputStream();

            BufferedReader reader = new BufferedReader(new InputStreamReader(in));
            PrintWriter writer = new PrintWriter(out);

            BufferedReader consoleReader = new BufferedReader(new InputStreamReader(System.in));

            System.out.print(""Enter a number (enter 0 to exit): "");
            String userInput = consoleReader.readLine();
            writer.println(userInput);
            writer.flush();

            String response = reader.readLine();
            System.out.println(""Server response: "" + response);

        } finally {
            if (socket != null) {
                socket.close();
            }
        }
    }
}
import java.io.*;
import java.net.*;

public class Server {
    public static void main(String[] args) throws IOException {
        ServerSocket serverSocket = null;
        try {
            serverSocket = new ServerSocket();
            InetAddress inetAddress = InetAddress.getByName(""0.0.0.0"");
            SocketAddress socketAddress = new InetSocketAddress(inetAddress, 8080);
            serverSocket.bind(socketAddress);

            Socket clientSocket = serverSocket.accept();
            System.out.println(""Connected to client: "" + clientSocket);
            // Connected to client: Socket[addr=/127.0.0.1,port=57884,localport=8080]

            InputStream in = clientSocket.getInputStream();
            OutputStream out = clientSocket.getOutputStream();

            BufferedReader reader = new BufferedReader(new InputStreamReader(in));
            PrintWriter writer = new PrintWriter(out);

            String clientInput = reader.readLine();
            int number = Integer.parseInt(clientInput);
            System.out.println(""client input: "" + clientInput);

            int square = number * number;
            writer.printf(""%d square is %d"", number, square);
            writer.println();
            writer.flush();

        } finally {
            if (serverSocket != null) {
                serverSocket.close();
            }
        }
    }
}
Trong ví dụ trên, Client và Server mở một kết nối TCP với nhau, sau đó trao đổi thông tin. Mô hình đại loại thế này:

Code trên khá dài do mình paste cả vào trong này để các bạn dễ dàng copy về máy và chạy thử nha, các bạn nên đọc code chứ đừng lướt qua, làm mọi thứ chậm lại thì chúng ta mới hiểu kỹ được.
Bài tập về nhà: Tự viết lại đoạn code trên, có thể biến tấu theo ý hiểu của mình. Nếu bạn đã đọc và làm bài tập trong hai bài trước thì sẽ thấy được tác dụng của việc làm bài tập nha. Nếu bạn chưa đọc hay làm bài tập của hai bài trước thì các bạn nên dừng tại đây, quay lại và đi theo lộ trình nhé.
Server và Client ở trên là khá là đơn giản, thô sơ. Chúng kết nối với nhau, trao đổi 1 thông tin và đóng kết nối.
Để dễ dàng hình dung thì chúng ta sẽ coi Server là một nhà hàng, nhà hàng này chỉ có một anh nhân viên phục vụ (một thread đang chạy), còn Client là Khách hàng, cô này tới nhà hàng để ăn tối.

Với cách xử lý trong ví dụ ở trên thì anh nhân viên của nhà hàng chỉ có thể phục vụ cho một cô khách hàng 🤫, mà cô này chỉ gọi được 1 món, sau khi bán hàng cho 1 người thì nhà hàng đóng cửa (Bạn chạy Server, bạn chạy Client, Client gọi Server và gửi 1 (và chỉ 1) phép tính, Server phục vụ cho Client đúng chỉ 1 phép tính đó (và chỉ 1 client đó), rồi hai bên đóng kết nối và tắt ứng dụng).
Chúng ta không muốn như thế chút nào phải không ạ, chúng ta - những chủ nhà hàng, chúng ta là tư bản mà, muốn nhân viên phải phục vụ được càng nhiều người càng tốt, mà một người phải gọi được nhiều món lại càng tốt. Thế nên ta cần phải cải tiến ba việc:
Thứ nhất là nhà hàng phải mở cửa liên tục
Thứ hai là một người có thể gọi được nhiều món
Thứ ba là một nhân viên phục vụ được nhiều người.
Let's do it.
Nhà hàng mở cửa liên tục
Để giải quyết vấn đề nhà hàng mở cửa liên tục, chúng ta phải chắc rằng sau khi phục vụ xong 1 người, nhà hàng sẽ không đóng cửa ngay (Server tắt ngay). Cách giải quyết đơn giản là chúng ta thêm vòng lặp, để khi nhà hàng phục vụ xong một người, nó sẽ chuyển sang phục vụ người khác.
public class Server {
    public static void main(String[] args) throws IOException {
		...
		while (true) { // thêm while vào đây nè
		    Socket clientSocket = serverSocket.accept();
		    ...
		    writer.println();
		    writer.flush();
		}
		...
    }
}
Đến đây lại nảy sinh một chuyện, rõ là anh chàng phục vụ nhà hàng này được đào tạo từ lớp Java IO. Lớp này có một tật là khi đã phục vụ một người, anh ta phải phục vụ xong người đó thì mới chuyển sang phục vụ người khác. Điều này là do cơ chế Blocking của Java IO, khi đọc/ghi dữ liệu, Java IO sẽ chờ cho tới khi có dữ liệu để đọc hoặc ghi thì mới đi tiếp (Việc này quan trọng lắm nha, các bạn nhớ kỹ giúp mình nhá)

Điều này tạo ra một cơ chế hàng đợi cho cho connection, vấn đề càng lớn khi có một vị khách order đồ lâu, dẫn tới những người ở sau phải đợi rất lâu.

Dưới đây là ví dụ một ông khách mất 10 giây để order
public class Client {
    public static void main(String[] args) throws Exception {
				...
        String userInput = consoleReader.readLine();
        writer.println(userInput);
        Thread.sleep(10_000); // ngủ 10 giây nhé
        writer.flush();
        ...
    }
}

Để giải quyết vấn đề này chúng ta có thể thêm người phục vụ. Lý tưởng nhất là cứ mỗi khách vào nhà hàng, sẽ có một người phục vụ khách đó, như thế sẽ không ai phải đợi ai nữa.
public class Server {
    public static void main(String[] args) throws IOException {
		...
		while (true) {
		      Socket clientSocket = serverSocket.accept();
		      new Thread(() -> { // mỗi ông đến là có một người phục vụ riêng :)))
				...
			      writer.println();
			      writer.flush();
		      }.start();
		}
		...
    }
}
Cứ mỗi connection tới, chúng ta sẽ tạo ra một Thread mới để xử lý. Việc này sẽ giải quyết được vấn đề block của thread khiến các client khác không kết nối được.

Khách hàng gọi được nhiều món
Vấn đề này thì dễ dàng giải quyết hơn, chúng ta có thể sử dụng vòng lặp. Nhưng nếu sử dụng vòng lặp thì ""lại"" nảy sinh một vấn đề khác, lúc nào thì khách hàng đã gọi hết món mong muốn? (Hay nói cách khác lúc nào thì đóng kết nối?) Với bài toán bình phương trên chúng ta có thể quy ước là nếu User mà nhập số 0 thì là gọi hết món.
Tổng kết lại chúng ta có code Client và Server như sau, code hơi dài, mình cho vào đây luôn để các bạn tiện copy vào máy chạy để hiểu thêm nha.
import java.io.*;
import java.net.*;

public class Client {
    public static void main(String[] args) throws IOException {
        Socket socket = null;
        try {
            socket = new Socket();
            InetAddress localhost = InetAddress.getByName(""localhost"");
            InetSocketAddress inetSocketAddress = new InetSocketAddress(localhost, 8080);

            socket.connect(inetSocketAddress);
            System.out.println(""Client info: "" + socket);
            // Client info: Socket[addr=localhost/127.0.0.1,port=8080,localport=57884]

            InputStream in = socket.getInputStream();
            OutputStream out = socket.getOutputStream();

            BufferedReader reader = new BufferedReader(new InputStreamReader(in));
            PrintWriter writer = new PrintWriter(out);

            BufferedReader consoleReader = new BufferedReader(new InputStreamReader(System.in));

            while (true) { // cho phép gọi nhiều món
                System.out.print(""Enter a number (0 to exit): "");
                String userInputStr = consoleReader.readLine();
                if (userInputStr.equals(""0"")) {
                    System.out.println(""Close connection"");
                    break;
                }
                writer.println(userInputStr);
                writer.flush();

                String response = reader.readLine();
                System.out.println(""Server response: "" + response);
            }

        } finally {
            if (socket != null) {
                socket.close();
            }
        }
    }
}
import java.io.*;
import java.net.*;

public class Server {
    public static void main(String[] args) throws IOException {
		    System.out.println(""Thread: "" + Thread.currentThread().getName());
        ServerSocket serverSocket = null;
        try {
            serverSocket = new ServerSocket();
            InetAddress inetAddress = InetAddress.getByName(""0.0.0.0"");
            SocketAddress socketAddress = new InetSocketAddress(inetAddress, 8080);
            serverSocket.bind(socketAddress);
            while (true) { // phục vụ được nhiều khách
                Socket clientSocket = serverSocket.accept();

                new Thread(() -> { // mỗi khách một người phục vụ riêng
                    try {
		                System.out.println(""Thread: "" + Thread.currentThread().getName());
                        System.out.println(""Connected to client: "" + clientSocket);
                        // Connected to client: Socket[addr=/127.0.0.1,port=57884,localport=8080]

                        InputStream in = clientSocket.getInputStream();
                        OutputStream out = clientSocket.getOutputStream();

                        BufferedReader reader = new BufferedReader(new InputStreamReader(in));
                        PrintWriter writer = new PrintWriter(out);

                        String clientInput;
                        while ((clientInput = reader.readLine()) != null) { // Cho phép gọi nhiều món
                            int number = Integer.parseInt(clientInput);
                            System.out.println(""client input: "" + clientInput);
                            if (number == 0) {
                                System.out.println(""Client requested to close connection."");
                                break;
                            }

                            int square = number * number;
                            writer.printf(""%d square is %d"", number, square);
                            writer.println();
                            writer.flush();
                        }
                    } catch (IOException e) {
                        e.printStackTrace();
                    }
                }).start();
            }
        } finally {
            if (serverSocket != null) {
                serverSocket.close();
            }
        }
    }
}
Nhân viên phục vụ được nhiều người
Để làm việc này thì chúng ta có thể sử dụng Pool thay vì cứ mỗi connection tới ta tạo ra một thread mới. Còn một cách nữa, cách này sẽ nói ở bài tiếp theo, bí mật bí mật.
Code đoạn pool này sẽ là bài tập về nhà của các bạn nha.
Bài tập về nhà:
Bài 1: Các bạn hãy copy thành nhiều Client khác nhau, chạy thử với Thread.sleep() để thấy sự khác biệt nhé, chú ý các thông tin Thread được tạo ra mỗi connection. Các bạn nên làm bài tập về nhà, các bạn sẽ hiểu thêm rất nhiều thay vì chỉ đọc thôi, học phải đi đôi với hành nè.
Bài 2: Hãy tự viết Server và Client cho riêng mình nhé. Trong đó Server thay vì luôn tạo Thread mới sẽ sử dụng pool thay thế.
Đến đây là bạn đã hiểu về Blocking Server rồi phải không nào? Tiếp theo chúng ta hãy cùng tìm hiểu về sự ưu - nhược của blocking nhé
Ưu
Dễ hiểu: cơ chế này khá dễ hiểu, một anh bồi bàn phục vụ hết một người, sau đó chuyển sang người khác, cứ thế mà làm.
Dễ bảo trì, dễ code: Cơ chế của Java IO khá là đơn giản, straightforward nên dễ dàng bảo trì và code cũng dễ nữa.
Nhược
Tốn tài nguyên: Dễ thấy cứ mỗi một khách hàng tới thì lại cần một phục vụ mới ra đón khách. Quả là tốn tiền thuê nhân viên.
Khó mở rộng: Tại vì sử dụng mỗi thread một kết nối tới cho nên khó lòng mở rộng nếu có nhiều connection. Ví dụ nếu lượng connection tăng là 1 triệu thì phải làm thế nào?
Blocking Server khá là dễ hiểu, dễ code và bảo trì. Nó là sự kết hợp của Java Networking và Java IO để có thể đọc ghi dữ liệu trong Socket. Chúng ta cũng đã đi qua về việc cải tiến Server để có thể có một Server tốt hơn, phục vụ được nhiều người cùng một lúc và có thể duy trì kết nối. Hi vọng sau bài này các bạn có một góc nhìn mới về Server.
P/S: Tại sao mình viết những bài thế này? Tại vì khi tìm hiểu trên mạng về Server, WebServer,... thì ra những bài viết rất chung chung, nó không làm thỏa mãn được mình. Nên mình tìm hiểu và viết lại trong bài này hi vọng các bạn hiểu hơn về Server, chúng ta có thể code được một Server đơn giản, hiểu được cơ chế của nó, nó không có gì quá cao siêu, ngoài tầm với và quá khó hiểu đúng không.
Link bài viết gốc https://truongphuoc.wordpress.com/2024/07/18/javanet-p3-blocking-server/",https://viblo.asia/p/javanet-p3-blocking-server-0gdJzD0nVz5
53,(EzyPlatform) Cách đóng gói dự án và đẩy dự án lên chợ,"Giới thiệu:
Ezy Platform là 1 nền tảng cộng đồng đông đảo người dùng với các plugin tiện ích được cài đặt dễ dàng để thêm vào dự án của người sử dụng, trong bài viết này chúng ta sẽ tìm hiểu về cách đóng gói dự án và đẩy lên chợ của Ezy Platform
B1: Đóng gói dự án
Đầu tiên, để đóng gói dự án chúng ta tìm tới thư mục dự án của bạn và mở CMD sau đó chạy lệnh export.bat như sau:
export.bat

để tăng version cho dự án, sử dụng lệnh: ezy.bat sv +1
ezy.bat sv +1

Sau khi hoàn thành đóng gói dự án thành công file sẽ được lưu tại thư mục target của dự án

B2: Đẩy dự án lên chợ Ezy Platform
B1: Truy cập trang chủ Ezy Platform và đăng ký tài khoản
B2: Sau khi đăng ký thành công truy cập phần dự án và chọn tạo mới

B3: Nhập các thông tin dự án của bạn, đối với hình ảnh phải đúng độ phân giải yêu cầu

B4: Sau khi hoàn thành tạo dự án, tiếp tới chúng ta chọn phần create fist version

B5: Nhập thông tin phiên bản và chọn file đóng gói dự án ở phần đầu bài viết (Lưu ý, tên file phải trùng với tên dự án).

B6: Sau khi đẩy dự án thành công cần chờ kết quả phê duyệt của Admin.

Khi có kết quả chúng ta có thể phát hành dự án

Sau khi hoàn thành các bước trên chúng ta đã hoàn thành đẩy dự án lên chợ của Ezy PlatForm, Các bạn có thể xem dự án của mình tại phần market",https://viblo.asia/p/ezyplatform-cach-dong-goi-du-an-va-day-du-an-len-cho-AZoJjrqeJY7
54,SỬ DỤNG AI ĐỂ PHÁT HUY SỨC MẠNH CỦA BUSINESS ANALYSIS,"Vince Mirabelli - một chuyên gia nổi tiếng trong lĩnh vực Quản lý quy trình và cải tiến liên tục đã từng cho rằng : “ Một Business Analyst biết cách tận dụng AI sẽ thay thế một BA không biết”.
Trong một thế giới nơi ngành Business Analysis (Phân tích nghiệp vụ) đóng vai trò như một ngọn hải đăng soi đường cho những chiến lược insight và giải quyết vấn đề, các chuyên gia trong lĩnh vực này chính là những người thay đổi cuộc chơi thực sự. Được trang bị kiến thức chuyên môn về các lĩnh vực khác nhau, kỹ năng tư duy phản biện và khả năng phát hiện ra những cơ hội tiềm ẩn, họ là lực lượng thúc đẩy cho việc ra quyết định sáng suốt và thành công của tổ chức.
Bây giờ hãy tưởng tượng nếu chúng ta có thể khuếch đại những sức mạnh đó-nâng cao khả năng trích xuất thông tin, tinh giản quy trình, và thúc đẩy đổi mới vượt xa hơn bao giờ hết. Vâng, điều đó không còn là khoa học viễn tưởng nữa (dù sẽ khá tuyệt nếu thấy các Business Analyst mặc áo choàng).
Sức mạnh của AI trong Business Analysis
Đó là bởi vì trí tuệ nhân tạo (AI) đang chuẩn bị cách mạng hóa cách chúng ta tiếp cận Business Analysis.
Bằng cách tận dụng sức mạnh của các công cụ AI, các chuyên gia phân tích kinh doanh có thể nâng cao năng lực của mình, nâng cao tầm ảnh hưởng và mở ra một kỷ nguyên mới của sự xuất sắc về chiến lược. Trong tập mới nhất của Business Analysis Live, Susan và Scott đã cùng Vince Mirabelli khám phá cách AI có thể nâng cao công việc của họ, và tại sao kỹ thuật gợi ý (prompt engineering) là chìa khóa để khai thác toàn bộ tiềm năng của nó.
Business Analysis Live - một podcast nói về phân tích kinh doanh được tổ chức bởi IIBA
Sau đây là 6 điều được rút ra từ podcast:
1. AI như một trợ lý thực tập
Các chuyên gia Business Analysis thường đảm đương nhiều trách nhiệm cùng lúc - thu thập yêu cầu, phân tích dữ liệu và đảm bảo sự thống nhất giữa các bên liên quan,... đây chỉ là một vài ví dụ. AI có thể giúp các Business Analyst tự động hóa các nhiệm vụ lặp đi lặp lại và tốn thời gian, giúp giải phóng thời gian quý báu để họ có thể tương tác nhiều hơn với các stakeholders (các bên liên quan).
AI được xem như một trợ lý thực tập của các Business Analyst
Mặc dù AI có thể tạo ra một số thông tin hữu ích, kết quả cần được xem xét và xác minh. “Hãy nghĩ về nó như việc có một thực tập sinh làm việc cho bạn,"" Scott nói. ""Bạn giao cho thực tập sinh những công việc ít phức tạp hơn, nhưng bạn biết rằng bạn phải kiểm tra công việc của họ để đảm bảo nó chính xác.""
2. AI và Business Analyst là một sự kết hợp lí tưởng
Các chuyên gia về Business Analysis đảm nhận nhiều vai trò. Họ là những thám tử khám phá các yêu cầu ẩn giấu, là những dịch giả kết nối giữa kinh doanh và công nghệ, hoặc là những người kể chuyện truyền đạt các ý tưởng phức tạp. Đây chỉ là một vài trong số nhiều ví dụ về những vai trò mà một Business Analyst phải làm.
Bằng cách cung cấp những thông tin chi tiết dựa trên dữ liệu, AI bổ sung và hoàn thiện cho tất cả những vai trò trên. Mặc dù vậy, chính kiến thức chuyên môn và tư duy phản biện của nhà phân tích mới là yếu tố bổ sung vào các yếu tố con người cần thiết. Khả năng hiểu được các nhu cầu kinh doanh phức tạp và diễn giải kết quả phân tích từ AI của họ đảm bảo rằng các tổ chức đưa ra các quyết định sáng suốt và đạt được những kết quả có tác động.
3. Business Analyst là “Người gác cổng” chất lượng
Để tận dụng hiệu quả Trí Tuệ Nhân Tạo (AI), các chuyên gia về Business Analysis cần phát triển các kỹ năng cụ thể, chẳng hạn như xây dựng những câu hỏi hiệu quả và diễn giải đầu ra.
Hãy hình dung tương tác với AI giống như một buổi phỏng vấn. Những câu hỏi chi tiết, giàu bối cảnh sẽ mang lại kết quả tốt hơn. Tránh đặt những câu hỏi mơ hồ giống như tìm kiếm trên Google. Thay vào đó, hãy cung cấp thông tin chi tiết để hướng dẫn các công cụ AI. Xét cho cùng, chất lượng của AI phụ thuộc vào dữ liệu chúng ta cung cấp.
Mặc dù AI có thể tạo ra câu trả lời, nhưng các chuyên gia Business Analysis vẫn cần xác nhận chúng. Hãy trở thành “Người gác cổng chất lượng” bằng cách hiểu những sắc thái, hạn chế và thiên kiến của các hệ thống AI.
4. Nghệ thuật đặt câu hỏi
Hãy tưởng tượng bạn viết yêu cầu công việc mà không có chi tiết - nó sẽ là một mớ hỗn độn! Điều tương tự cũng xảy ra khi yêu cầu AI thực hiện bất kỳ nhiệm vụ nào. Những câu hỏi mơ hồ dẫn đến phản hồi của AI không tối ưu. Chúng ta hãy cùng phân tích kỹ thuật xây dựng câu hỏi (prompt engineering):
Giống như hầu hết mọi thứ trong cuộc sống, bối cảnh đóng vai trò rất quan trọng. Khi tương tác với AI, bối cảnh là yếu tố then chốt. Hãy mô tả vấn đề, dữ liệu liên quan và kết quả mong muốn. Ví dụ, thay vì hỏi: ""Thời tiết hôm nay thế nào?"", hãy cụ thể hóa chúng: ""Cung cấp dự báo thời tiết cuối tuần cho Istanbul, Thổ Nhĩ Kỳ.""
Và đừng quên các chi tiết, yếu tố cuối cùng thúc đẩy tính chính xác. Giống như một yêu cầu được xây dựng tốt, một câu hỏi chi tiết sẽ mang lại kết quả tốt hơn. Ví dụ, nếu bạn tìm kiếm các đề xuất về giày, đừng chỉ hỏi ""giày dép"". Hãy nêu rõ dịp sử dụng, ngân sách và sở thích về kiểu dáng.
AI không tự hoạt động. Thay vào đó, nó là một công cụ do con người kiểm soát (ít nhất là cho đến nay), và các chuyên gia Business Analysis vẫn giữ vai trò chủ đạo. Trong khi AI trả lời các câu hỏi, công việc của bạn là đánh giá, xác nhận và đưa ra quyết định sáng suốt dựa trên những gì AI tạo ra.
5. Định kiến và đạo đức
Trí tuệ Nhân tạo (AI) học hỏi từ dữ liệu, và dữ liệu thì có thể chứa đựng định kiến. Điều đó có nghĩa là các chuyên gia phân tích kinh doanh cần phải cảnh giác bằng cách nhận ra sự định kiến khi nó xuất hiện và đưa ra các cân nhắc về đạo đức khi sử dụng AI.
Hãy nhớ rằng AI thừa hưởng các định kiến từ dữ liệu được huấn luyện. Cho dù cố ý hay vô tình, bạn có trách nhiệm sửa chữa chúng. Quyết định của AI ảnh hưởng đến cuộc sống, vì vậy hãy luôn hành động với sự công bằng, minh bạch và trách nhiệm. Các chuyên gia Business Analysis là la bàn đạo đức định hướng cho quá trình hoạt động của AI.
6. Lời khuyên của Vince
Còn nhớ Vince, khách mời podcast của chúng ta chứ? Anh ấy nhấn mạnh rằng các chuyên gia Business Analysis vẫn là người nắm giữ tay lái. Đây quả là một phép ẩn dụ hoàn hảo để mô tả mối quan hệ của chúng ta với công nghệ đáng kinh ngạc này.
Cuộc trò chuyện giữa Susan, Scott và Vince
Mặc dù AI là một cơ phó mạnh mẽ, bạn vẫn là cơ trưởng. Là một chuyên gia phân tích kinh doanh, hãy sử dụng AI để định hướng nhưng hãy luôn giữ tay chắc chắn trên các nút điều khiển. Với chuyên môn của bạn dẫn đường, AI sẽ trở thành một công cụ hữu giá trong kho vũ khí của bạn, giúp bạn lái doanh nghiệp hướng đến thành công.
7. Kết luận
AI là một cộng sự tuyệt vời. Bằng cách nắm vững kỹ thuật xây dựng câu hỏi (prompt engineering), hiểu biết về định kiến và duy trì cảnh giác về mặt đạo đức, bạn có thể khai thác những sức mạnh tiềm ẩn của AI. Vì vậy, hãy cùng chào đón AI, học ngôn ngữ của nó và vươn tới những tầm cao mới!
Hy vọng những thông tin được tổng hợp trong bài sẽ giúp bạn hiểu thêm được tầm quan trọng và sức mạnh của AI trong ngành Business Analysis. Đừng quên đón xem các bài viết mới nhất sẽ được cập nhật thường xuyên tại BAC's Blog
Unlocking the Power of Business Analysis with AI
",https://viblo.asia/p/su-dung-ai-de-phat-huy-suc-manh-cua-business-analysis-E1XVObN8LMz
55,Banking Infrastructure on Cloud,"Loạt bài viết ngắn chia sẻ về kiến trúc hạ tầng của hệ thống ngân hàng trên Cloud (AWS). Đây chỉ là những kiến thức mình học được thông qua sản phẩm ngân hàng số của bên mình.
Banking Infrastructure on Cloud
AWS Account Management
Provisioning Infrastructure for Multi AWS Accounts
Networking for Multi AWS Accounts
Kubernetes for Multi AWS Accounts: Kubernetes Infrastructure for Scale
Kubernetes for Multi AWS Accounts: Kubernetes Cross Cluster Comunication
Chaos Engineering
Core Banking on Cloud
Security Consider
Mình thì cũng chả phải dân chuyên gì trong ngành này, chỉ là thấy hay nên chia sẻ những gì mình học được trong quá trình tiếp nhận hệ thống, bảo trì, nâng cấp và phát triển thêm tính năng với sự giúp đỡ của những đồng đội rất giỏi và tài năng, cộng với những anh síp cực tốt.
AWS Account Management
Để triển khai hạ tầng ngân hàng trên AWS thì công việc đầu tiên ta cần làm đó quyết định cách tổ chức và quản lý tài khoản AWS.  Ta sử dụng một tài khoản hay nhiều tài khoản AWS? Nếu nhiều tài khoản thì tạo tài khoản theo tiêu chí gì? Làm thế nào để quản lý nhiều tài khoản?

Tạo một hay nhiều tài khoản
Theo gợi ý của AWS thì đối với các doanh nghiệp có quy mô lớn thì ta nên triển khai hệ thống theo kiểu Landing Zone với nhiều tài khoản AWS khác nhau. Mục đích của việc tạo nhiều tài khoản là để tránh bị giới hạn AWS Service Quotas, tránh bị ảnh hưởng toàn bộ nếu có một tài khoản nào đó bị hack và dễ dàng quản lý AWS Billing. Với hệ thống banking thì ta nên chọn triển khai hạ tầng trên nhiều tài khoản AWS.
Tạo tài khoản theo tiêu chí gì
Tiêu chí phổ biến để tạo tài khoản là dựa theo môi trường và những thành phần phổ biến trong một hệ thống phần mền.
Ví dụ khi phát triển sản phẩm thì ta có môi trường là DEV, UAT, STAGING, PROD. Đối với các môi trường như DEV, UAT, STAGING có thể gôm lại thành môi trường nonprod. Còn những thành phần phổ biến trong một hệ thống thì bao gồm Networking, Workload, Operation (CI/CD), Monitoring, Logging, Data System.

Vậy ta có thể tạo các tài khoản với tên như sau (nonprod dùng cho các môi trường không phải production):
networking-nonprod
workload-nonprod
operation-nonprod
observability-nonprod
data-nonprod
networking-prod
workload-prod
operation-prod
observability-prod
data-prod
Networking dùng để quản lý network ra vào, tất cả các request đi vào và ra đều phải đi qua networking account trước rồi mới đi tiếp. Mục đích là để ta truy vết được toàn bộ request đi vào ra hệ thống.
Workload account dùng để triển khai ứng dụng, database, cache.
Operation account dùng để thực thi các tác vụ liên quan tới CI/CD, tạo hạ tầng cho các tài khoản khác.
Observability account dùng để triển khai hệ thống monitoring, logging và tracing.
Data account dùng để thực thi các tác vụ về dữ liệu như thu thập, xử lý dữ liệu và hiển thị dữ liệu đẹp cho người dùng nội bộ.
Quản lý nhiều tài khoản
Khi tạo nhiều tài khoản như trên thì ta quản lý như thế nào về mặt AWS Billing và truy cập. Ta cần một một tài khoản nữa với mục đích là để quản lý toàn bộ tài khoản trên, có thể đặt tên tài khoản này là root. Để tài khoản root có thể quản lý được các tài khoản trên thì ta cần sử dụng AWS Organizations kết hợp với AWS Control Tower.

Vấn đề tiếp theo là về việc truy cập các tài khoản khác nhau. Khi ta có nhiều tài khoản AWS như trên thì khi truy cập ta cần làm thế nào để tiện nhất? Ta không thể dùng Console thông thường rồi login và logut để truy cập từng tài khoản được. Bên cạnh đó còn vấn đề về IAM User và Premission, nếu có một bạn cần truy cập nhiều tài khoản thì không lẻ ta phải vào từng tài khoản để tạo IAM User cho bạn? Để dễ dàng truy cập các tài khoản trong Control Tower thì AWS hỗ trợ dịch vụ IAM Identity Center. Ta chỉ cần tạo quyền và user ở một nơi và họ có thể truy cập được các tài khoản khác nhau thông qua IAM Identity Center, hình minh họa.

Bài tiếp theo mình sẽ nói về cách sử dụng Operation Account để tạo hạ tầng cho các tài khoản khác.
Điểm hẹn ngân hàng số Vikki by HDBank",https://viblo.asia/p/banking-infrastructure-on-cloud-aAY4q7KQLPw
56,(EzyPlatform) hướng dẫn phân trang lấy ra giới hạn số lượng bài viết trong dự án Web Blog,"Giới thiệu về PaginationModel và chức năng
1. PaginationModel:
Đầu tiên để hiểu về cách phân trang trong EzyPlatform chúng ta có Class PaginationModel với các thuộc tính sau:
EzyPlatform
PaginationModel

Trong đó:
PageToken chứa các dữ liệu về nút next và prev với kiểu dữ liệu là String
PageToken
next
prev
String
Continuation  có chứa 2 thuộc tính kiểm tra hasNext và hasPrev với kiểu dữ liệu là Boolean nhằm kiểm tra xem điểm đầu và điểm cuối của dữ liệu
Continuation
hasNext
hasPrev
Boolean
2. PaginationPostService
Trong bộ thư viện Ezyarticle chúng ta có các package service được cấu hình sẵn các phương thức hỗ trợ cho việc sử dụng dễ dàng hơn.
Ezyarticle
Ở đây khi làm việc với các bài Post ta có Class PaginationPostService sử dụng cho việc phân trang các bài Post:
PaginationPostService

ta có thể thấy Class này được extends tới Abstract Class CommonPaginationService đây là 1 class được sử dụng chung cho việc phân trang cho các Entity trong dự án của bạn, Class này nằm trong thư mục ezyplatform.service các bạn có thể tìm hiểu thêm tại đường dẫn:
CommonPaginationService
package org.youngmonkeys.ezyplatform.service;
3. PaginationModelFetchers
Class này được sử dụng cho việc lấy dữ liệu phân trang, nó cung cấp các phương thức để lấy trang đầu tiên, trang cuối, trang tiếp theo và trang trước dựa trên các tham số như PaginationService, filter, nextPageToken, prevPageToken, thông tin là trang cuối (lastPage) và giới hạn số phần tử trên trang (limit).
Class này nằm trong package:
package org.youngmonkeys.ezyplatform.pagination;

4.CommonValidator
Class này sử dụng các phương thức để thực hiện các kiểm tra và xử lý lỗi cho các tham số đầu vào trong Web
Class này nằm trong package:
package org.youngmonkeys.ezyplatform.validator;

Cách hoạt động:
Class CommonValidator có các phương thức chủ yếu để kiểm tra tính hợp lệ của các đầu vào từ người dùng.
CommonValidator
Nếu một điều kiện không được đáp ứng (ví dụ: pageSize không hợp lệ, kích thước Collection quá lớn, v.v.), nó sẽ ném ra một HttpBadRequestException với một thông báo lỗi phù hợp.
Sử dụng Collections.singletonMap để chỉ định lỗi cụ thể (ví dụ: ""pageSize"": ""invalid"").
Cách cài đặt
B1: đầu tiên chúng ta tạo ra Class Response chứa các thông tin Model cần gửi tới Font-End theo ví dụ sau:
Response
Trong Class này mình chỉ lấy ra các nội dung chính của bài viết như: tiêu đề, nội dung, hình ảnh..
B2: tiếp theo tại Class Service của bạn hãy thêm các thông tin sau:
Service
@Service
@AllArgsConstructor
public class WebBlogPostControllerService {

    private final WebPostFilterFactory postFilterFactory;
    private final WebPaginationPostService paginationPostService;
    private final WebBlogPostModelDecorator blogPostModelDecorator;

    public PaginationModel<WebBlogPostResponse> getBlogPostPagination(
        String keyword,
        String nextPageToken,
        String prevPageToken,
        boolean lastPage,
        int limit
    ) {
        PaginationModel<PostModel> pagination = PaginationModelFetchers.getPaginationModel(
            this.paginationPostService,
            postFilterFactory.newDefaultPostFilterBuilder(keyword)
                .postStatus(PostStatus.PUBLISHED.toString())
                .build(),
            nextPageToken,
            prevPageToken,
            lastPage,
            limit
        );
        return blogPostModelDecorator.decorateBlogPostPagination(
            pagination
        );
    }
Chúng ta có thể thấy tại phương thức getBlogPostPagination được tạo ra với nhiệm vụ lấy danh sách Blog để phân trang dựa trên các tham số được truyền vào và sau khi có được pagination, phương thức này sẽ gửi dữ liệu tới Class Decorator để tiến hành bước tiếp theo
getBlogPostPagination
B3: sau khi có dữ liệu, chúng ta tiếp tục tạo ra Class Decorator với các thông tin như sau:
Decorator
@EzySingleton
@AllArgsConstructor
public class WebBlogPostModelDecorator {

    private final WebMediaService mediaService;
    private final WebPostSlugService postSlugService;
    private final WebBlogModelToResponseConverter blogModelToResponseConverter;

    public PaginationModel<WebBlogPostResponse> decorateBlogPostPagination(
        PaginationModel<PostModel> pagination
    ) {
        List<PostModel> models = pagination.getItems();
        List<Long> postIds = newArrayList(
            models,
            PostModel::getId
        );
        Map<Long, String> slugByPostId = postSlugService.getLatestSlugMapByPostIds(
            postIds
        );
        Set<Long> imageIds = newHashSet(
            models,
            PostModel::getFeaturedImageId
        );
        Map<Long, MediaNameModel> imageById = mediaService
            .getMediaNameMapByIds(imageIds);
        return pagination.map(it ->
            blogModelToResponseConverter.toBlogPostResponse(
                it,
                slugByPostId.get(it.getId()),
                imageById.get(it.getFeaturedImageId())
            )
        );
    }
}
Class này có nhiệm vụ lấy ra các thông tin được gửi tới từ phương thức getBlogPostPagination trước đó và tiến hành tìm kiếm các thông tin mà phương thức blogModelToResponseConverter.toBlogPostResponse (được khởi tạo ở bước 1) yêu cầu
getBlogPostPagination
blogModelToResponseConverter.toBlogPostResponse
B4: Cuối cùng tại Controller chúng ta truyền tham số và gọi tới Service như sau:
@Controller
@AllArgsConstructor
public class HomeController {

    private final WebBlogPostControllerService blogPostControllerService;
    private final WebCommonValidator commonValidator;

    @DoGet(""/"")
    public View home(
        @RequestParam(value = ""keyword"") String keyword,
        @RequestParam(value = ""nextPageToken"") String nextPageToken,
        @RequestParam(value = ""prevPageToken"") String prevPageToken,
        @RequestParam(value = ""lastPage"") boolean lastPage,
        @RequestParam(value = ""limit"", defaultValue = ""2"") int limit
    ) {
        commonValidator.validatePageSize(limit);
        return View.builder()
            .template(""home"")
            .addVariable(
                ""postPagination"",
                blogPostControllerService.getBlogPostPagination(
                    keyword,
                    nextPageToken,
                    prevPageToken,
                    lastPage,
                    limit
                )
            )
            .addVariable(VIEW_VARIABLE_PAGE_TITLE, ""home"")
            .build();
    }
Tại controller, ta sử dụng CommonValidator sẽ kiểm tra xem các giá trị pageSize có hợp lệ hay không, nếu các giá trị hợp lệ chúng ta sẽ gửi Variable tới giao diện như sau:
CommonValidator
.addVariable(
   ""postPagination"",
   blogPostControllerService.getBlogPostPagination(
       keyword,
       nextPageToken,
       prevPageToken,
       lastPage,
       limit
   )
Tóm tắt: Bài viết này  giới thiệu và hướng dẫn cách phân trang bằng EzyPlatForm, bạn có thể tùy biến và sử dụng cho dự án của mình",https://viblo.asia/p/ezyplatform-huong-dan-phan-trang-lay-ra-gioi-han-so-luong-bai-viet-trong-du-an-web-blog-zOQJwoRdJMP
57,Bạn đã biết gì về prompt engineering? (P4) - Học được gì từ chiến lược thiết kế prompt của các kỹ sư Google?,"Trong API docs của Google cung cấp cho mô hình Gemini, có mục Prompt design strategies, tức một số chiến lược chúng ta có thể áp dụng khi thiết kế prompt cho mô hình ngôn ngữ lớn. Tuy đây là tài liệu dành cho Gemini nhưng mình thấy nó hoàn toàn có thể áp dụng cho các LLM khác. Hãy cùng xem các kỹ sư Google có cho chúng ta biết thêm điều gì hay ho mới mẻ về prompt engineering không nhé:
Đưa ra chỉ dẫn cho mô hình về những việc cần làm là một cách đơn giản và hiệu quả để tùy chỉnh hành vi của mô hình. Hãy đảm bảo rằng các chỉ dẫn đưa ra là rõ ràng và cụ thể.
Xác định nhiệm vụ cần thực hiện: Mô tả chi tiết nhiệm vụ mà bạn muốn mô hình thực hiện.
Đưa ra các ràng buộc: Cho mô hình biết những gì nên làm và không nên làm.
Xác định định dạng của phản hồi: Đưa ra hướng dẫn chỉ định định dạng (format) của phản hồi. Ví dụ: bạn có thể yêu cầu mô hình đưa ra phản hồi dưới dạng bảng, dạng json, danh sách gạch đầu dòng, danh sách từ khóa, câu hoặc đoạn văn.
Bạn có thể đưa vào prompt các ví dụ về các phản hồi đúng. Từ các ví dụ, mô hình sẽ cố gắng xác định các pattern và mối quan hệ để áp dụng khi tạo phản hồi. Few-shot prompting thường được sử dụng để điều chỉnh định dạng, cách diễn đạt, phạm vi hoặc khuôn mẫu chung của phản hồi. Hãy sử dụng các ví dụ cụ thể và đa dạng để giúp mô hình xác định trọng tâm và tạo ra kết quả chính xác hơn.
Zero-shot prompt có thể sẽ kém hiệu quả hơn few-shot vì các ví dụ giúp cho mô hình biết cách áp dụng chỉ dẫn. Trên thực tế, nếu ví dụ đủ rõ ràng và đủ tốt, thậm chí bạn có thể không cần cung cấp chỉ dẫn cho mô hình.
Tìm số lượng ví dụ tối ưu: Bạn có thể thử nghiệm với số lượng ví dụ cung cấp trong lời nhắc để có kết quả đúng như mong muốn nhất. Các mô hình như PaLM và Gemini thường có thể nắm được các pattern bằng cách sử dụng một vài ví dụ, các mô hình đơn giản hơn như BERT có thể cần nhiều hơn. Tuy nhiên, nếu đưa vào quá nhiều ví dụ có thể dẫn đến overfit.
Sử dụng các ví dụ để cho mô hình thấy những gì cần làm theo sẽ hiệu quả hơn so với việc chỉ cho mô hình những gì cần tránh.
Cung cấp định dạng nhất quán cho các ví dụ: Đảm bảo rằng cấu trúc và định dạng của các ví dụ giống nhau để tránh phản hồi có định dạng không mong muốn, hãy đặc biệt chú ý đến thẻ XML, khoảng trắng, xuống dòng, vv.
Bạn có thể đưa vào prompt các hướng dẫn và thông tin mà mô hình cần để giải quyết vấn đề thay vì cho rằng mô hình có tất cả thông tin cần thiết. Thông tin theo ngữ cảnh này giúp mô hình hiểu được các ràng buộc và chi tiết về những gì bạn yêu cầu nó thực hiện.
Tiền tố là một từ hoặc cụm từ mà bạn thêm vào nội dung prompt để phục vụ một số mục đích, tùy thuộc vào vị trí bạn đặt tiền tố:
Tiền tố đầu vào: được thêm vào đầu vào giúp chỉ ra các phần quan trọng về mặt ngữ nghĩa của đầu vào cho mô hình. Ví dụ: tiền tố ""tiếng Anh:"" và ""tiếng Pháp:"" phân định hai ngôn ngữ khác nhau.
Tiền tố đầu ra: Mặc dù đầu ra do mô hình tạo ra nhưng bạn có thể thêm tiền tố cho đầu ra trong lời nhắc. Tiền tố đầu ra cung cấp cho mô hình thông tin về những gì được mong đợi dưới dạng phản hồi. Ví dụ: tiền tố đầu ra ""JSON:"" báo hiệu cho mô hình rằng đầu ra phải ở định dạng JSON.
Tiền tố ví dụ: Trong few-shot prompts, việc thêm tiền tố vào ví dụ sẽ cung cấp các nhãn mà mô hình có thể sử dụng khi tạo đầu ra, giúp xử lý (parsing) nội dung đầu ra dễ dàng hơn.
Trong ví dụ sau, ""Text:"" là tiền tố đầu vào và ""The answer is:"" là tiền tố đầu ra
Classify the text as one of the following categories.
- large
- small
Text: Rhino
The answer is: large
Text: Mouse
The answer is: small
Text: Snail![image.png](https://images.viblo.asia/8447d5b7-3d80-49e0-8814-5a2419ff979a.png
The answer is: small
Text: Elephant
The answer is:
Cơ chế hoạt động của các mô hình ngôn ngữ lớn là tự động hoàn thành đoạn text được đưa vào. Nếu bạn ""mớm"" cho mô hình một nội dung dang dở, mô hình sẽ trả về phần tiếp theo của đầu vào đó dựa trên mọi ví dụ hoặc ngữ cảnh có sẵn trong prompt. Đôi khi yêu cầu mô hình viết tiếp nội dung input lại dễ dàng hơn việc mô tả nhiệm vụ bằng ngôn ngữ tự nhiên.
Việc thêm một phần câu trả lời vào prompt có thể hướng dẫn mô hình tuân theo định dạng mong muốn.
Ví dụ hướng dẫn mô hình tạo dàn ý bài luận:
Prompt 1:
Create an outline for an essay about hummingbirds.
Ouptut:
I. Introduction
A. Hook: Begin with an intriguing fact or description of hummingbirds.
B. Provide some background information on hummingbirds.
1. Mention their small size and long, thin beaks.
2. Highlight their unique ability to hover and fly backward.
Prompt 1 không chỉ định format cho dàn ý nên mô hình đã chọn một format tùy ý. Để mô hình trả về dàn ý ở một định dạng cụ thể, bạn có thể thêm một đoạn văn bản mẫu cho phần bắt đầu của dàn ý và để mô hình hoàn thành tiếp phần còn lại.
Prompt 2:
Create an outline for an essay about hummingbirds.
I. Introduction
   *
Ouptut:
I. Introduction
* Capture the reader's attention with an interesting![image.png](https://images.viblo.asia/8447d5b7-3d80-49e0-8814-5a2419ff979a.png anecdote or fact about hummingbirds.
* Provide a brief background on hummingbirds, including their unique characteristics.
* State the main points that will be covered in the essay.
Đối với các trường hợp cần đến các prompt phức tạp, một cách tiếp cận là chia mọi thứ thành các thành phần đơn giản hơn.
Chia nhỏ chỉ dẫn: Thay vì có nhiều chỉ dẫn trong một prompt, hãy chia nhỏ thành một prompt cho mỗi chỉ dẫn và quyết định áp dụng prompt nào dựa trên thông tin đầu vào của người dùng.
Xâu chuỗi prompt: Chia nhiều bước tuần tự thành các prompt riêng biệt và xâu chuỗi chúng sao cho đầu ra của prompt trước trở thành đầu vào của prompt sau.
Thực hiện các tác vụ song song khác nhau trên các phần khác nhau của dữ liệu và tổng hợp các kết quả để cho ra kết quả cuối cùng. Ví dụ: bạn có thể yêu cầu mô hình thực hiện một thao tác trên phần đầu tiên của dữ liệu, thực hiện một thao tác khác trên phần còn lại của dữ liệu và tổng hợp kết quả.
Mỗi request mà bạn gửi đến một mô hình bao gồm các giá trị tham số giúp kiểm soát cách mô hình tạo ra phản hồi. Mô hình có thể tạo ra các kết quả khác nhau cho các giá trị tham số khác nhau. Thử nghiệm với các giá trị tham số khác nhau để có được kết quả tốt nhất cho nhiệm vụ. Một số loại tham số phổ biến:
Max output tokens: Số lượng token tối đa có thể được tạo trong phản hồi. 100 token tương ứng với khoảng 20 từ tiếng Anh.
Max output tokens
Temperature: kiểm soát mức độ ngẫu nhiên trong việc lựa chọn token. Temperature cao hơn có thể dẫn đến kết quả đa dạng hoặc sáng tạo hơn. Nhiệt độ bằng 0 mang tính tất định, nghĩa là phản hồi có xác suất cao nhất luôn được chọn. Đối với hầu hết các trường hợp, hãy thử bắt đầu với nhiệt độ 0,2. Nếu mô hình trả về phản hồi quá chung chung, quá ngắn hoặc mô hình đưa ra phản hồi dự phòng, hãy tăng dần lên.
Temperature
Top-K: giá trị thấp hơn cho các phản hồi ít ngẫu nhiên hơn và giá trị cao hơn cho các phản hồi ngẫu nhiên hơn. Top-K mặc định là 40.
Top-K
Top-P: giá trị thấp hơn cho các phản hồi ít ngẫu nhiên hơn và giá trị cao hơn cho các phản hồi ngẫu nhiên hơn. Top-P mặc định là 0,95.
Top-P

Thiết kế prompt là một quá trình lặp đi lặp lại, ít nhất là một vài lần trước khi bạn thu được một version có thể tạo ra phản hồi mong muốn một cách nhất quán. Một số điều bạn có thể thử:
Sử dụng các từ hoặc cách diễn đạt khác nhau trong prompt thường mang lại những phản hồi khác nhau từ mô hình mặc dù tất cả chúng đều có cùng một ý nghĩa. Nếu bạn không nhận được kết quả mong đợi từ prompt của mình, hãy thử diễn đạt lại nó. Nếu bạn không thể yêu cầu mô hình làm theo hướng dẫn của mình cho một nhiệm vụ, hãy thử đưa ra hướng dẫn cho mô hình về một nhiệm vụ tương tự.
Version 1:
How do I bake a pie?

Version 2:
Suggest a recipe for a pie.

Version 3:
What's a good pie recipe?
Thay đổi thứ tự các thành phần của prompt: Hãy thử thay đổi thứ tự nội dung của prompt và xem điều đó ảnh hưởng như thế nào đến phản hồi.
Phản hồi dự phòng là phản hồi được mô hình trả về khi prompt hoặc bản thân phản hồi của mô hình kích hoạt bộ lọc an toàn (safety filter). Ví dụ như: ""I'm not able to help with that, as I'm only a language model"". Nếu mô hình phản hồi bằng phản hồi dự phòng, hãy thử tăng tham số temperature.
""I'm not able to help with that, as I'm only a language model""
temperature
Tránh dựa vào mô hình để đưa ra các thông tin có tính thực tế, fact.
Các vấn đề về toán học và logic",https://viblo.asia/p/ban-da-biet-gi-ve-prompt-engineering-p4-hoc-duoc-gi-tu-chien-luoc-thiet-ke-prompt-cua-cac-ky-su-google-Rk74amEM4eO
58,Cấu hình menu cho trang web bằng EzyPlatform và sử dụng nhiều ngôn ngữ cho trang web,"Cấu hình menu cho trang web bằng EzyPlatform
B1: tạo class Service gọi tới WebMenuItemService  và WebMenuRepository để thực hiện lấy dữ liệu menu từ phía Admin tạo ra với các thông tin sau
private final WebMenuItemService menuItemService;
    private final WebMenuRepository menuRepository;

    public List<MenuItemModel> getMaiMenuItems() {
        //tìm kiếm menu bằng tên
        Menu menu = menuRepository.findByField(""name"", ""main"");
        return menuItemService.getMenuItemsByMenuId(
                menu.getId()
        );
    }
B2: sau khi đã thực hiện khởi tạo class Service phía Controller sẽ tiến hành gửi thông tin lưu vào View để gửi dữ liệu tới giao diện
trong HomeController thêm thuộc tính vào View.builder():
HomeController
View.builder()
.addVariable (
""mainMenuItems"",
[tên Class Service vừa tạo].getMainMenuItems()
)
ví dụ:
@Controller
@AllArgsConstructor
public class HomeController {

    private final WebBlogMenuItemService blogMenuItemService;

    @DoGet(""/"")
    public View home() {
        return View.builder()
            .template(""home"")
            .addVariable(
                ""mainMenuItems"",
                blogMenuItemService.getMaiMenuItems()
            )
            .addVariable(VIEW_VARIABLE_PAGE_TITLE, ""home"")
            .build();
    }
}
B3: sau khi hoàn tất thao tác phía giao diện sử dụng thymleaf để lặp qua các menuItems
<li th:each=""mainMenuItem : ${mainMenuItems}"" >
    [[${mainMenuItem.linkText}]]
</li>
Trong MenuItemModel có chứa các thuộc tính sau:
private long id;
    private String linkText;
    private String linkType;
    private String customLink;
    private long linkId;
    private int level;
    private long orderValue;
    private long parentId;
B4: sau khi đã hoàn thành các bước trên, chúng ta chạy server admin và khởi tạo menu:
b1: chọn menu
b2: đặt tên menu
b3: lưu lại thông tin menu

sau khi tạo xong chúng ta sẽ thêm các thông tin cho menu
chọn Customize Links
chọn add new và điền các thông tin như tên và URL

khi thêm thông tin xong, phía customize Links sẽ xuất hiện thông tin menu đã thêm, chúng ta chọn và thêm vào menu

khi đã hoàn thành tất cả các thao tác trên, chúng ta sẽ chạy lại chương trình và kiểm tra, thông tin menu sẽ xuất hiện

**Cấu hình web đa ngôn ngữ với ezyPlatform **
B1: trong folder [project_name]_theme\src\main\java\view
tạo Class Web[name]ViewDecorator extends tới WebViewDecorator
ví dụ:
Web[name]ViewDecorator extends tới WebViewDecorator
@EzySingleton
public class WebBlogViewDecorator extends WebViewDecorator {}
B2: trong folder [project_name]_theme\src\main\resources\mesages\Resource Bundle ‘messages’
tạo file message[ngôn ngữ].properties và điền các thông tin ngôn ngữ cần chuyển đổi, vd:
message[ngôn ngữ].properties
trong message_vi.properties: chứa các thông tin như
home= Trang chủ
posts= Danh sách bài viết
contact_me = Thông tin liên hệ
about_me = Giới thiệu
B3: sửa lỗi hiển thị tiếng việt:
chọn file -> setting -> tìm kiếm file encoding -> tích chọn Transparent native-to-ascii conversion",https://viblo.asia/p/cau-hinh-menu-cho-trang-web-bang-ezyplatform-va-su-dung-nhieu-ngon-ngu-cho-trang-web-qPoL7R6jJvk
59,(EzyPlatform) Xây dựng phần Header cho Blog,"Một trang web trong dự án EzyPlatform, chứa các thành phần (flagments) được ghép lại và tạo thành một trang web. Điều này giúp cho ứng dụng được chia nhỏ thành cách thành phần, có thể tái sử dụng và dễ bảo trì. 
Cấu trúc của một trang web đã được định nghĩa trong trang  page.html của dự án. Một trang web sẽ có 3 thành phần cơ bản (flagment): header, content và footer.

Fragment: là tên của một mảnh (một thành phần trong trang web). VD: header là một thành phần của trang web →  header là một flagment → header được định nghĩa là một flagment và trang web muốn sử dụng sẽ thêm flagment tự định nghĩa (header) đó vào.
1. Tạo một flagment
Tạo một file html, trong file html đó sử dụng một thuộc tính để định nghĩa cho một thành phần là một flagment. VD: <footer th:flagment=""footer"" class=""content"">

Các trang web được tạo ra sẽ chứa cấu trúc (khung) của trang page - một thành phần được khai báo trong thẻ html: layout:decorate=""~{ezytheme}"" → Định nghĩa trang web hiện tạo sử dụng trang page.html

Khi tạo một trang web mới, trang web đó sẽ chứa cấu trúc khung của trang page. Do đó để thêm một thành phần (fragment) vào trang web mới chúng ta phải thêm fragment đó vào một trong 3 thành phần (flagment) có sẵn của trang page (header, content, footer)
Để sử dụng một thành phần (fragment) cho trang web, sử dụng cú pháp: <div th:replace=”~{<đường_dẫn_đến_flagment> :: <tên_flagment>}”</div>. (VD: <div th:replace=""~{fragments/header :: header}""></div>).

th- : namespace do thymeleaf tạo ra.
2. Tạo một trang web mới
Một trang web mới được  tạo ra dựa trên cấu trúc của trang page.html. Do đó, cần phải thêm cú pháp: layout:decorate=""~{ezytheme}"" vào trong thẻ html của trang đó. (Làm tương tự nhưu trang home.html có sẵn trong dự án).
Trong folder view → scss → components → Tạo một file scss (VD: _main-navbar.scss)
Sử dụng ""_"" trước tên file để khi sử dụng sẽ không tạo ra các file css nữa.

Trong folder view → scss → vendor → main.scss → thêm đường dẫn đến file scss vừa tạo ở trên. (VD: @import “components/_main-navbar“).

Bước 1: Vào trang Admin → settings → websites → Tạo ngôn ngữ mới.

Bước 2: Trong thư mục: …/resources/message/Resource Bundle ‘messages’ → Tạo một file properties cho ngôn ngữ vừa tạo: messages_<code>.properties

Mỗi lần chỉnh sửa file message thì phải chạy lại để trình duyệt có thể nhận được thay đổi.
Tóm tắt: phần này giới thiệu cách hoạt động của flagment trong một dự án được tạo ra trên nền tảng EzyPlatform. Các tự định nghĩa một flagment, cách tạo mới các file scss và cách sử dụng đa ngôn ngữ.",https://viblo.asia/p/ezyplatform-xay-dung-phan-header-cho-blog-oK9VyZyA4QR
60,[NVIDIA Tools] Bài 9: Occupancy Phần 2,"Ở phần 1 mình đã giới thiệu occupancy thì ở phần 2 mình sẽ đi sâu hơn về occupancy trong việc cải thiện achieved occupancy
Trước khi đi vào bài học mình xin giải thích 2 khái niệm khá quan trọng trong bài viết này:
Tail effect: nếu tổng số thread không chia hết cho warp(32) thì lúc này sẽ xuất hiện tail effect, tail effect là số thread còn lại được chạy cuối cùng trong warp, số thread còn lại càng bé thì tail effect càng ảnh hưởng lớn ==> dẫn đến chương trình chậm

Ví dụ: ta có data với N =80 và số thread là 40 ==> ta sẽ cần 2 warp để asign 40 thread thì wap thứ 2 chỉ sử dụng 8 thread ==> phí tài nguyên và vì N=80 tức là sẽ sử dụng 4 warp trong khi chúng ta chỉ cần 3 warp là đủ
Ví dụ: ta có data với N =80 và số thread là 40 ==> ta sẽ cần 2 warp để asign 40 thread thì wap thứ 2 chỉ sử dụng 8 thread ==> phí tài nguyên và vì N=80 tức là sẽ sử dụng 4 warp trong khi chúng ta chỉ cần 3 warp là đủ
waves: là 1 tên khác để gọi warp
Theoretical occupancy cho chúng ta biết giới hạn trên ( upper - bound ) của các active warps trên 1 SM. Tuy nhiên, trong thực tế, các thread trong các block có thể thực thi với tốc độ khác nhau và có thể kết thúc các phần thực thi của chúng ở các thời điểm khác nhau. Do đó, số lượng warp thực tế hoạt động trên mỗi SM sẽ biến động theo thời gian, phụ thuộc vào cách thức thực thi của các thread trong các block.
Vì vậy chúng ta mới có thêm khái niệm mới - Achieved occupancy để giải quyết vấn đề này:  Achieved occupancy sẽ xem xét trên warp schedulers và sử dụng Hardware performance counters để mà xác định các active warp mỗi clock cycle.
Các bạn có thể xem bài warp schedulers để hiểu rõ hơn
Những nguyên nhân khiến Low Achieved occupancy
Chúng ta sẽ kiểm tra xem Achieved có gần với gTheoretical không. Achieved sẽ thấp hơn Theoretical khi số lượng active warp không được duy trì suốt thời gian SM hoạt động. Điều này có thể xảy ra khi không đủ thread cho các warp hoặc khi sự chia sẻ tài nguyên không hiệu quả dẫn đến stall active warp khi thực thi kernel trên GPU.
Achieved occupancy không thể vượt qua Theoretical occupancy, do đó trước hết chúng ta sẽ dùng thread config để đạt 100% Theoretical occupancy. Sau đó kiểm tra xem sự chênh lệch giữa Achieved - Theoretical, nếu có sự chênh lệch lớn thì sẽ do những nguyên nhân sau đây:
Unbalanced workload within blocks
Nếu các warp trong 1 block không thực thi cùng 1 lúc thì chúng ta đã gặp vấn đề Unbalanced trong block ( Có thể hiểu là số lượng thread trong 1 block quá nhiều dẫn đến 1 vài warp bị stalled, vì mỗi thread sẽ có 1 số lượng register nhất định ) và khi chúng ta sử dụng nhiều thread trong 1 block ==> less utilization ở các warp cuối ==> xuất hiện tail effect
Thay vì sử dụng tối đa thread trong 1 block (1024 thread) thì hãy cân nhắc và lựa chọn cho phù hợp với data
Unbalanced workload across blocks
Nếu các block không thực thi cùng 1 lúc trong mỗi SM thì chúng ta cũng gặp vấn đề Unbalanced trong grid ( Có thể hiểu là số lượng block trong mỗi SM quá nhiều dẫn đến stall) chúng ta có thể khắc phục bằng cách thay đổi số block trong grid hoặc là sử dụng stream để chạy các kernel 1 cách đồng thời
Các bạn có thể xem lại bài streaming để hiểu rõ hơn. Và để trả lời câu hỏi chia bao nhiêu stream là thích hợp thì hãy chia sao mà hạn chế tail effect của mỗi thread trong block
Too few blocks launched
Chúng ta không sử dụng tối đa SM ( số lượng block sử dụng ít hơn số lượng block có thể chạy cùng 1 lúc trong SM ). Hiện tượng full waves - full warps xảy ra khi tổng số SM * tổng số active warp trong mỗi SM
Ví dụ: ta có 15 SM và 100% theoretical occupancy với 4 blocks mỗi SM ==> full waves là 60 blocks, nếu chỉ dùng 45 blocks ==> chỉ đạt được  75% achieved occupancy
Tuy nhiên, đừng bao giờ quá chú tâm vào việc cải thiện occupancy quá nhiều, vì chưa chắc occupancy cao thì tốt

Như những bài viết trước mình đã nói, mỗi thread sẽ có 1 số lượng register nhất định, nếu occupancy càng cao ==> dùng nhiều thread ==> số lượng register mỗi thread giảm ==> khả năng tính toán của thread cũng giảm
Ví dụ: 6 nhân công 6 công việc thì nếu chia đều mỗi nhân công 1 công việc ==> nhân công sẽ khỏe hơn, nếu 2 nhân công cho 6 công việc thì nhân công sẽ tốn nhiều sức hơn ==> vì vậy chúng ta cần xác định là liệu công việc đó có nặng hay không để biết cần sử dụng bao nhiêu nhân công là phù hợp
Tóm lại: trước khi cải thiện occupancy thì hãy xem xét là liệu thuật toán của chúng ta có phức tạp không để từ đó sử dụng số thread cho phù hợp
Câu hỏi đặt ra là làm sao chúng ta xác định thế nào là phức tạp, thế nào là đơn giản? VÌ mỗi máy tính có một tốc độ xử lí khác nhau nên không thể quy chung được ?
Ở những bài sau mình sẽ nói rõ hơn về điều này",https://viblo.asia/p/nvidia-tools-bai-9-occupancy-phan-2-GAWVpMb5405
61,Làm thế nào để cải thiện hiệu suất phát triển với PHP 8,"PHP 8 là một phiên bản quan trọng của ngôn ngữ PHP, giới thiệu nhiều tính năng mới và cải tiến nhằm nâng cao hiệu suất phát triển, hiệu suất và chất lượng ngôn ngữ tổng thể. Trong bài viết này, chúng ta sẽ khám phá cách PHP 8 thúc đẩy hiệu suất phát triển thông qua các tính năng và cải tiến ngôn ngữ khác nhau.
Hệ Thống Kiểu Nâng Cao
PHP 8 giới thiệu hệ thống kiểu nâng cao, bao gồm các đối số có tên, khai báo kiểu được cải thiện và hỗ trợ Union Types. Những cải tiến này làm cho mã rõ ràng hơn và giảm khả năng xảy ra lỗi runtime do vấn đề kiểu. Khai báo kiểu nâng cao cũng giúp IDE cung cấp gợi ý mã tốt hơn và phân tích tĩnh, cải thiện trải nghiệm phát triển.
// Đối số có tên
function greet(string $name, string $greeting): string {
    return ""$greeting, $name!"";
}

// Union Types
function processValue(int|float $value): void {
    // Logic xử lý
}
Tính Năng Ngôn Ngữ Mới: Biểu Thức Match
PHP 8 giới thiệu biểu thức match, một thay thế mạnh mẽ và linh hoạt hơn cho câu lệnh switch. Biểu thức match cho phép bạn thực hiện khớp mẫu dựa trên giá trị của một biểu thức và trả về kết quả tương ứng. Điều này làm cho mã ngắn gọn và dễ đọc hơn, đặc biệt khi xử lý nhiều điều kiện.
$result = match ($status) {
    'success' => 'Operation was successful',
    'failure' => 'Operation failed',
    'in_progress' => 'Operation is still in progress',
};
Toán Tử Nullsafe
PHP 8 giới thiệu toán tử nullsafe (?->), một biến thể của toán tử null coalescing (??). Điều này làm cho việc xử lý các đối tượng có thể null trở nên thuận tiện hơn, tránh các kiểm tra null phức tạp.
// Trong PHP 7 bạn có thể viết:
$length = $obj->getNestedObject()->getString()->length ?? 0;

// Trong PHP 8 bạn có thể đơn giản hóa nó thành:
$length = $obj?->getNestedObject()?->getString()?->length ?? 0;
Attributes
Attributes là một tính năng mới trong PHP 8 cho phép bạn thêm metadata vào các lớp, phương thức, thuộc tính, v.v., theo cách khai báo. Điều này làm cho mã ngắn gọn hơn và cải thiện tính dễ đọc.
#[Route(""/api/users"", methods: [""GET""])]
class UserController {
    #[Inject]
    private UserService $userService;

    #[Authorize(""ADMIN"")]
    public function getUser(int $id): JsonResponse {
        // Logic xử lý
    }
}
Trình Biên Dịch JIT
PHP 8 giới thiệu trình biên dịch Just-In-Time (JIT), có thể biên dịch mã PHP thành mã máy gốc động, cải thiện hiệu suất thực thi. Trình biên dịch JIT có thể tăng đáng kể hiệu suất, đặc biệt trong các tác vụ nặng về tính toán.
Cải Tiến Chuỗi và Mảng
PHP 8 giới thiệu một loạt các cải tiến về chuỗi và mảng, bao gồm các hàm chuỗi mới và cú pháp đường tắt cho mảng. Ví dụ, hàm str_contains kiểm tra xem một chuỗi có chứa chuỗi khác hay không, và từ khóa array có thể được sử dụng để tạo mảng một cách ngắn gọn.
// Cải tiến chuỗi
if (str_contains($haystack, $needle)) {
    // Logic chứa
}

// Cú pháp đường tắt mới cho mảng
$array = [1, 2, ...$anotherArray, 4, 5];
Kết Luận
PHP 8 cải thiện đáng kể hiệu suất phát triển thông qua việc giới thiệu các tính năng ngôn ngữ mới, hệ thống kiểu nâng cao và cải tiến hiệu suất. PHP 8.4 dự kiến sẽ mang đến nhiều tối ưu hóa và tính năng mạnh mẽ hơn nữa. Theo các nguồn tin chính thức, PHP 8.4 sẽ được phát hành vào ngày 21 tháng 11 năm 2024, và nhiều nhà phát triển đang rất mong đợi phiên bản này.
Nếu bạn muốn trải nghiệm PHP 8.4 sớm, bạn có thể thực hiện điều đó thông qua ServBay, bao gồm PHP 8.4 (Dev). Cài đặt chỉ cần một cú nhấp chuột, và bạn có thể tải về miễn phí từ trang web của ServBay nếu quan tâm.
Tải về: https://www.servbay.com
Có câu hỏi? Hãy xem trang hỗ trợ của chúng tôi để được trợ giúp. Thêm vào đó, bạn được mời tham gia cộng đồng Discord của chúng tôi, nơi bạn có thể kết nối với các dev khác, chia sẻ hiểu biết và tìm kiếm hỗ trợ.
Nếu bạn muốn nhận thông tin mới nhất, hãy theo dõi X(Twitter) và Facebook.
Hãy cùng code, cộng tác và sáng tạo!",https://viblo.asia/p/lam-the-nao-de-cai-thien-hieu-suat-phat-trien-voi-php-8-vlZL98RZJQK
62,Ensuring Compliance with Attendance Management Systems: Legal Requirements and Data Privacy Considerations,"
In our life today, we go through many ways of digitization, which brings many organizations to apply Attendance Tracking Systems as a way to automate their attendance management processes. The main benefits of these software solutions are not only tracking employee attendance but also handling n of figures, formulas, and a lot of software and systems. However, with the implementation of such systems, it is crucial for organizations to guarantee compliance with legal requirements and data protection to avoid any potential risks or liabilities.
This blog post will go through the legality of attendance registries, the matter of data privacy, the right steps for the implementation of the technology, collecting the employees' involvement and consent, ""green"" operation, and intercontinental issues' navigation.
Understanding the Legal Landscape of Attendance Tracking
Comprehending the complicated and extensive list of legal requirements becomes the first and also most challenging part for companies when they deploy the software Attendance Tracking Systems. Various laws prescribe the way of employers to handle and record employee hours worked due to which it is very important to have good knowledge of all these distinctions in order not to face any legal problems.
For example, Conformity with the Fair Labor Standards Act (FLSA) in the United States is the first thing to be done if one wants to avoid penalties, as this is a federal rule, which stipulates a minimum wage, overtime, and recordkeeping requirements. At the local and the state level, different legislation may be suggested, which somehow makes it harder to meet all the federally imposed regulatory requirements. Business Owners should make investigations for the information and present it to the authorities, who will explain them the legal laws that are relevant to their business activities that might pretend to the industry that they are in.
Moreover, companies should be acquainted with the laws that govern employee privacy and surveillance. The means for monitoring employee attendance is having to mark your work station or use your biometric print for clocking in and out which is generally proper. However, practices that ensure employees' safety and well-being should not be implemented in violation of privacy rights. This is why it is essential for companies to set a scale among tracking attendance in terms of the operation and observance of privacy rights that workers received. Reaching out to legal advisors to deal with these rules and make sure you know the complete procedure is not only acceptable but also sometimes the only feasible method to protect the organization against potential risks and ensure that attendance monitoring practices are both functional and legal.
The Shared Value of Data Privacy in Attendance Systems
Privacy of data that is entailed within attendance systems is a very sensitive and urging subject-matter since it not only includes the protecting of these user data from being damaged but also the keeping it away from the possibilities of having them unauthorized access or the risk of data breaches. Attendance Tracking Solutions require organizations to keep the personal data of their workers (e.g., work hours, locations, and possibly biometric data) safe and secured. That's why the need for structural implementation of the right regulations for data protection becomes more pronounced. Enterprises are required to integrate high-fidelity security measures like end-to-end encryption, whereby only authorized users can access the platform and thereby ensure the integrity and confidentiality of the data.
Furthermore, they are recommended to expose the explanation as to what the scope of the data is, what techniques are applied in data processing and what measures are adopted to secure employees' privacy. The development of a set of policies that cover these methods enhances not only regulatory capacity but also the chance of productive human relations in the area of personal information. Besides this, verifying that data manipulation procedures are aligned with the GDPR or CCPA principles and requirements affirms that the organization is committed to protect the rights of their employees and is able to operate with good data management practice. More than establishing a legal-aided framework, these actions are about showing any employee that privacy and dignity are free from jeopardy.
Best Practices for Implementing Attendance Tracking Software
Prior to the launching of Attendance Tracking Software, companies must first review the privacy impact assessment to identify it if it might invade the privacy and also think of ways on how they will handle to do that. One of the basic steps is the encryption of the sensitive data to prevent it from being accessed by strangers. Ideas Regarding the importance of data protection are Instructors are Also informing staff about the data protection guidelines and how they individually act are crucial to the safeguarding of their information. Frequent review and testing of software are essential to identifying and fixing any security flaws to increase the overall security of the system.
Furthermore, they need to be sure that the tracker is customized to gather only the necessary data for the attendance, without collecting the personal information. The practice of data minimization, which is also the application of the least data necessary for the task, thus Creating Privacy Protection Article type further recommends minimization of the personal data that employees are collecting. Involving workers in implementation by inviting their feedback not only improves the system but also promotes transparency and trust in the organization. Following these protocols will not only help companies comply with legal and data privacy standards but will also make the Attendance Tracking software more secure and efficient.
Good To Read :- Must-Have Features for an Effective Attendance Management System
Gaining Employee Trust and Consent
Deploying an Attendance Tracking System requires a tactful approach of dealing with employee fears and making them agree to the implementation. The active communication is the first step in building trust, and the company must clearly explain the objectives of using such programs, operation, as well as the information it has and how it relates to the employee's lives. Therefore, it is a requirement to educate staff on howp they will act correctly if the idea is to be received positively. They need to know that they are asked directly to consent and a review-policy must be in place that protects their before the company may process the data.
They need to have an avenue where they can give concerns on the program, however, they will be discusssed same day, along with their answers. The Interaction with staff in this manner, at the same time, is legal because the response protects the privacy of every person, without which, the response of the person may be wrong, and therefore, it cannot be used to gain the loyalty of the society.
Regular Audits and Updates to Stay Compliant
To ensure that their Attendance Tracking Software is following the pertinent laws and regulations that keep shifting organizations also do periodic audits. These audits are actually just tools to ""check in"" the area to see infrastructure ""holes"" or any misuse of the data. In addition to that, it is very important that these findings are not taken as a one-time thing but rather part of an ongoing process that reflects a dynamic nature of the data protection laws and cybersecurity threats. In which forensic auditors would need to look into the time and attendance system, the way data is stored, and security measures implemented while adjusting for the perceived risks of data breaches.
Being updated on the latest laws and technology is another thing that holds the same importance. Data protection laws and employee privacy are areas that are dynamically changing and not static as such, any non-compliances of these by an organization that does not change its practice according to the changes can constitute a serious legal responsibility of the organization to face. Organizations should therefore set up a compliance officer or a team that will overlook the legal developments and also manage the process of software upgrading with the alignment with the new regulatory requirements.
Regularly running audits and making a commitment to broadband software updating in line with new legislative and technological changes are big steps that are, in one way or another, used as the equipment for these very organizations to assure their compliance stance stands. In so doing, firms would be able to easily tackle the challenges in the context of the compliance of the aspects of attendance tracking as the industry changes.
Navigating International Considerations
Global entities are having to become knowledgeable of and compliant with a variety of international data protection and employee surveillance rules and regulations which, even when taken separately, are challenging but vital. On the one hand, European countries, unlike their non-EU counterparts, have codified strong data protection laws. Canada and Australia are some other nations that have rules on the security of employee data.
Companies need to go through legal checklists for each country to which they plant their Headquarters Time and Attendance System. The importance of this is to ensure all legal requirements are met and compliance in local area is not affected. Reb)| Being attentive toward these different jurisdictions can forestall possible lawsuits and establish the company as a legal and compliant one. Reaching out to local legal professionals and integrating privacy requirements that are specific to the region into personal identification systems are measures that are logical to take for ensuring compliance.
Struggling with the complexities of legal compliance and data privacy in attendance management? Let ROCKEYE simplify your compliance journey with robust Attendance Monitoring System tailored to meet all legal requirements and protect your data.
Final thought
Choose the best solution to the problem of Attendance Tracking Solutions deployment through the lens of adherence to regulation and safeguarding of privacy of data.If you take this seriously, you can find opportunities to use technology for good that do not involve the surveillance of staff. This will indeed look like the knight-championing staff privacy rights, who continually adopt the latest in cybersecurity techniques with unparalleled dedication and incorruptible integrity. The company would not like to come across as not compliant and still have errors, as this is a terrible example of irresponsible action. Training the workforce about the new features provided due to the change in legislation and also the new devices available makes one the best tool for on-going compliance.
Getting employees on board in a transparent way and having them consent have great impact on trust, which is the key success factor of implementing these systems. A company will be able to deal effectively with the vulnerabilities and stay in line with the developing legal standards as can be evidenced from the regular audits carried out and the updating of the systems. These tactics enable the management of the business to perform the operations it undertakes in a more efficient way thereby creating a respectful and dignified working environment. Being watchful and having foresight in decision-making, businesses can harmonize the development of a fully compliant, safe and reliable attendance management system.",https://viblo.asia/p/ensuring-compliance-with-attendance-management-systems-legal-requirements-and-data-privacy-considerations-3kY4g0weJAe
63,TỐI ƯU HÓA HỆ THỐNG VỚI MESSAGE QUEUE,"Để scale hệ thống, các dự án thường sẽ tách rời các thành phần khác nhau bên trong nó.
Khi ấy việc scale từng thành phần có thể diễn ra một cách độc lập.
Và message queue là một chiến lược quan trọng để giải quyết vấn đề này, được sử dụng bởi nhiều hệ thống phân tán trong các dự án thực tế.
Message queue cũng là một trong những ví dụ phổ biến về ứng dụng của Queue (Hàng đợi). Nhưng nhiều bạn đi phỏng vấn lại rất hay bị tạch ở đây, vì chỉ biết lý thuyết mà không hiểu Queue trong thực tế có thể ứng dụng vào việc gì.
Cùng mình tìm hiểu trong bài viết này để hiểu hơn về nó nhé!
🔑 ĐỊNH NGHĨA
Message queue là một thành phần được lưu trữ trong memory, hỗ trợ giao tiếp bất đồng bộ.
Nó hoạt động như một buffer và phân phối các request bất đồng bộ, dựa trên nguyên lý của queue (Hàng đợi), tức là Vào Trước Ra Trước (First In First Out - FIFO)
⛩️ KIẾN TRÚC CƠ BẢN
Kiến trúc cơ bản của message queue rất đơn giản.

Nhìn vào hình minh họa ở trên, các bạn sẽ thấy:
Đầu vào của message queue là các Producer (một số tài liệu khác có thể sẽ gọi là Publisher). Tác dụng của nó là tạo message và publish chúng vào message queue.
Đầu vào của message queue là các Producer (một số tài liệu khác có thể sẽ gọi là Publisher). Tác dụng của nó là tạo message và publish chúng vào message queue.
Các message khi được đẩy vào queue lúc này sẽ được sắp xếp theo thứ tự, giống như cách các bạn xếp hàng mua vé xem phim vậy đó.
Các message khi được đẩy vào queue lúc này sẽ được sắp xếp theo thứ tự, giống như cách các bạn xếp hàng mua vé xem phim vậy đó.
Sau đó, các service hoặc server được gọi là Consumer (một số tài liệu khác có thể sẽ gọi là Subscriber) sẽ subscribe message queue để nhận được các message, và thực hiện các hành động tương ứng với các message đó.
Sau đó, các service hoặc server được gọi là Consumer (một số tài liệu khác có thể sẽ gọi là Subscriber) sẽ subscribe message queue để nhận được các message, và thực hiện các hành động tương ứng với các message đó.
Nhìn thấy từ ""subscribe"" các bạn có thể quen không nhỉ? Ông nào hay xem Youtube thì chắc hẳn sẽ rất quen thuộc với lời kêu gọi ""Nếu bạn thấy hay, thì hãy tặng mình 1 like và 1 subscribe nhé!"" của tác giả kênh Youtube đó đúng không nào?
Và mô hình hoạt động của message queue này cũng kiểu như vậy.
Consumer sẽ nhấn nút Subscribe ""kênh Youtube"" tên là Message queue. Để khi nào tác giả của kênh Youtube đó là Publisher có đăng video mới (message), thì ông người xem Consumer này sẽ nhận được thông báo và bật video để xem (xử lý message)
Mô hình này còn có một tên gọi khác là mô hình Pub/Sub. Viết tắt của Publish/Subscribe.
✅ TÁC DỤNG
Nhờ việc có thể tách rời 2 thành phần là Producer và Consumer, message queue trở thành một kiến trúc rất được ưa chuộng để xây dựng ứng dụng có thể dễ dàng mở rộng (scalable) và đáng tin cậy (reliable).
Với message queue, Producer hoàn toàn có thể publish các message lên queue. Dù cho Consumer chưa sẵn sàng để xử lý, thì message cũng vẫn được lưu trữ ở đó theo thứ tự mà Producer gửi lên.
Và ngược lại, Consumer vẫn có thể đọc message từ queue và xử lý, ngay cả khi Producer đang gặp vấn đề.
Đó chính là sự độc lập mà mình nói ở trên.
❌ NGUY HIỂM!!!
Tuy nhiên, không phải vì thế mà chúng ta có thể chủ quan, khi gặp lỗi ở Producer hoặc Consumer thì cứ mặc kệ nó được.
Vì nếu để Producer bị lỗi quá lâu, ứng dụng sẽ không hoạt động theo logic dự kiến nữa, do các message không được tạo ra để đẩy vào queue.
Vì nếu để Producer bị lỗi quá lâu, ứng dụng sẽ không hoạt động theo logic dự kiến nữa, do các message không được tạo ra để đẩy vào queue.
Hoặc nếu logic ở phía trước của Producer không được xử lý tốt, thì cũng có thể gây ra tình trạng Producer đẩy quá nhiều message vào queue khiến cho các Consumer không xử lý kịp, dẫn đến hiện tượng tràn queue.
Hoặc nếu logic ở phía trước của Producer không được xử lý tốt, thì cũng có thể gây ra tình trạng Producer đẩy quá nhiều message vào queue khiến cho các Consumer không xử lý kịp, dẫn đến hiện tượng tràn queue.
Còn nếu để Consumer bị lỗi quá lâu, cũng sẽ có thể dẫn đến hiện tượng tràn queue, do có quá nhiều message được đẩy vào queue mà không được xử lý.
Còn nếu để Consumer bị lỗi quá lâu, cũng sẽ có thể dẫn đến hiện tượng tràn queue, do có quá nhiều message được đẩy vào queue mà không được xử lý.
🔰 VÍ DỤ THỰC TẾ
Dự án mình từng triển khai thi công chức năng gửi email marketing.
Ở phiên bản đầu tiên của chức năng này, bọn mình chỉ đơn giản triển khai với API thông thường. Khi người dùng nhấn nút Gửi email, thì sẽ gọi API gửi mail để xử lý và gửi email đó đến địa chỉ người nhận.
Nhưng vấn đề thực sự phát sinh khi team marketing ở công ty mình thực hiện gửi email hàng loạt tới hàng trăm nghìn người trong danh sách khách hàng của công ty, để quảng bá chiến dịch mới trong cùng một thời điểm, service gửi mail của dự án mình ngất lên ngất xuống ngay lập tức.
Khi đó, bọn mình đã sử dụng Kafka để triển khai message queue khắc phục tình trạng này.

Khi người dùng gửi email, email đó sẽ được đẩy vào queue.
Nhờ vậy mà bọn mình có thể cấu hình để kiểm soát được việc cho phép xử lý bao nhiêu email để gửi đi trong một thời điểm, bằng cách giới hạn số message lấy ra từ queue.
Và khi cần thiết, các producer và các consumer cũng có thể được scale một cách độc lập.
Khi số message trong queue nhiều lên, bọn mình có thể bổ sung các consumer để giảm thời gian xử lý.
Ngược lại, khi số message trong queue trống trong hầu hết thời gian, số lượng các consumer có thể giảm xuống để đỡ lãng phí tài nguyên.
LỜI NHẮN
Bạn có thể tham khảo thêm những bài viết trong series ""System Design - Thiết kế Hệ thống"" của mình trên blog này nhé. Hi vọng kiến thức này hữu ích với bạn.
Follow mình trên Facebook ""CLB Lập trình - THPT Ngọc Tảo"" hoặc kênh Youtube ""Tờ Mờ Sáng học Lập trình"" để cùng nhau học tập, chia sẻ những kiến thức công nghệ và lập trình hoàn toàn miễn phí nhé!
Facebook CLB Lập trình - THPT Ngọc Tảo: https://www.facebook.com/clb.it.ngoctao/
Youtube Tờ Mờ Sáng học Lập trình: https://www.youtube.com/@tmsangdev
Hẹn gặp lại 👋


BẠN CÓ THỂ ĐỌC THÊM
Clean Architecture: A Craftsman’s Guide to Software Structure and Design - Robert C. Martin
Designing Data – Insensitive applications - Martin Kleppmann
System Analysis and Design - Alan Dennis, Barbara Haley Wixom, Roberta M. Roth
System Design Interview - Alex Xu
Modern Systems Analysis and Design - Joseph Valacich, Joey George
Head First Design Patterns - Eric Freeman, Elisabeth Robson",https://viblo.asia/p/toi-uu-hoa-he-thong-voi-message-queue-zXRJ8O924Gq
64,Flutter CI/CD to TestFlight with Github Actions,"
Nếu bạn tìm thấy bài viết này thì khả năng cao bạn cũng đã biết sơ về các khái niệm như Github Actions, CI/CD, TestFlight, Flutter. Ở đây mình sẽ không giải thích thêm mấy từ khóa này.
Bài này thiên về hướng dẫn, note những thao tác cần thiết và không phải về lý thuyết nên mình sẽ không giải thích vì rõ nó là gì, vì sao cần nó, tại sao có nó ví dụ như các file, từ khóa .certSigningRequest, .p8, .p12, .mobileprovision, .plist, Certificate, Provisioning Profile,...bla bla
Khi bạn làm việc với các service khác, luôn luôn phải ""đăng ký"" để sử dụng. Apple cũng như vậy, và tùy vào mỗi bên sẽ yêu cầu các thông tin khác nhau.
Account Apple Developer để có thể đăng nhập được App Store Connect
Project Flutter, Github
Macbook/MiniMac/MacOS
Bạn chưa có tài khoản Apple ?  Đăng ký chứ còn gì nữa ~~ 🫠
Tài khoản cá nhân phí hằng năm là 99$
Tài khoản doanh nghiệp phí hằng năm là 299$
2.1 App Store Connect API Key
Để có thể đẩy file .ipa lên testflight bằng câu lệnh khi CI/CD, Bạn cần phải có apiKey, apiIssuer và file private key .p8
Truy cập vào link này và tạo key, tải file về sẽ có 1 file .p8
https://appstoreconnect.apple.com/access/integrations
Tên của file .p8 sẽ có dạng AuthKey_$apiKey.p8
AuthKey_$apiKey.p8
2.2 Apple Certificate và Provisioning profile
Bước này rất quan trọng, không có nó không Sign remote được
2.2.1 Tạo CertificateSigningRequest
Nó giống như việc bạn đi xin việc, nhà tuyển dụng cần biết bạn là ai, ở đâu, mail nào, sử dụng thiết bị nào... na ná thế.
Tại máy tính của bạn làm theo thao tác sau:
Mở keychain Access.app trong máy Mac của bạn
keychain Access.app
Phía trên bên góc trái Keychain Access -> Certificate Assistant -> Request a Cert ...
Nhập mail của bạn (mail đã đăng ký AppleID), tên chữ ký, và chọn Saved to disk
Sau khi tạo xong nó sẽ ra 1 file như này CertificateSigningRequest.certSigningRequest
CertificateSigningRequest.certSigningRequest
2.2.2 Tạo Certificate
Truy cập vào trang tạo Cert tại Apple Developer
nhấn nút + to tổ bố kế bên chữ Certificates
Nhấn tiếp tục và Chọn Apple Distribution
Nhấn tiếp tục và Upload file .certSigningRequest vừa tạo từ keychain
.certSigningRequest
Nhấn tiếp tục và và kết thúc, ở đây bạn ko cần download file .cer về
.cer
2.2.3 Tạo Profiles
Bấm qua mục Profiles
Nhấn tạo
mục Distribution chọn App Store Connect
App ID bạn chọn đến App hiện tại của bạn ( cái app đã tạo từ tab Identifiers )
Chọn Certificate đã tạo từ bước 2.2.2
Provisioning Profile Name -> Đặt tên cho cái Profile của bạn -> Ấn Generate
Download file về và bạn sẽ được 1 file .mobileprovision
Video Tham khảo, nó ko chính xác.

2.2.4 Testing Profiles
Bạn phải kiểm tra file .mobileprovision vừa tạo có Sign được không, nếu không được do bạn có vấn đề !
Cách để kiểm tra như sau:
Trong project Flutter của bạn Mở Xcode folder IOS
Bên trong Xcode tab Signing Bỏ chọn Automatically manage signing
Automatically manage signing
Mục IOS -> Provisioning profile -> Nhấn để import file .mobileprovision bạn vừa tạo ở bước 2.2.3
.mobileprovision
Nếu không thấy xuất hiện vấn đề gì như hình thì file đã OK, Sign thành công
Nếu bạn thấy có bất kì lỗi đỏ to đùng nào được hiển thị ra, yếu tố tâm linh nhất đó là reset lại máy, vấn đề sẽ được giải quyết ! (mình đã bị lỗi No signing certificate ""iOS Distribution"" found, thử reset máy và thành công )
Nếu không giải quyết được thì google lỗi đó
Câu lệnh thần thánh bạn cần biết để convert file sang base64
Sau khi gõ, nội dung base64 sẽ được lưu trong clipboard của bạn (chỉ việc Ctrl+V để paste ra)
base64 -i path/to/file.txt | pbcopy
Truy cập vào Reponsitory -> Settings -> Secrets and variables để tạo mới 1 secret
3.1 Tạo Chữ ký khi build cho bản android (có thể bỏ qua)
Bạn không biết .jks, key.properties của android là gì ? Bạn có thể xem cách tạo từ bài viết gốc flutter tại đây
Khi bạn đã có tạo được 2 trên, chúng sẽ có đường dẫn như sau trong source flutter:
android/app/upload-keystore.jks
android/key.properties
android/app/upload-keystore.jks
android/key.properties
3.1.1
Tạo 1 secrect và đặt tên là UPLOAD_KEYSTORE_BASE64
UPLOAD_KEYSTORE_BASE64
Tiếp đó mở terminal command line lên và gõ
base64 -i android/app/upload-keystore.jks | pbcopy
Sau khi gõ xong Ctrl+V vào ô value Secrect, tiếp đó ấn Add là xong.
3.1.2
tạo lần lượt các github secret mang tên STOREPASSWORD, KEYPASSWORD tương ứng với giá trị trong file android/key.properties của bạn
STOREPASSWORD
KEYPASSWORD
android/key.properties
3.2 CERTIFICATE .p12 base64
Bạn có thể lấy từ keychain Access.app cũng được, nhưng ở đây mình sẽ hướng dẫn lấy từ XCode.
keychain Access.app
Xcode -> Settings -> Accounts
Chọn Team của bạn (đã được Sign thành công ở bước 2.2.4) -> Click vào Manage Certificates...
Manage Certificates...
Ở mục Apple Distribution -> chuột phải vào certificate còn sáng -> nhấn Export
Bạn sẽ thấy được file .p12, Đặt tên thành BUILD_CERTIFICATE cho mình, cài đặt password cho file đó -> nhấn Save
.p12
BUILD_CERTIFICATE
Chuyển file BUILD_CERTIFICATE.p12 sang base64 thông qua lệnh
BUILD_CERTIFICATE.p12
base64 -i BUILD_CERTIFICATE.p12 | pbcopy
Tạo github secret BUILD_CERTIFICATE_BASE64 từ file tương tự như 3.1.1
BUILD_CERTIFICATE_BASE64
Tạo github secret P12_PASSWORD password vừa nhập khi tạo file .p12
P12_PASSWORD
3.3 Upload Profile .mobileprovision base64
Tạo github secret BUILD_PROVISION_PROFILE_BASE64 Lấy từ Provisioning Profile .mobileprovision ở bước 2.2.3
BUILD_PROVISION_PROFILE_BASE64
Ở đây mình đặt nó tên là PROVISIONING_PROFILE.mobileprovision nên dùng lệnh:
base64 -i PROVISIONING_PROFILE.mobileprovision | pbcopy
Tạo github secret KEYCHAIN_PASSWORD với pass tùy ý đặt
KEYCHAIN_PASSWORD
3.4 ExportOptions.plist
Lưu ý phải Sign được profile trước đó.
Để có được file này chính xác làm theo các bước sau:
XCode -> phía trên tab có chữ Product -> chọn Archive
Đợi 1 lúc build xong sẽ tự nhảy sang màn hình Organizer -> nhấn vào bản vừa build -> chọn Distribute App
Chọn mục Custom -> App Store Connect -> Next chọn Export
Đợi nó xoay xoay 1 chút, nó sẽ hiện cho bạn màn hình chọn, cứ giữ nguyên mạc định và bấm Next
Chọn đến Profile của bạn -> bấm Next -> Cuối cùng là Export
Sau khi Export thành công, sẽ có 1 folder vừa tạo ra, đi vào trong Copy file ExportOptions.plistđó vào folder ios tại project flutter
ExportOptions.plist
File ExportOptions.plist sẽ có dạng như sau:
<?xml version=""1.0"" encoding=""UTF-8""?>
<!DOCTYPE plist PUBLIC ""-//Apple//DTD PLIST 1.0//EN"" ""http://www.apple.com/DTDs/PropertyList-1.0.dtd"">
<plist version=""1.0"">
	<dict>
		<key>destination</key>
		<string>export</string>
		<key>manageAppVersionAndBuildNumber</key>
		<true/>
		<key>method</key>
		<string>app-store</string>
		<key>provisioningProfiles</key>
		<dict>
			<key>xxxxxxx</key>
			<string>xxxxxxx</string>
		</dict>
		<key>signingCertificate</key>
		<string>Apple Distribution</string>
		<key>signingStyle</key>
		<string>manual</string>
		<key>stripSwiftSymbols</key>
		<true/>
		<key>teamID</key>
		<string>xxxxxxx</string>
		<key>uploadSymbols</key>
		<true/>
	</dict>
</plist>
Nếu trong file export đó method ghi là app-store-connect bạn có thể sửa lại thành app-store để tránh lỗi method của flutter
app-store-connect
app-store
Chi tiết thì cứ để nguyên trong lúc CI/CD, nó báo lỗi bạn sẽ biết được từ khóa mình đề cập...
3.5 App Store Connect Private API key .p8 base64
Tạo github secret APPSTORE_KEY_P8_BASE64 lấy ở bước 2.1 Chuyển file .p8 thành base64.
APPSTORE_KEY_P8_BASE64
Nhớ copy ráp đúng cái tên $apiKey để convert
base64 -i AuthKey_$apiKey.p8 | pbcopy
Tạo github secret APPSTORE_APIKEY lấy ở bước 2.1, với apiKey tương ứng
APPSTORE_APIKEY
Tạo github secret APPSTORE_APIISSUER với apiIssuer
APPSTORE_APIISSUER
3.6 Github Repository Token
Token dùng để tự tạo 1 bản Release sau khi build apk, ipa và upload thành công lên testflight.
Truy cập vào Github -> Developer Setting -> Personal Access Token hoặc nhấn Tại đây cho lẹ
Chọn Generate New Token (Classic)
Chỉ cần chọn quyền Repo là được
Quay lại repository của project, tạo github secret REPOSITORY_TOKEN với token vừa tạo
REPOSITORY_TOKEN
3.7 Full workflows
Bên trong source code của bạn, hãy tạo 1 github workflow với đường dẫn như sau .github/workflows/.main.yaml
.github/workflows/.main.yaml
name: Build & Release

on:
  # sự kiện khi push lên nhánh master hoặc tag mới bắt đầu chữ 'v' sẽ chạy job
  push:
    # branches:  
    # - master
    tags:
      - ""v*""

jobs:
  build:
    name: Build
    # chạy trên hệ điều hành macos
    runs-on: macos-latest
    steps:
      # Bắt đầu clone repository về máy
      - name: Clone repository
        uses: actions/checkout@v4

      # Setup Java để chạy
      - name: Set up Java
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: 17

      # Setup Flutter
      - name: Set up Flutter
        uses: subosito/flutter-action@v2
        with:
          channel: stable
          # version flutter sẽ được lấy từ chính file pubspec.yaml của bạn
          flutter-version-file: pubspec.yaml
          architecture: x64

      # Xem lại log version Flutter và XCodeBuild
      - name: Check Flutter Version
        run: flutter --version
      - name: Check XCodeBuild Version
        run: xcodebuild -version

      # Cài đặt các dependencies
      - name: Install dependencies
        run: flutter pub get

      # Download keystore upload-keystore.jks được tạo từ keytool lúc release Google Play
      - name: Download Android Keystore
        run: |
          echo ${{ secrets.UPLOAD_KEYSTORE_BASE64 }} | base64 --decode > android/app/upload-keystore.jks

      # Tạo key.properties lúc release Google Play
      - name: Create key.properties
        run: |
          # Download keystore first (ensure success before creating key.properties)
          if [[ $? -eq 0 ]]; then
            echo ""storePassword=${{ secrets.STOREPASSWORD }}"" >> android/key.properties
            echo ""keyPassword=${{ secrets.KEYPASSWORD }}"" >> android/key.properties
            echo ""keyAlias=upload"" >> android/key.properties
            echo ""storeFile=upload-keystore.jks"" >> android/key.properties
          else
            echo ""Error: Downloading keystore failed. Skipping key.properties creation.""
            exit 1
          fi

      # Tạo chứng chỉ và provisioning profile cho iOS để XCodeBuild có thể build
      - name: Install the Apple certificate and provisioning profile
        env:
          BUILD_CERTIFICATE_BASE64: ${{ secrets.BUILD_CERTIFICATE_BASE64 }} # lấy từ keychain access -> export -> export as p12
          P12_PASSWORD: ${{ secrets.P12_PASSWORD }} # mật khẩu khi export p12
          BUILD_PROVISION_PROFILE_BASE64: ${{ secrets.BUILD_PROVISION_PROFILE_BASE64 }} # lấy từ xcode -> export -> export as provisioning profile
          KEYCHAIN_PASSWORD: ${{ secrets.KEYCHAIN_PASSWORD }} # mật khẩu keychain
        run: |
          # create variables
          CERTIFICATE_PATH=$RUNNER_TEMP/build_certificate.p12
          PP_PATH=$RUNNER_TEMP/build_pp.mobileprovision
          KEYCHAIN_PATH=$RUNNER_TEMP/app-signing.keychain-db

          # import certificate and provisioning profile from secrets
          echo -n ""$BUILD_CERTIFICATE_BASE64"" | base64 --decode -o $CERTIFICATE_PATH
          echo -n ""$BUILD_PROVISION_PROFILE_BASE64"" | base64 --decode -o $PP_PATH

          # create temporary keychain
          security create-keychain -p ""$KEYCHAIN_PASSWORD"" $KEYCHAIN_PATH
          security set-keychain-settings -lut 21600 $KEYCHAIN_PATH
          security unlock-keychain -p ""$KEYCHAIN_PASSWORD"" $KEYCHAIN_PATH

          # import certificate to keychain
          security import $CERTIFICATE_PATH -P ""$P12_PASSWORD"" -A -t cert -f pkcs12 -k $KEYCHAIN_PATH
          security list-keychain -d user -s $KEYCHAIN_PATH

          # apply provisioning profile
          mkdir -p ~/Library/MobileDevice/Provisioning\ Profiles
          cp $PP_PATH ~/Library/MobileDevice/Provisioning\ Profiles

      # Bắt đầu build các file cho Android
      # khi split-per-abi 
      # - app-armeabi-v7a-release.apk: đại đa số các máy thường dùng bản này (file nhẹ)
      # - app-arm64-v8a.apk: dành cho máy mới (Samsung đời mới chẵn hạn, file vừa) 
      # - app-x86_64-release.apk: đồ cổ, intel (file vừa)
      - name: Build Android
        run: |
          # Bản gom lại cho tất cả các máy, máy nào cũng cài được
          flutter build apk --release

          # Bản chia nhỏ theo từng loại máy, máy nào cần cài thì cài
          flutter build apk --release --split-per-abi

          # Bản cần để upload lên Google Play
          flutter build appbundle

      # Pod install cho iOS
      - name: Pod install
        run: cd ios && pod install --repo-update && cd ..

      # Bắt đầu build file ipa cho iOS
      # ExportOptions.plist được tạo từ XCode -> Product -> Archive -> Export -> Development -> Next -> Next -> Save
      - name: Build iOS
        run: |
          flutter build ipa --release --export-options-plist=ios/ExportOptions.plist
      
      # Upload các file đã build lên GitHub artifacts
      - name: Collect the file and upload as artifact
        uses: actions/upload-artifact@v4.3.3
        with:
        # Đặt tên là app-release 
          name: app-release
          path: |
            build/app/outputs/flutter-apk/*.apk
            build/app/outputs/bundle/release/*.aab
            build/ios/ipa/*.ipa

      # Này rất cần thiết, Xóa keychain và provisioning profile sau khi build xong
      - name: Clean up keychain and provisioning profile
        if: ${{ always() }}
        run: |
          security delete-keychain $RUNNER_TEMP/app-signing.keychain-db
          rm ~/Library/MobileDevice/Provisioning\ Profiles/build_pp.mobileprovision

  # Release job, upload the ipa to App Distribution
  release:
    name: Release IPA
    # Cần job [build] trước đó
    needs: [build]
    runs-on: macos-latest
    steps:

      # Download các file đã build và được upload lên artifact từ job [build] trước đó
      - name: Get app-release from artifacts
        uses: actions/download-artifact@v4.1.7
        with:
          # Lấy từ app-release
          name: app-release
          # Lưu vào thư mục build
          path: build
          merge-multiple: true

      # Cài đặt private key .p8 vào máy để xcode có thể nhận diện được
      - name: Install private API key P8
        env:
          APPSTORE_KEY_P8_BASE64: ${{ secrets.APPSTORE_KEY_P8_BASE64 }}
          APPSTORE_APIKEY: ${{ secrets.APPSTORE_APIKEY }}
        run: |
          mkdir -p ~/private_keys
          echo -n ""$APPSTORE_KEY_P8_BASE64"" | base64 --decode > ~/private_keys/AuthKey_$APPSTORE_APIKEY.p8
      
      # Log ra cấu trúc của các file hiện tại
      - name: Display structure of downloaded files
        run: ls -R


      - name: Upload to AppStore
        env:
          APPSTORE_APIKEY: ${{ secrets.APPSTORE_APIKEY }} # lấy từ appstore connect -> users and access -> keys -> create key
          APPSTORE_APIISSUER: ${{ secrets.APPSTORE_APIISSUER }} # lấy từ appstore connect -> users and access -> keys -> create key
          # khi chạy lệnh này cần phải có file AuthKey_$APPSTORE_APIKEY.p8 đã tải vô trong private_keys trước đó
        run: |
          xcrun altool --upload-app --type ios -f build/ios/ipa/*.ipa --apiKey $APPSTORE_APIKEY --apiIssuer $APPSTORE_APIISSUER

      # Đóng gói file đã build và upload lên GitHub Releases
      - name: Push to Releases file
        uses: ncipollo/release-action@v1.14.0
        with:
          name: ${{ github.ref_name }} # tên release
          artifacts: |
            build/app/outputs/flutter-apk/*.apk
            build/app/outputs/bundle/release/*.aab
            build/ios/ipa/*.ipa
          tag: ${{ github.ref_name }} # tag release
          token: ""${{ secrets.REPOSITORY_TOKEN }}"" # token repo, tạo từ setting -> developer settings -> personal access token
Sau cả tỉ lần mình thử nghiệm thì cuối cùng cũng chạy được
Hi vọng qua bài viết này bạn sẽ nắm được đại khái các cách làm:
Tạo Cert, Profile cho Xcode để có thể Sign được và đem sang máy khác
Biết cách lấy Key của App Store Connect API
Hiểu thêm về github secret, github token
Tự động tạo ra được github artifact, github release, tự upload file ipa lên testflight
Tại sao lại sử dụng github secret cho rườm rà thế ?
Như bạn đã biết, 1 vài tác hại xấu khi public repo và bị lộ key, tốn tiền tỉ cho các phí dịch vụ cho những sai lầm không đáng đó thì ta có Secret giảm tiểu được những điều đó, tránh những mong muốn không nên xảy ra.
https://docs.github.com/en/actions/deployment/deploying-xcode-applications/installing-an-apple-certificate-on-macos-runners-for-xcode-development
https://stackoverflow.com/questions/74869907/trying-to-set-certificate-and-provisioning-profile-in-github-actions-for-xcodebu
https://github.com/Apple-Actions/upload-testflight-build/issues/27",https://viblo.asia/p/flutter-cicd-to-testflight-with-github-actions-0gdJzDlkVz5
65,"ChatGPT Series 8: Vấn đề ""Ảo giác"" trong LLMs và cách giảm thiểu nó (Hallucination Mitigation)","Tổng quan
Ảo giác (Hallucination) trong text generation là hiện tượng mô hình AI tạo/ sinh ra các văn bản, mặc dù thường ngữ pháp chính xác và nghe thì có vẻ hợp lý, nhưng không dựa trên đầu vào được cung cấp hoặc thậm chí có thể không chính xác về mặt thực tế. Vấn đề này đặc biệt phổ biến trong các hệ thống như GPT-3, nơi các chi tiết được tạo ra có thể lệch lạc hoặc thậm chí mâu thuẫn với đầu vào. Gần đây, các mô hình như ChatGPT ra đời, hay mới đây là các mô hình khác như Gemeni, GPT-4, GPT-4o thì vấn đề ảo giác vẫn luôn nhức nhối và hiện hữu.
Vấn đề ảo giác bắt nguồn từ những nguyên nhân nào?



Dữ liệu training chưa đủ: Việc mô hình chưa tiếp xúc với dữ liệu đa dạng trong quá trình đào tạo có thể không thiết lập mối tương quan chính xác giữa đầu vào và đầu ra phù hợp, dẫn đến nội dung ảo giác.




Mô hình bị overfitting: Overfit với dữ liệu đào tạo có thể khiến mô hình tạo ra đầu ra phản ánh tập hợp đào tạo nhưng không phù hợp với đầu vào mới hoặc khác đi một chút.




Giám sát không đủ (Inadequate Supervision): Không có hướng dẫn thích hợp, mô hình có thể dựa quá nhiều vào logic nội tại của nó, dẫn đến đầu ra có mang tính ""ảo giác"" cao.




Giới hạn knowledge: Mô hình ngôn ngữ tự nhiên như ChatGPT, GPT4, ... được huấn luyện với kho tri thức giới hạn tới 1 ngày nào đó và do đó nó sẽ không biết về bất kỳ thông tin nào sau ngày bị cutoff knowledge để training đó. Vì vậy,  nó có thể trả lời câu hỏi của bạn với thông tin lỗi thời không còn còn phù hợp, mới được cập nhật.
Dữ liệu training chưa đủ: Việc mô hình chưa tiếp xúc với dữ liệu đa dạng trong quá trình đào tạo có thể không thiết lập mối tương quan chính xác giữa đầu vào và đầu ra phù hợp, dẫn đến nội dung ảo giác.
Dữ liệu training chưa đủ: Việc mô hình chưa tiếp xúc với dữ liệu đa dạng trong quá trình đào tạo có thể không thiết lập mối tương quan chính xác giữa đầu vào và đầu ra phù hợp, dẫn đến nội dung ảo giác.
Mô hình bị overfitting: Overfit với dữ liệu đào tạo có thể khiến mô hình tạo ra đầu ra phản ánh tập hợp đào tạo nhưng không phù hợp với đầu vào mới hoặc khác đi một chút.
Mô hình bị overfitting: Overfit với dữ liệu đào tạo có thể khiến mô hình tạo ra đầu ra phản ánh tập hợp đào tạo nhưng không phù hợp với đầu vào mới hoặc khác đi một chút.
Giám sát không đủ (Inadequate Supervision): Không có hướng dẫn thích hợp, mô hình có thể dựa quá nhiều vào logic nội tại của nó, dẫn đến đầu ra có mang tính ""ảo giác"" cao.
Giám sát không đủ (Inadequate Supervision): Không có hướng dẫn thích hợp, mô hình có thể dựa quá nhiều vào logic nội tại của nó, dẫn đến đầu ra có mang tính ""ảo giác"" cao.
Giới hạn knowledge: Mô hình ngôn ngữ tự nhiên như ChatGPT, GPT4, ... được huấn luyện với kho tri thức giới hạn tới 1 ngày nào đó và do đó nó sẽ không biết về bất kỳ thông tin nào sau ngày bị cutoff knowledge để training đó. Vì vậy,  nó có thể trả lời câu hỏi của bạn với thông tin lỗi thời không còn còn phù hợp, mới được cập nhật.
Giới hạn knowledge: Mô hình ngôn ngữ tự nhiên như ChatGPT, GPT4, ... được huấn luyện với kho tri thức giới hạn tới 1 ngày nào đó và do đó nó sẽ không biết về bất kỳ thông tin nào sau ngày bị cutoff knowledge để training đó. Vì vậy,  nó có thể trả lời câu hỏi của bạn với thông tin lỗi thời không còn còn phù hợp, mới được cập nhật.
Vấn đề ảo giác trong việc sinh nội dung bởi LLMs cho ta thấy được sự cần thiết phải sàng lọc liên tục để nâng cao độ tin cậy của hệ thống AI trong các tình huống thực tế. Tuy nhiên, nó cũng mở ra cánh cửa cho những nghiên cứu tiên phong trong lĩnh vực trí tuệ nhân tạo và xử lý ngôn ngữ tự nhiên.
Trong bài viết này, chúng ta sẽ cùng tìm hiểu về những chiến lược, cách thức để giải quyết hay giảm thiểu tác động của vấn đề hallucination ở các giai đoạn khác nhau trong quy trình phát triển mô hình LLM.

Training
Prompting
Post-process sau quá trình sinh (Post-Response Generation)
Training
Prompting
Post-process sau quá trình sinh (Post-Response Generation)
Training
Chúng ta sẽ bắt đầu bằng cách xem xét cách mà chúng ta có thể giảm thiểu ảo giác ngay từ giai đoạn training mô hình LLM.
Reinforcement Learning from Human Feedback (RLHF)
Tương tự như Human-in-loop, RLHF có thể giúp giảm thiểu ảo giác nhờ feedback của human (người). Bằng cách thu thập dữ liệu mà human rank các reponses khác nhau do mô hình LLM tạo ra dựa trên chất lượng, sự kỳ vọng, ưa thích của người đánh giá, mô hình sẽ học cách điều chỉnh kết quả đầu ra của mình để phù hợp hơn với mong đợi của người hơn.
Ý tưởng đằng sau việc sử dụng RLHF nhờ feedback của Con người (RLHF) để giảm ảo giác là vì con người là tác nhân có thể nhận biết và cung cấp phản hồi về độ chính xác và mức độ liên quan của các responses của mô hình LLM. Bằng cách đưa quá trình feedback của người vào quá trình huấn luyện, mô hình có thể học cách phân biệt giữa thông tin chính xác và không chính xác, giảm khả năng xuất hiện ảo giác. Ngoài ra, RLHF có thể giúp mô hình hiểu rõ, nhận biết hậu quả của nó khi cung cấp những vấn đề sai lệch và giúp cải thiện khả năng tạo ra các phản hồi liên quan và đúng sự thật.
Prompting
Prompting là một kỹ thuật quan trọng trong việc tương tác với các mô hình ngôn ngữ lớn (LLM) sau khi chúng đã được huấn luyện. Đây cũng là một bước đầu tiên trong việc hiểu và giảm thiểu hiện tượng ""hallucination"" (ảo giác), tức là khi mô hình tạo ra thông tin không chính xác hoặc không có cơ sở trong thực tế.
Retrieval Augmented Generation (RAG)
RAG (Retrieval Augmented Generation) giúp loại bỏ hiện tượng ""hallucination"" trong các mô hình ngôn ngữ lớn (LLM) bằng cách cung cấp cho chúng thêm thông tin cụ thể về ngữ cảnh trong quá trình tạo văn bản. Hallucination xảy ra khi một LLM tạo ra các phản hồi dựa trên các mẫu đã học từ dữ liệu huấn luyện thay vì dựa vào ki ến thức thực tế. Điều này xảy ra khi mô hình thiếu thông tin cụ thể về lĩnh vực hoặc gặp khó khăn trong việc nhận ra giới hạn của kiến thức của mình.
RAG giải quyết vấn đề này bằng cách tích hợp các nguồn kiến thức bên ngoài (external knowledge) vào quá trình tạo văn bản. Nó cho phép LLM truy cập vào dữ liệu mới nhất hoặc cụ thể về ngữ cảnh từ một cơ sở dữ liệu bên ngoài, dữ liệu này sẽ được truy xuất và cung cấp cho mô hình khi tạo phản hồi. Bằng cách này, RAG đưa thêm ngữ cảnh vào lời nhắc, giúp LLM hiểu rõ hơn về chủ đề và giảm khả năng xảy ra hiện tượng hallucination.
Ví dụ, trong trường hợp của một chatbot được thiết kế để cung cấp thông tin về xe hơi, RAG có thể truy xuất chi tiết sản phẩm cụ thể và thông tin ngữ cảnh từ một cơ sở dữ liệu bên ngoài để bổ sung cho đầu vào của người dùng. Kết quả là, LLM nhận được một lời nhắc toàn diện và chi tiết hơn, cho phép nó tạo ra các phản hồi chính xác và liên quan hơn.
Hơn nữa, RAG có thể được kết hợp với các kỹ thuật thiết kế lời nhắc tiên tiến, chẳng hạn như cơ sở dữ liệu vector, để cải thiện hiệu suất của LLMs. Bằng cách tận dụng các phương pháp này, các công ty có thể sử dụng LLMs với dữ liệu nội bộ hoặc dữ liệu từ bên thứ ba một cách hiệu quả, đảm bảo rằng các phản hồi được tạo ra không chỉ mạch lạc mà còn chính xác về mặt thực tế.
Nói chung, RAG là một phương pháp giá trị để giảm thiểu tình trạng ""ảo giác"" trong LLMs, cho phép chúng tạo ra các câu trả lời đáng tin cậy và thông tin hơn nhờ việc bổ sung khả năng của chúng với các nguồn kiến thức bên ngoài (external knowledge).
Contextual Prompting
Trapping LLM “Hallucinations” Using Tagged Context Prompts của Đại học Maryland sử dụng một kỹ thuật gắn thẻ nguồn (tagging sources) vào các lời nhắc ngữ cảnh cung cấp cho các mô hình ngôn ngữ lớn (LLMs) để giảm hiện tượng ""ảo giác"".
Giải thích chi tiết hơn, tức là

Các ngữ cảnh được cung cấp cho LLM cùng với câu hỏi là các bản tóm tắt của các bài viết trên Wikipedia, các phần sách, v.v.
Các đoạn ngữ cảnh này được gắn thẻ (tags) bằng cách chèn các định danh duy nhất như “(source 1234)” hoặc “(source 4567)” vào cuối các câu.
Ví dụ:

“Paris là thủ đô của Pháp. (source 1234)”
“Pháp nằm ở Tây Âu. (source 4567)”


Các thẻ nguồn này là các số duy nhất tương ứng với các câu cụ thể trong đoạn ngữ cảnh gốc.
Các ngữ cảnh được cung cấp cho LLM cùng với câu hỏi là các bản tóm tắt của các bài viết trên Wikipedia, các phần sách, v.v.
Các đoạn ngữ cảnh này được gắn thẻ (tags) bằng cách chèn các định danh duy nhất như “(source 1234)” hoặc “(source 4567)” vào cuối các câu.
Ví dụ:

“Paris là thủ đô của Pháp. (source 1234)”
“Pháp nằm ở Tây Âu. (source 4567)”
“Paris là thủ đô của Pháp. (source 1234)”
“Pháp nằm ở Tây Âu. (source 4567)”
Các thẻ nguồn này là các số duy nhất tương ứng với các câu cụ thể trong đoạn ngữ cảnh gốc.
Khi prompting được cung cấp câu hỏi và ngữ cảnh đã được gắn thẻ (tagged context) cho LLM, phương pháp của paper này cũng thêm instruction ví dụ như yêu cầu “Cung cấp chi tiết và bao gồm các nguồn trong câu trả lời (Provide details and include sources in the answer).”
Kết quả là, LLM được guide để có các tham chiếu đến các nguồn đã gắn thẻ này khi tạo ra phản hồi.
Các tags được cung cấp cũng là một cách để kiểm tra xem phản hồi của LLM có dựa trên các nguồn ngữ cảnh đã cho hay không.
Nếu phản hồi chứa các source tags khớp, điều đó chỉ ra rằng LLM đã dựa vào ngữ cảnh được cung cấp thay vì có khả năng xảy ra ""ảo giác"".
Các tác giả nhận thấy rằng kỹ thuật tagging này giảm các phản hồi ""ảo giác"" khoảng 99% so với khi không có ngữ cảnh.
Chain of Verification
Meta AI cũng giới thiệu paper nhằm giảm hiện tượng ""ảo giác"" bằng cách đề xuất CoVe (Chain of Verification). CoVe tạo ra một vài câu hỏi kiểm tra/ xác nhận (verification) để hỏi, sau đó thực hiện các câu hỏi này để kiểm tra sự đồng thuận. Hình ảnh dưới đây từ bài báo gốc giúp hiển thị khái niệm này. (Bạn có thể xem thêm ở bài viết Prompting Engineer 2 mình viết về kỹ thuật này.

CoVe yêu cầu LLM thực hiện nhiều bước sau khi tạo ra phản hồi ban đầu:



Tạo phản hồi cơ bản cho truy vấn. Điều này có thể chứa các sai sót hoặc ""ảo giác"".




Lập chiến lược verify (planning) - LLM tạo ra một bộ câu hỏi kiểm tra để xác thực công việc của chính nó.




Thực hiện verify - LLM trả lời các câu hỏi kiểm tra một cách độc lập.




Tạo ra phản hồi cuối cùng - Chỉnh sửa phản hồi ban đầu dựa trên kết quả kiểm tra.
Tạo phản hồi cơ bản cho truy vấn. Điều này có thể chứa các sai sót hoặc ""ảo giác"".
Tạo phản hồi cơ bản cho truy vấn. Điều này có thể chứa các sai sót hoặc ""ảo giác"".
Lập chiến lược verify (planning) - LLM tạo ra một bộ câu hỏi kiểm tra để xác thực công việc của chính nó.
Lập chiến lược verify (planning) - LLM tạo ra một bộ câu hỏi kiểm tra để xác thực công việc của chính nó.
Thực hiện verify - LLM trả lời các câu hỏi kiểm tra một cách độc lập.
Thực hiện verify - LLM trả lời các câu hỏi kiểm tra một cách độc lập.
Tạo ra phản hồi cuối cùng - Chỉnh sửa phản hồi ban đầu dựa trên kết quả kiểm tra.
Tạo ra phản hồi cuối cùng - Chỉnh sửa phản hồi ban đầu dựa trên kết quả kiểm tra.
Các câu hỏi verify thường được trả lời chính xác hơn so với các sự thật được nêu trong các đoạn văn dài.
Một số biến thể khác nhau của CoVe kiểm soát attention trong quá trình kiểm tra để tránh lặp lại thông tin sai:



Join CoVe:


Một prompt duy nhất (single prompt) cho planning và trả lời câu hỏi.
Một prompt duy nhất chứa truy vấn (query), phản hồi ban đầu (baseline response), câu hỏi verify (verification questions) và câu trả lời (answers).
Attention có thể tập trung vào toàn bộ ngữ cảnh (context) bao gồm cả phản hồi ban đầu (baseline response) có thể chứa thông tin sai.
Dễ lặp lại những vấn đề ""ảo giác"" tương tự khi trả lời câu hỏi.




Two-Step CoVe:


Prompt riêng biệt cho planning dựa trên baseline response.
Prompt riêng biệt cho trả lời các questions mà context không chứa baseline response.
Tránh việc attention trực tiếp vào baseline response có thể chứa thông tin sai.
Nhưng attention vẫn có thể attend vào tất cả các câu hỏi và câu trả lời.




Factored CoVe:


Mỗi câu hỏi được trả lời hoàn toàn độc lập.
Prompt hoàn toàn độc lập cho mỗi câu hỏi verify.
Không attend đến bất kỳ ngữ cảnh nào khác bao gồm cả baseline response.
Tránh sự can thiệp, kết nối giữa các câu hỏi.




Factor+Revise:


Kiểm tra chéo cụ thể giữa câu hỏi và câu trả lời so với các facts ban đầu.
Thêm prompt để kiểm tra chéo giữa câu hỏi và câu trả lời so với các facts ban đầu.
Attention đến cả các facts ban đầu và câu hỏi & câu trả lời.
Đánh dấu các điểm không nhất quán cho phản hồi cuối cùng.


Lưu ý rằng các phương pháp này không được sử dụng song song. Chúng là các biến thể khác nhau của CoVe.
Join CoVe:


Một prompt duy nhất (single prompt) cho planning và trả lời câu hỏi.
Một prompt duy nhất chứa truy vấn (query), phản hồi ban đầu (baseline response), câu hỏi verify (verification questions) và câu trả lời (answers).
Attention có thể tập trung vào toàn bộ ngữ cảnh (context) bao gồm cả phản hồi ban đầu (baseline response) có thể chứa thông tin sai.
Dễ lặp lại những vấn đề ""ảo giác"" tương tự khi trả lời câu hỏi.
Join CoVe:
Một prompt duy nhất (single prompt) cho planning và trả lời câu hỏi.
Một prompt duy nhất chứa truy vấn (query), phản hồi ban đầu (baseline response), câu hỏi verify (verification questions) và câu trả lời (answers).
Attention có thể tập trung vào toàn bộ ngữ cảnh (context) bao gồm cả phản hồi ban đầu (baseline response) có thể chứa thông tin sai.
Dễ lặp lại những vấn đề ""ảo giác"" tương tự khi trả lời câu hỏi.
Two-Step CoVe:


Prompt riêng biệt cho planning dựa trên baseline response.
Prompt riêng biệt cho trả lời các questions mà context không chứa baseline response.
Tránh việc attention trực tiếp vào baseline response có thể chứa thông tin sai.
Nhưng attention vẫn có thể attend vào tất cả các câu hỏi và câu trả lời.
Two-Step CoVe:
Prompt riêng biệt cho planning dựa trên baseline response.
Prompt riêng biệt cho trả lời các questions mà context không chứa baseline response.
Tránh việc attention trực tiếp vào baseline response có thể chứa thông tin sai.
Nhưng attention vẫn có thể attend vào tất cả các câu hỏi và câu trả lời.
Factored CoVe:


Mỗi câu hỏi được trả lời hoàn toàn độc lập.
Prompt hoàn toàn độc lập cho mỗi câu hỏi verify.
Không attend đến bất kỳ ngữ cảnh nào khác bao gồm cả baseline response.
Tránh sự can thiệp, kết nối giữa các câu hỏi.
Factored CoVe:
Mỗi câu hỏi được trả lời hoàn toàn độc lập.
Prompt hoàn toàn độc lập cho mỗi câu hỏi verify.
Không attend đến bất kỳ ngữ cảnh nào khác bao gồm cả baseline response.
Tránh sự can thiệp, kết nối giữa các câu hỏi.
Factor+Revise:


Kiểm tra chéo cụ thể giữa câu hỏi và câu trả lời so với các facts ban đầu.
Thêm prompt để kiểm tra chéo giữa câu hỏi và câu trả lời so với các facts ban đầu.
Attention đến cả các facts ban đầu và câu hỏi & câu trả lời.
Đánh dấu các điểm không nhất quán cho phản hồi cuối cùng.
Factor+Revise:
Kiểm tra chéo cụ thể giữa câu hỏi và câu trả lời so với các facts ban đầu.
Thêm prompt để kiểm tra chéo giữa câu hỏi và câu trả lời so với các facts ban đầu.
Attention đến cả các facts ban đầu và câu hỏi & câu trả lời.
Đánh dấu các điểm không nhất quán cho phản hồi cuối cùng.
Lưu ý rằng các phương pháp này không được sử dụng song song. Chúng là các biến thể khác nhau của CoVe.
Tóm lại, CoVe yêu cầu mô hình có khả năng tự kiểm tra (verify) các phản hồi của mình bằng cách lập kế hoạch, chiến lược (planning) và trả lời các câu hỏi tập trung vào yêu cầu cụ thể, tránh việc điều kiện hóa vào các phản hồi có thể sai của chính mình. Điều này chứng minh giảm ""ảo giác"" trong nhiều nhiệm vụ khác nhau.
Model
Bây giờ, chúng ta thử tiến sâu tìm hiểu vào mô hình và xem xét các nghiên cứu khác nhau về cách giảm thiểu ảo giác xem như thế nào.
DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models
Microsoft giới thiệu một phương pháp mới gọi là Decoding by Contrasting Layers (DoLa), nhằm giảm thiểu hiện tượng ""hallucination"" trong các mô hình ngôn ngữ lớn (LLMs) mà không cần tới quá trình huấn luyện hoặc truy xuất (retrieval) thông tin. Kỹ thuật này dựa trên vấn đề mà chúng ta biết rằng knowledge thực tế trong LLMs chủ yếu được encode trong các layer của kiến trúc transformers.
Trong quá trình tạo văn bản đầu ra, DoLa không dựa vào việc lựa chọn cứng nhắc một premature layer (là một trong những lớp đầu tiên hoặc lớp sớm hơn trong kiến trúc transformer) để tương phản. Thay vào đó, nó chủ động lựa chọn lớp premature layer phù hợp cho việc giải mã của từng token. Cách DoLa xác định lớp này là bằng cách tính toán độ lệch Jensen-Shannon (JSD) giữa phân phối đầu ra của premature layer và mature layers (là lớp cuối cùng trong kiến trúc của mô hình). JSD đóng vai trò là thước đo sự khác biệt giữa hai phân phối xác suất. Logic cơ bản là chọn lớp tiền kỳ có JSD lớn nhất khi được so sánh với lớp trưởng thành, nhằm tối đa hóa sự tương phản giữa kiến thức thực tế và xu hướng ngôn ngữ được gói gọn trong các lớp này.
Trái ngược với mature layers, premature layers ít có khả năng nắm bắt kiến thức thực tế và ngữ cảnh toàn diện. Thay vào đó, chúng tập trung vào việc xử lý các mẫu ngôn ngữ cơ bản và các cấu trúc cú pháp. Các lớp mature layers nằm ở giai đoạn cuối của kiến trúc transformer và thường có khả năng nắm bắt kiến thức phức tạp và ngữ nghĩa toàn diện hơn.
Chúng ta cùng phân tích chi tiết về chức năng của nó:



Đối với mỗi token được giải mã, DoLa chọn động một layer premature bằng cách xác định lớp nào có phân bố đầu ra khác biệt nhất (theo JSD) so với phân bố của mature layer.




JSD cao hơn cho thấy sự khác biệt rõ rệt hơn giữa nội dung thực tế và ngôn ngữ được mã hóa trong hai layers.




Premature layer biểu thị các mẫu (patterns) của ngôn ngữ cơ bản hơn, trong khi mature layer đại diện nhiều hơn cho knowledge thực tế.




DoLa sau đó tính toán phân bố xác suất của token tiếp theo bằng cách so sánh các logit output của mature layer và premature layer được chọn. Cụ thể, điều này liên quan đến việc trừ các xác suất log của premature layer từ các xác suất log của mature layer.




Kết quả là, phân bố xác suất được tạo ra sẽ làm nhấn mạnh những thông tin thực tế trong khi giảm bớt các patterns đơn thuần.
Đối với mỗi token được giải mã, DoLa chọn động một layer premature bằng cách xác định lớp nào có phân bố đầu ra khác biệt nhất (theo JSD) so với phân bố của mature layer.
Đối với mỗi token được giải mã, DoLa chọn động một layer premature bằng cách xác định lớp nào có phân bố đầu ra khác biệt nhất (theo JSD) so với phân bố của mature layer.
JSD cao hơn cho thấy sự khác biệt rõ rệt hơn giữa nội dung thực tế và ngôn ngữ được mã hóa trong hai layers.
JSD cao hơn cho thấy sự khác biệt rõ rệt hơn giữa nội dung thực tế và ngôn ngữ được mã hóa trong hai layers.
Premature layer biểu thị các mẫu (patterns) của ngôn ngữ cơ bản hơn, trong khi mature layer đại diện nhiều hơn cho knowledge thực tế.
Premature layer biểu thị các mẫu (patterns) của ngôn ngữ cơ bản hơn, trong khi mature layer đại diện nhiều hơn cho knowledge thực tế.
DoLa sau đó tính toán phân bố xác suất của token tiếp theo bằng cách so sánh các logit output của mature layer và premature layer được chọn. Cụ thể, điều này liên quan đến việc trừ các xác suất log của premature layer từ các xác suất log của mature layer.
DoLa sau đó tính toán phân bố xác suất của token tiếp theo bằng cách so sánh các logit output của mature layer và premature layer được chọn. Cụ thể, điều này liên quan đến việc trừ các xác suất log của premature layer từ các xác suất log của mature layer.
Kết quả là, phân bố xác suất được tạo ra sẽ làm nhấn mạnh những thông tin thực tế trong khi giảm bớt các patterns đơn thuần.
Kết quả là, phân bố xác suất được tạo ra sẽ làm nhấn mạnh những thông tin thực tế trong khi giảm bớt các patterns đơn thuần.
Phương pháp này rất linh hoạt. Đối với các token tương đối đơn giản và nơi mà phân bố giữa các lớp giống nhau (biểu hiện qua JSD thấp hơn), các lớp đầu tiên có thể được chọn làm premature layer. Ngược lại, đối với các token đòi hỏi kiến thức thực tế phức tạp, một layer premature cao hơn có thể được chọn để tăng cường sự khác biệt với mature layer.
Về mặt thực nghiệm, khi DoLa được thử nghiệm trên các nhiệm vụ khác nhau như hỏi đáp trắc nghiệm, hỏi đáp mở và tạo văn bản, phương pháp này đã cho thấy những cải thiện đáng kể về tính xác thực và độ chính xác, vượt qua các phương pháp giải mã truyền thống và các kỹ thuật giải mã tương phản khác. Ngoài ra, DoLa chỉ thêm một lượng nhỏ chi phí tính toán trong giai đoạn suy luận, làm cho nó trở thành một phương pháp nhẹ nhưng hiệu quả.
Về cơ bản, DoLa cung cấp một phương pháp động để so sánh kiến thức được mã hóa trong các lớp transformer nhằm giảm thiểu hiện tượng ảo giác, và khả năng lựa chọn layer premature phù hợp cho mỗi token là yếu tố trung tâm của hiệu quả của nó.
Phương pháp được DoLa sử dụng, như đã mô tả, bao gồm việc so sánh các đầu ra giữa một layer premature và một layer mature. Layer mature, thường là lớp cuối cùng trong mô hình, được cho là mã hóa nhiều kiến thức thực tế hơn, trong khi các layer premature, ở giai đoạn đầu của mạng, chứa các patterns ngôn ngữ cơ bản hơn.
Lý do cho việc chọn động một premature layer (thay vì lớp mature) nằm ở chính mục tiêu của phương pháp:

1.Cơ chế tương phản: Bằng cách so sánh đầu ra của hai lớp (premature và mature), DoLa nhằm mục đích khuếch đại thông tin thực tế được mã hóa trong lớp mature trong khi giảm bớt các mẫu ngôn ngữ cơ bản trong lớp premature.


Khả năng thích ứng động: Trong khi lớp mature vẫn nhất quán (vì luôn là lớp cuối cùng), việc chọn một lớp premature động cung cấp khả năng thích ứng. Đối với các token hoặc ngữ cảnh khác nhau, sự khác biệt giữa lớp mature và một lớp premature cụ thể có thể rõ rệt hơn, dẫn đến độ phân kỳ Jensen-Shannon cao hơn. Bằng cách chọn các lớp premature khác nhau cho các token khác nhau, DoLa có thể tối đa hóa sự khác biệt này tốt hơn.


3.Highlight thông tin thực tế: Các đầu ra của lớp mature đã được mong đợi là thực tế hơn. Giá trị của việc chọn lớp premature nằm ở việc so sánh nó với lớp mature. Điều này càng nhấn mạnh nội dung thực tế trong các đầu ra của lớp mature.


Tính linh hoạt: Phạm vi các đầu ra có thể từ các lớp premature cung cấp một phổ các mẫu ngôn ngữ. Bằng cách có khả năng chọn từ phổ này, DoLa có thể thích ứng so sánh các loại mẫu ngôn ngữ khác nhau với kiến thức thực tế trong lớp mature, tùy thuộc vào ngữ cảnh.
1.Cơ chế tương phản: Bằng cách so sánh đầu ra của hai lớp (premature và mature), DoLa nhằm mục đích khuếch đại thông tin thực tế được mã hóa trong lớp mature trong khi giảm bớt các mẫu ngôn ngữ cơ bản trong lớp premature.
Khả năng thích ứng động: Trong khi lớp mature vẫn nhất quán (vì luôn là lớp cuối cùng), việc chọn một lớp premature động cung cấp khả năng thích ứng. Đối với các token hoặc ngữ cảnh khác nhau, sự khác biệt giữa lớp mature và một lớp premature cụ thể có thể rõ rệt hơn, dẫn đến độ phân kỳ Jensen-Shannon cao hơn. Bằng cách chọn các lớp premature khác nhau cho các token khác nhau, DoLa có thể tối đa hóa sự khác biệt này tốt hơn.
Khả năng thích ứng động: Trong khi lớp mature vẫn nhất quán (vì luôn là lớp cuối cùng), việc chọn một lớp premature động cung cấp khả năng thích ứng. Đối với các token hoặc ngữ cảnh khác nhau, sự khác biệt giữa lớp mature và một lớp premature cụ thể có thể rõ rệt hơn, dẫn đến độ phân kỳ Jensen-Shannon cao hơn. Bằng cách chọn các lớp premature khác nhau cho các token khác nhau, DoLa có thể tối đa hóa sự khác biệt này tốt hơn.
3.Highlight thông tin thực tế: Các đầu ra của lớp mature đã được mong đợi là thực tế hơn. Giá trị của việc chọn lớp premature nằm ở việc so sánh nó với lớp mature. Điều này càng nhấn mạnh nội dung thực tế trong các đầu ra của lớp mature.
Tính linh hoạt: Phạm vi các đầu ra có thể từ các lớp premature cung cấp một phổ các mẫu ngôn ngữ. Bằng cách có khả năng chọn từ phổ này, DoLa có thể thích ứng so sánh các loại mẫu ngôn ngữ khác nhau với kiến thức thực tế trong lớp mature, tùy thuộc vào ngữ cảnh.
Tính linh hoạt: Phạm vi các đầu ra có thể từ các lớp premature cung cấp một phổ các mẫu ngôn ngữ. Bằng cách có khả năng chọn từ phổ này, DoLa có thể thích ứng so sánh các loại mẫu ngôn ngữ khác nhau với kiến thức thực tế trong lớp mature, tùy thuộc vào ngữ cảnh.
Về cơ bản, layer mature hoạt động như một điểm tham chiếu nhất quán, trong khi việc chọn premature layer cho phép mô hình thích ứng nhấn mạnh nội dung thực tế hơn các mẫu ngôn ngữ, từ đó nhằm giảm thiểu hiện tượng ảo giác và cải thiện độ chính xác thực tế của nội dung được tạo ra.
Giảm Thiểu Hiện Tượng Ảo Giác Sau Khi Tạo Câu Trả Lời (Post-Response Generation)
Giảm Thiểu Hiện Tượng Ảo Giác của Mô Hình Ngôn Ngữ Bằng Sự Căn Chỉnh Tương Tác Giữa Question và Knowledge
Source Paper: https://arxiv.org/pdf/2305.13669.pdf
MixAlign là framework mới được thiết kế để tạo điều kiện tương tác liền mạch giữa người dùng và cơ sở dữ liệu kiến thức. Điểm mạnh cốt lõi của nó nằm ở việc đảm bảo các queries của người dùng chính xác phản ánh thông tin được lưu trữ trong cơ sở dữ liệu kiến thức. Thành phần quan trọng trong thiết kế của MixAlign là việc bao gồm các can thiệp của con người, được tích hợp một cách cẩn thận để giảm thiểu hiện tượng ảo giác.
Khi nhận được truy vấn từ người dùng, MixAlign sử dụng mô hình ngôn ngữ để tinh chỉnh truy vấn này, đảm bảo nó phản ánh chính xác các thuộc tính được mô tả trong sơ đồ cơ sở dữ liệu kiến thức. Sử dụng truy vấn đã được tinh chỉnh, hệ thống trích xuất các đoạn bằng chứng liên quan. Tuy nhiên, khi đối mặt với nhiều câu trả lời tiềm năng có thể gây mơ hồ, MixAlign kích hoạt cơ chế tập trung vào con người của nó.
Trong giai đoạn này, MixAlign xác định thuộc tính nổi bật nhất để phân biệt các ứng viên. Tận dụng thuộc tính này, mô hình ngôn ngữ sẽ xây dựng một câu hỏi làm rõ mục tiêu. Ví dụ, nếu ""mùa"" xuất hiện như là thuộc tính phân biệt, nó có thể đặt câu hỏi như, ""Bạn đang đề cập đến mùa nào?"" Phản hồi của người dùng sau đó sẽ làm rõ truy vấn, xóa bỏ những mơ hồ xung quanh các ứng viên.
Sự tương tác hợp tác này, trong đó những hiểu biết của con người là then chốt, sau đó được tích hợp lại vào hệ thống. Được trang bị sự căn chỉnh đã được tinh chỉnh này, mô hình ngôn ngữ sẽ tạo ra một câu trả lời cuối cùng chi tiết và chính xác.
Về cơ bản, MixAlign kết hợp các quy trình tự động với các can thiệp kịp thời của con người. Bằng cách này, nó giải quyết các mơ hồ tiềm ẩn ngay từ khi mới xuất hiện, vượt qua một rào cản lớn được quan sát thấy trong các hệ thống tạo nội dung hỗ trợ truy xuất truyền thống và hình ảnh dưới đây từ bài báo gốc minh họa điều đó.

Human-in-the-loop
Nói chung, việc sử dụng nguồn lực từ con người để review xác thực đầu ra LLM có thể giúp giảm thiểu tác động của ảo giác và cải thiện chất lượng cũng như độ tin cậy tổng thể của văn bản được tạo, nhưng cũng đòi hỏi kèm theo về nguồn chi phí khổng lồ.
Các nghiên cứu liên quan tới vấn đề Hallucination
FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation
Source paper: https://arxiv.org/abs/2305.14251
Bài báo này của tác giả Min và cộng sự từ Đại học Washington, Đại học Massachusetts Amherst, Viện Nghiên cứu AI Allen, và Meta AI tập trung vào việc đánh giá độ chính xác thực tế của văn bản dài được tạo ra bởi các mô hình ngôn ngữ lớn (LMs).
Bài báo giới thiệu FACTSCORE, một phương pháp đánh giá mới đo lường độ chính xác thực tế của văn bản được tạo ra bởi các LMs. Nó phân tách một văn bản thành các sự kiện nguyên tử và tính toán tỷ lệ phần trăm của những sự kiện này được hỗ trợ bởi nguồn kiến thức đáng tin cậy. Phương pháp này đặc biệt cần thiết vì văn bản thường chứa hỗn hợp thông tin được hỗ trợ và không được hỗ trợ, khiến cho việc đánh giá chất lượng theo kiểu nhị phân trở nên không đủ.
FACTSCORE giải quyết hai ý tưởng chính: sử dụng các sự kiện nguyên tử làm đơn vị để đánh giá và đánh giá độ chính xác thực tế dựa trên một nguồn kiến thức cụ thể. Nó định nghĩa một sự kiện nguyên tử là một câu ngắn chứa một thông tin duy nhất. Cách tiếp cận này cho phép đánh giá độ chính xác thực tế chi tiết hơn so với các phương pháp trước đây. Bài báo sử dụng tiểu sử của những người làm cơ sở để đánh giá do tính khách quan của chúng và bao gồm nhiều quốc tịch, nghề nghiệp, và mức độ hiếm gặp khác nhau.
Bạn có thể xem hình ảnh dưới đây từ bài báo cho thấy tổng quan về FACTSCORE, một tỷ lệ các sự kiện nguyên tử (các mẩu thông tin) được hỗ trợ bởi một nguồn kiến thức nhất định. FACTSCORE cho phép đánh giá độ chính xác thực tế chi tiết hơn, ví dụ, trong hình, mô hình đứng đầu đạt được điểm số 66.7% và mô hình đứng dưới đạt 10.0%, trong khi các công trình trước đây sẽ chỉ định 0.0 cho cả hai. FACTSCORE có thể dựa trên đánh giá của con người, hoặc được tự động hóa, cho phép đánh giá một tập hợp lớn các LMs mà không cần nỗ lực từ con người.

Bộ estimator tự động FACTSCORE của họ đầu tiên chia một thế hệ thành một loạt các sự kiện nguyên tử và sau đó kiểm tra từng cái với nguồn tri thức đã cho. Họ sử dụng LLAMA 7B được huấn luyện trên Super Natural Instructions và ChatGPT như là một LMEVAL, và các công cụ truy xuất dựa trên T5 tổng quát để truy xuất đoạn văn.
Bài báo đánh giá ba mô hình ngôn ngữ thương mại hàng đầu: InstructGPT, ChatGPT, và PerplexityAI. Các mô hình này gặp khó khăn với lỗi chính xác thực tế, với điểm FACTSCORE lần lượt là 42%, 58%, và 71%. Nghiên cứu nhấn mạnh rằng độ chính xác thực tế của các mô hình này giảm đáng kể khi độ hiếm của các thực thể được thảo luận trong văn bản tăng lên.
Để giải quyết tính tốn thời gian và tốn kém của việc đánh giá bằng con người, các tác giả đề xuất một mô hình tự động để ước lượng FACTSCORE. Mô hình này phân tích văn bản thành các sự kiện nguyên tử và kiểm tra từng cái với nguồn tri thức, gần như tương đương với FACTSCORE với sai số dưới 2%. Điều này cho phép đánh giá một tập lớn các mô hình ngôn ngữ mới mà không cần nỗ lực thủ công từ con người.
Bài báo cũng trình bày ứng dụng của bộ ước lượng tự động này bằng cách đánh giá 12 mô hình ngôn ngữ mới được phát hành gần đây, cung cấp cái nhìn sâu sắc về độ chính xác thực tế của chúng. Phương pháp này có thể tốn $65K nếu được đánh giá bởi con người, nhấn mạnh tính tiết kiệm chi phí của phương pháp tự động.
Cuối cùng, bài báo đề xuất các công việc tương lai để nâng cao FACTSCORE, bao gồm việc xem xét các khía cạnh khác của tính chính xác thực tế như recall (phạm vi thông tin thực tế), cải thiện bộ ước lượng để gần đúng hơn với độ chính xác thực tế, và sử dụng FACTSCORE để chỉnh sửa các thế hệ của mô hình.
Tổng thể thì FACTSCORE cho thấy một tiến bộ đáng kể trong việc đánh giá độ chính xác thực tế của văn bản được tạo ra bởi các mô hình ngôn ngữ, cung cấp một phương pháp chi tiết và tiết kiệm chi phí để đánh giá độ chính xác của văn bản dài.
G-Eval: NLG Evaluation Using GPT-4 with Better Human Alignment
Source paper: https://arxiv.org/abs/2303.16634
Source code: https://github.com/nlpyang/geval
Chất lượng của các văn bản được tạo ra bởi các hệ thống tạo ngôn ngữ tự nhiên (NLG) rất khó đo lường tự động. Các chỉ số dựa trên tham chiếu thông thường, như BLEU và ROUGE, đã được chứng minh là có mối tương quan tương đối thấp với đánh giá của con người, đặc biệt đối với các nhiệm vụ đòi hỏi sự sáng tạo và đa dạng. Các nghiên cứu gần đây đề xuất sử dụng các mô hình ngôn ngữ lớn (LLM) như các chỉ số không dựa trên tham chiếu để đánh giá NLG, với lợi ích là có thể áp dụng cho các nhiệm vụ mới không có tham chiếu từ con người. Tuy nhiên, các bộ đánh giá dựa trên LLM này vẫn có mức độ tương đồng với con người thấp hơn so với các bộ đánh giá trung bình cỡ vừa.
Bài báo này của Liu et al. giới thiệu G-Eval, một khung sử dụng các mô hình ngôn ngữ lớn với chuỗi suy nghĩ (CoT) và một mô hình điền vào mẫu, để đánh giá chất lượng của các kết quả NLG.
Table bên dưới từ bài báo minh họa khung tổng thể của G-Eval. Đầu tiên, họ nhập phần Giới thiệu Nhiệm vụ và Tiêu chí Đánh giá (evaluation criteria) vào LLM, và yêu cầu nó tạo ra một chuỗi CoT của các Bước Đánh giá (evaluation steps) chi tiết. Sau đó, sử dụng yêu cầu này cùng với chuỗi suy nghĩ đã tạo để đánh giá các kết quả NLG theo mô hình điền vào mẫu. Cuối cùng, sử dụng tổng hợp có trọng số xác suất của các điểm đầu ra làm điểm số cuối cùng.

Trong paper, họ cũng thực nghiệm với hai nhiệm vụ tạo văn bản, bao gồm tóm tắt văn bản (text summarization) và tạo đoạn hội thoại (dialogue generation) và cho thấy rằng G-Eval với mô hình nền tảng GPT-4 đạt được độ tương quan Spearman là 0.514 với đánh giá của con người trong nhiệm vụ tóm tắt văn bản, vượt trội hơn tất cả các phương pháp trước đây với một khoảng cách khá lớn.
Họ cũng đề xuất phân tích sơ bộ về hành vi của việc đánh giá dựa trên LLM và nêu những vấn đề là việc đánh giá dựa trên LLM có thể thiên vị, ưu ái cho các văn bản do LLM tạo ra.
Nguồn tham khảo
Options for Solving Hallucinations in Generative AI
Retrieval Augmented Generation
Hallucination Detection and Hallucination Mitigation: An Investigation",https://viblo.asia/p/chatgpt-series-8-van-de-ao-giac-trong-llms-va-cach-giam-thieu-no-hallucination-mitigation-GAWVpMmo405
66,DataFrames trong Apache Spark,"DataFrames là một khái niệm quan trọng trong Apache Spark, cung cấp một cách linh hoạt và hiệu quả để làm việc với dữ liệu dưới dạng bảng có cấu trúc, tương tự như trong cơ sở dữ liệu quan hệ.

Dưới đây là một số điểm cần hiểu rõ về DataFrames:
1. Bảng dữ liệu có cấu trúc:
DataFrames được tổ chức dưới dạng các hàng và cột, tương tự như các bảng trong cơ sở dữ liệu quan hệ. Mỗi cột trong DataFrame có một tên và một kiểu dữ liệu, giúp cho việc thực hiện các thao tác truy vấn và biến đổi dữ liệu dễ dàng hơn.
2. API dễ sử dụng:
Apache Spark cung cấp một API dễ sử dụng cho việc làm việc với DataFrames. API này hỗ trợ nhiều ngôn ngữ lập trình như Scala, Java, Python và R, giúp cho các nhà phát triển có thể làm việc với DataFrames trong môi trường phát triển mà họ thoải mái nhất.
Dưới đây là một ví dụ cụ thể về cách sử dụng API dễ sử dụng của DataFrames trong Apache Spark bằng Python:
# Import thư viện và modules cần thiết
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Khởi tạo một phiên SparkSession
spark = SparkSession.builder \
    .appName(""Example DataFrame API"") \
    .getOrCreate()

# Đọc dữ liệu từ một tệp CSV vào DataFrame
df = spark.read.csv(""data.csv"", header=True, inferSchema=True)

# Hiển thị cấu trúc của DataFrame
df.printSchema()

# Hiển thị 5 hàng đầu tiên của DataFrame
df.show(5)

# Thực hiện một số phép biến đổi và truy vấn trên DataFrame
# Ví dụ: Lọc các hàng có giá trị 'age' lớn hơn 30
filtered_df = df.filter(col(""age"") > 30)

# Hiển thị 5 hàng đầu tiên của DataFrame sau khi lọc
filtered_df.show(5)

# Tính tổng của cột 'salary'
total_salary = df.selectExpr(""sum(salary)"").collect()[0][0]
print(""Tổng lương của tất cả nhân viên:"", total_salary)

# Ghi DataFrame vào một tệp Parquet
df.write.parquet(""output.parquet"")

# Đóng phiên SparkSession
spark.stop()
Trong ví dụ này, chúng ta sử dụng API của DataFrames trong Apache Spark để thực hiện các thao tác sau:
Đọc dữ liệu từ một tệp CSV vào DataFrame.
In ra cấu trúc của DataFrame và hiển thị 5 hàng đầu tiên.
Lọc các hàng có giá trị cột 'age' lớn hơn 30.
Tính tổng của cột 'salary'.
Ghi DataFrame vào một tệp Parquet.
Đóng phiên SparkSession.
Các hàm và phương thức trong API của DataFrames như filter, selectExpr, collect, write,... cho phép bạn thực hiện các thao tác truy vấn và biến đổi dữ liệu một cách dễ dàng và linh hoạt. Điều này làm cho việc làm việc với DataFrames trong Apache Spark trở nên đơn giản và hiệu quả.
filter
selectExpr
collect
write
3. Tối ưu hóa hiệu suất:
DataFrames được tối ưu hóa để tận dụng các tính năng in-memory của Apache Spark, giảm thiểu việc truy cập dữ liệu từ đĩa và tối ưu hóa việc chuyển đổi dữ liệu giữa các phần của DataFrame trên cụm. Điều này giúp tăng tốc độ xử lý dữ liệu và cải thiện hiệu suất của ứng dụng của bạn.
Để minh họa về tối ưu hóa hiệu suất của DataFrames trong Apache Spark, hãy xem xét một ví dụ cụ thể:
Giả sử bạn có một tập dữ liệu lớn chứa thông tin về các đơn đặt hàng từ một trang web thương mại điện tử. Tập dữ liệu này có hàng triệu hàng đơn với các trường như ID đơn hàng, ID khách hàng, ngày đặt hàng, tổng số tiền, v.v.
Bạn muốn tính toán tổng số tiền mà mỗi khách hàng đã chi tiêu trong một khoảng thời gian nhất định. Bạn có thể sử dụng DataFrames của Apache Spark để thực hiện phân tích này một cách dễ dàng và hiệu quả.
Dưới đây là một ví dụ về cách bạn có thể sử dụng DataFrame để thực hiện tính toán này và cách Apache Spark tối ưu hóa hiệu suất:
from pyspark.sql import SparkSession
from pyspark.sql.functions import sum

# Khởi tạo SparkSession
spark = SparkSession.builder \
    .appName(""CalculateTotalSpent"") \
    .getOrCreate()

# Đọc dữ liệu từ tập tin CSV thành DataFrame
orders_df = spark.read.csv(""orders.csv"", header=True, inferSchema=True)

# Lọc các đơn đặt hàng trong khoảng thời gian nhất định
filtered_orders_df = orders_df.filter((orders_df.order_date >= '2024-01-01') & (orders_df.order_date <= '2024-06-01'))

# Tính tổng số tiền chi tiêu của mỗi khách hàng
total_spent_per_customer = filtered_orders_df.groupBy(""customer_id"").agg(sum(""total_amount"").alias(""total_spent""))

# Hiển thị kết quả
total_spent_per_customer.show()

# Dừng SparkSession
spark.stop()
Trong ví dụ này:
Đầu tiên, chúng ta khởi tạo một SparkSession, là một điểm nhập cho việc làm việc với dữ liệu của chúng ta trong Apache Spark.
Đầu tiên, chúng ta khởi tạo một SparkSession, là một điểm nhập cho việc làm việc với dữ liệu của chúng ta trong Apache Spark.
SparkSession
Tiếp theo, chúng ta đọc dữ liệu từ tập tin CSV vào DataFrame sử dụng phương thức read.csv().
Tiếp theo, chúng ta đọc dữ liệu từ tập tin CSV vào DataFrame sử dụng phương thức read.csv().
read.csv()
Sau đó, chúng ta lọc các đơn đặt hàng trong khoảng thời gian nhất định bằng cách sử dụng phương thức filter().
Sau đó, chúng ta lọc các đơn đặt hàng trong khoảng thời gian nhất định bằng cách sử dụng phương thức filter().
filter()
Chúng ta sử dụng phương thức groupBy() để nhóm các đơn đặt hàng theo customer_id và sử dụng hàm tổng sum() để tính tổng số tiền chi tiêu của mỗi khách hàng.
Chúng ta sử dụng phương thức groupBy() để nhóm các đơn đặt hàng theo customer_id và sử dụng hàm tổng sum() để tính tổng số tiền chi tiêu của mỗi khách hàng.
groupBy()
customer_id
sum()
Cuối cùng, chúng ta hiển thị kết quả bằng phương thức show() và dừng SparkSession.
Cuối cùng, chúng ta hiển thị kết quả bằng phương thức show() và dừng SparkSession.
show()
SparkSession
Apache Spark tối ưu hóa hiệu suất bằng cách thực hiện các phép tính một cách song song trên các partition của dữ liệu và tận dụng tính năng in-memory của nó. Điều này giúp cho việc xử lý dữ liệu lớn trở nên nhanh chóng và hiệu quả.
4. Hỗ trợ đa ngôn ngữ:
DataFrames hỗ trợ làm việc với dữ liệu bằng nhiều ngôn ngữ lập trình khác nhau, cho phép bạn sử dụng ngôn ngữ phù hợp nhất với nhu cầu của mình. Điều này làm cho việc phát triển và triển khai các ứng dụng Spark trở nên dễ dàng và linh hoạt hơn.
Dưới đây là một ví dụ cụ thể về cách Apache Spark hỗ trợ đa ngôn ngữ:
Giả sử bạn đang làm việc trên một dự án phân tích dữ liệu sử dụng Apache Spark và bạn đã chọn Python làm ngôn ngữ chính để phát triển ứng dụng của mình.
Đọc dữ liệu từ tệp CSV và xử lý dữ liệu bằng Python:
# Import SparkSession từ gói pyspark.sql
from pyspark.sql import SparkSession

# Khởi tạo SparkSession
spark = SparkSession.builder \
    .appName(""example"") \
    .getOrCreate()

# Đọc dữ liệu từ tệp CSV vào DataFrame
df = spark.read.csv(""data.csv"", header=True, inferSchema=True)

# Hiển thị 10 hàng đầu tiên của DataFrame
df.show(10)

# Thực hiện xử lý dữ liệu bằng Python
result_df = df.filter(df['age'] > 30).select(df['name'], df['age'])

# Hiển thị kết quả
result_df.show()
Sử dụng SQL trong Python để truy vấn dữ liệu:
# Tạo view tạm thời từ DataFrame
df.createOrReplaceTempView(""people"")

# Thực hiện truy vấn SQL trên DataFrame
result_sql = spark.sql(""SELECT name, age FROM people WHERE age > 30"")

# Hiển thị kết quả
result_sql.show()
Làm việc với RDDs bằng Scala:
// Import SparkSession từ gói org.apache.spark.sql
import org.apache.spark.sql.SparkSession

// Khởi tạo SparkSession
val spark = SparkSession.builder
    .appName(""example"")
    .getOrCreate()

// Đọc dữ liệu từ tệp CSV vào RDD
val rdd = spark.sparkContext.textFile(""data.csv"")

// Xử lý RDD bằng Scala
val filteredRdd = rdd.filter(line => line.split("","")(1).toInt > 30)

// Hiển thị kết quả
filteredRdd.take(10).foreach(println)
Như bạn có thể thấy, Apache Spark cho phép bạn làm việc với dữ liệu sử dụng nhiều ngôn ngữ lập trình khác nhau như Python và Scala trong một dự án. Điều này làm cho việc phát triển và triển khai ứng dụng Spark trở nên linh hoạt và thuận tiện hơn, đồng thời tận dụng được sức mạnh của các ngôn ngữ lập trình phổ biến.
5. Tích hợp với các công cụ phân tích dữ liệu:
DataFrames được tích hợp chặt chẽ với các công cụ phân tích dữ liệu khác trong hệ sinh thái Spark như Spark SQL, MLlib và GraphX. Điều này cho phép bạn thực hiện các công việc phức tạp như truy vấn dữ liệu, xây dựng và huấn luyện các mô hình machine learning trên dữ liệu của mình một cách dễ dàng và linh hoạt.
Dưới đây là một ví dụ cụ thể về cách DataFrames được tích hợp với các công cụ phân tích dữ liệu khác trong hệ sinh thái Apache Spark:
Ví dụ: Xử lý dữ liệu bán lẻ với DataFrames và Spark SQL
Giả sử bạn có một tập dữ liệu lớn về bán lẻ chứa thông tin về các giao dịch mua sắm, bao gồm thông tin về sản phẩm, khách hàng, và doanh số bán hàng. Bạn muốn phân tích dữ liệu này để hiểu rõ hơn về hành vi mua sắm của khách hàng và các xu hướng bán hàng.
Đầu tiên, bạn có thể sử dụng Apache Spark để nạp dữ liệu từ tệp CSV hoặc cơ sở dữ liệu vào một DataFrame:
from pyspark.sql import SparkSession

# Khởi tạo SparkSession
spark = SparkSession.builder \
    .appName(""Retail Analysis"") \
    .getOrCreate()

# Nạp dữ liệu từ tệp CSV vào DataFrame
df = spark.read.csv(""path/to/retail_data.csv"", header=True, inferSchema=True)
Sau đó, bạn có thể sử dụng Spark SQL để thực hiện các truy vấn phức tạp trên DataFrame để trả lời các câu hỏi phân tích dữ liệu:
# Tạo view tạm thời từ DataFrame
df.createOrReplaceTempView(""retail_data"")

# Truy vấn thông tin về doanh số bán hàng theo sản phẩm
sales_by_product = spark.sql(""""""
    SELECT product_id, SUM(sales_amount) AS total_sales
    FROM retail_data
    GROUP BY product_id
    ORDER BY total_sales DESC
"""""")

sales_by_product.show()
Ngoài ra, bạn có thể sử dụng DataFrames để chuẩn bị dữ liệu cho việc xây dựng mô hình machine learning, ví dụ như dự đoán doanh số bán hàng trong tương lai dựa trên các yếu tố như giá cả, quảng cáo, và thời gian:
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression

# Tạo một VectorAssembler để biến đổi các đặc trưng thành một vector
assembler = VectorAssembler(inputCols=[""price"", ""advertising"", ""time""], outputCol=""features"")
data = assembler.transform(df)

# Chia dữ liệu thành tập huấn luyện và tập kiểm tra
train_data, test_data = data.randomSplit([0.8, 0.2])

# Xây dựng mô hình hồi quy tuyến tính
lr = LinearRegression(featuresCol=""features"", labelCol=""sales_amount"")
model = lr.fit(train_data)

# Đánh giá mô hình trên tập kiểm tra
predictions = model.transform(test_data)
Trong ví dụ này, chúng ta sử dụng DataFrames và Spark SQL để thực hiện phân tích dữ liệu và sử dụng dữ liệu được xử lý để xây dựng một mô hình hồi quy tuyến tính để dự đoán doanh số bán hàng. Điều này thể hiện cách DataFrames được tích hợp chặt chẽ với các công cụ phân tích dữ liệu khác trong hệ sinh thái Apache Spark để giúp bạn thực hiện các tác vụ phức tạp trong lĩnh vực Big Data.
Tóm lại, DataFrames là một khái niệm quan trọng trong Apache Spark, cung cấp một cách linh hoạt và hiệu quả để làm việc với dữ liệu dưới dạng bảng có cấu trúc, giúp cho việc xử lý và phân tích dữ liệu trở nên dễ dàng hơn trong lĩnh vực Big Data.",https://viblo.asia/p/dataframes-trong-apache-spark-BQyJKG55JMe
67,Finetune BERT cho bài toán che thông tin cá nhân - Phần 2:  Huấn luyện và đánh giá mô hình,"Phần  1 chúng ta đã cùng nhau xác định các bước để tạo ra một mô hình xử lý bài toán NER thông qua việc finetuning BERT
Quá trình finetune BERT cho bài toán masking thông tin cá nhân bao gồm các bước sau:
Chuẩn bị dữ liệu: Tùy thuộc vào mục tiêu bài toán chúng ta sẽ tập chung vào từng bộ dữ liệu cụ thể
Chuẩn bị dữ liệu: Tùy thuộc vào mục tiêu bài toán chúng ta sẽ tập chung vào từng bộ dữ liệu cụ thể
Tiền xử lý dữ liệu: Xử lý dữ liệu văn bản để đưa vào mô hình BERT, bao gồm tokenization, padding, và chuyển đổi các thẻ nhãn thành số.
Tiền xử lý dữ liệu: Xử lý dữ liệu văn bản để đưa vào mô hình BERT, bao gồm tokenization, padding, và chuyển đổi các thẻ nhãn thành số.
Finetune BERT: Sử dụng kiến trúc BERT pre-trained và tiếp tục training mô hình trên tập dữ liệu đã chuẩn bị. Trong quá trình này, mô hình sẽ học cách nhận dạng và che dấu các thông tin cá nhân.
Finetune BERT: Sử dụng kiến trúc BERT pre-trained và tiếp tục training mô hình trên tập dữ liệu đã chuẩn bị. Trong quá trình này, mô hình sẽ học cách nhận dạng và che dấu các thông tin cá nhân.
Đánh giá và tối ưu hóa: Kiểm tra hiệu suất của mô hình trên tập dữ liệu test, và tiến hành các bước tối ưu hóa như điều chỉnh siêu tham số, thay đổi kiến trúc mô hình, hoặc thu thập thêm dữ liệu nếu cần thiết.
Đánh giá và tối ưu hóa: Kiểm tra hiệu suất của mô hình trên tập dữ liệu test, và tiến hành các bước tối ưu hóa như điều chỉnh siêu tham số, thay đổi kiến trúc mô hình, hoặc thu thập thêm dữ liệu nếu cần thiết.
Vậy thì ở phần trước chúng ta đã xử lý xong 2 bước đầu tiên, trong phần 2 này mình sẽ tiếp tục với quá trình finetune BERT và đánh giá mô hình.
Chúng ta cần định nghĩa optimizer. Ở đây, đơn giản sử dụng Adam với learning rate mặc định. Người ta cũng có thể quyết định sử dụng những cái nâng cao hơn như AdamW (Adam với bản sửa lỗi suy giảm trọng số), SGD, hoặc một learning rate schedule. Tuy nhiên để đơn giản ở đây mình chỉnh sử dụng  Adam()
optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
Xây dựng  training functions
# Defining the training function on the 80% of the dataset for tuning the bert model
def train(epoch):
    tr_loss, tr_accuracy = 0, 0
    nb_tr_examples, nb_tr_steps = 0, 0
    tr_preds, tr_labels = [], []
    # put model in training mode
    model.train()
    
    for idx, batch in enumerate(training_loader):
        
        ids = batch['ids'].to(device, dtype = torch.long)
        mask = batch['mask'].to(device, dtype = torch.long)
        targets = batch['targets'].to(device, dtype = torch.long)

        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)
        loss, tr_logits = outputs.loss, outputs.logits
        tr_loss += loss.item()

        nb_tr_steps += 1
        nb_tr_examples += targets.size(0)
        
        if idx % 100==0:
            loss_step = tr_loss/nb_tr_steps
            print(f""Training loss per 100 training steps: {loss_step}"")
           
        # compute training accuracy
        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)
        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)
        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)
        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)
        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)
        targets = torch.masked_select(flattened_targets, active_accuracy)
        predictions = torch.masked_select(flattened_predictions, active_accuracy)
        
        tr_preds.extend(predictions)
        tr_labels.extend(targets)
        
        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())
        tr_accuracy += tmp_tr_accuracy
    
        # gradient clipping
        torch.nn.utils.clip_grad_norm_(
            parameters=model.parameters(), max_norm=MAX_GRAD_NORM
        )
        
        # backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    epoch_loss = tr_loss / nb_tr_steps
    tr_accuracy = tr_accuracy / nb_tr_steps
    print(f""Training loss epoch: {epoch_loss}"")
    print(f""Training accuracy epoch: {tr_accuracy}"")
Vì là finetuning nên việc chúng ta huấn luyện quá nhiều epoch thì sẽ dẫn đến việc mô hình quá bị fit vào dữ liệu huấn luyện mà mất đi tính tổng quát của nó (pretrain model đã được huấn luyện trên 1 tập dữ liệu lớn hơn rất nhiều). Ở đây chúng ta chỉ cần huấn luyện thêm 1,2 epochs để mô hình học được những đặc trưng mới từ dữ liệu huấn luyện
for epoch in range(EPOCHS):
    print(f""Training epoch: {epoch + 1}"")
    train(epoch)
Sau khi chúng ta đã huấn luyện mô hình của mình, chúng ta có thể đánh giá hiệu suất của nó trên tập kiểm tra được giữ lại (chiếm 20% dữ liệu). Lưu ý rằng ở đây sẽ không cập nhật gradient , mô hình chỉ đơn giản đưa ra các logits của nó.
def valid(model, testing_loader):
    # put model in evaluation mode
    model.eval()
    
    eval_loss, eval_accuracy = 0, 0
    nb_eval_examples, nb_eval_steps = 0, 0
    eval_preds, eval_labels = [], []
    
    with torch.no_grad():
        for idx, batch in enumerate(testing_loader):
            
            ids = batch['ids'].to(device, dtype = torch.long)
            mask = batch['mask'].to(device, dtype = torch.long)
            targets = batch['targets'].to(device, dtype = torch.long)
            
            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)
            loss, eval_logits = outputs.loss, outputs.logits
            
            eval_loss += loss.item()

            nb_eval_steps += 1
            nb_eval_examples += targets.size(0)
        
            if idx % 100==0:
                loss_step = eval_loss/nb_eval_steps
                print(f""Validation loss per 100 evaluation steps: {loss_step}"")
              
            # compute evaluation accuracy
            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)
            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)
            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)
            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)
            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)
            targets = torch.masked_select(flattened_targets, active_accuracy)
            predictions = torch.masked_select(flattened_predictions, active_accuracy)
            
            eval_labels.extend(targets)
            eval_preds.extend(predictions)
            
            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())
            eval_accuracy += tmp_eval_accuracy
    
    #print(eval_labels)
    #print(eval_preds)

    labels = [id2label[id.item()] for id in eval_labels]
    predictions = [id2label[id.item()] for id in eval_preds]

    #print(labels)
    #print(predictions)
    
    eval_loss = eval_loss / nb_eval_steps
    eval_accuracy = eval_accuracy / nb_eval_steps
    print(f""Validation Loss: {eval_loss}"")
    print(f""Validation Accuracy: {eval_accuracy}"")

    return labels, predictions
Chúng ta có thể dễ dàng tính được
Validation Loss: 0.02922834331467394
Validation Accuracy: 0.9602300367723403
Tuy nhiên, chỉ số ""Validation Accuracy"" là không đáng tin cậy, vì nhiều nhãn là ""ngoài"" (O), thậm chí sau khi loại bỏ các dự đoán trên các token [PAD]. Điều quan trọng là xem xét độ chính xác,  giá trị f1 của từng thẻ riêng lẻ.
from seqeval.metrics import classification_report
labels, predictions = valid(model, testing_loader)
print(classification_report([labels], [predictions]))
precision    recall  f1-score   support

         geo       0.79      0.88      0.83      4613
         gpe       0.89      0.89      0.89      1523
         org       0.71      0.56      0.63      2761
         per       0.78      0.81      0.79      2183
         tim       0.82      0.81      0.81      1772

   micro avg       0.79      0.79      0.79     12852
   macro avg       0.80      0.79      0.79     12852
weighted avg       0.79      0.79      0.79     12852
Có một cánh nhanh để kiểm tra tính hiệu quả của mô hình, chúng ta có thể đưa vào một câu của tập test hoặc câu bất kỳ nào đó để kiểm tra
from transformers import pipeline

pipe = pipeline(task=""token-classification"", model=model.to(""cpu""), tokenizer=tokenizer, aggregation_strategy=""simple"")
pipe(""My name is Niels and New York is a city"")
Kết quả khá ổn khi mô hình có thể bắt được các token được đánh nhãn là 'per' hoặc 'geo', chúng ta có thể sử đánh giá trên trên một tập dữ liệu lớn để kiểm tra tính hiệu quả và xem xét những điểm cần tối ưu.
[{'end': None,
  'entity_group': 'per',
  'score': 0.71306735,
  'start': None,
  'word': 'ni'},
 {'end': None,
  'entity_group': 'per',
  'score': 0.8921481,
  'start': None,
  'word': '##els'},
 {'end': None,
  'entity_group': 'geo',
  'score': 0.95278734,
  'start': None,
  'word': 'new york'}]
Cuối cùng, hãy lưu các tệp mô hình và bộ mã hóa để chúng ta có thể dễ dàng sử dụng lại chúng sau này. Có 2 options:
Chúng ta có thể lưu tất cả cục bộ, chỉ đơn giản bằng cách gọi model.save_pretrained() và tokenizer.save_pretrained(), cung cấp đường dẫn thư mục làm đối số.
Cách khác là có thể đẩy các tệp lên HuggingFace hub. Bằng cách này, bạn có thể chia sẻ mô hình của mình với cộng đồng. Tất cả các tệp sẽ được theo dõi bởi git, vì mỗi mô hình trên hub có kho lưu trữ git riêng của nó.
Cả hai tùy chọn đều cho phép tái sử dụng mô hình/bộ mã hóa bằng cách sử dụng phương thức from_pretrained()
Các bạn có thể sử dụng đoạn code dưới đây để thực hiện cách 2 nhé
from huggingface_hub import notebook_login
notebook_login()
model_name = ""bert-finetuned-ner""

# upload files to the hub
tokenizer.push_to_hub(
    repo_path_or_name=model_name,
    organization=""nielsr"",
    commit_message=""Add tokenizer"",
    use_temp_dir=True,
)
model.push_to_hub(
    repo_path_or_name=model_name,
    organization=""nielsr"",
    commit_message=""Add model"",
    use_temp_dir=True,
)
Trong bài viết này, chúng ta đã thảo luận về cách Finetune mô hình BERT để giải quyết bài toán che thông tin cá nhân. Chúng ta đã xây dựng một pipeline từ xử lý dữ liệu, đào tạo và đánh giá mô hình, sau đó sử dụng mô hình sau khi training để thực hiện suy luận nhanh chóng trên các câu mới. Kết quả cho thấy việc Finetune BERT đạt hiệu suất ổn trên bộ dữ liệu này, với độ chính xác, F1 ở tương đối tốt. Điều này cho thấy BERT là một nền tảng mạnh mẽ có thể được điều chỉnh để giải quyết các bài toán NLP phức tạp như che thông tin cá nhân. Cũng như cách chia sẻ chúng trên HuggingFace hub để dễ dàng sử dụng lại hoặc chia sẽ với cộng đồng. Hy vọng rằng bài viết này đã cung cấp cho bạn một cái nhìn tổng quan về cách Finetune BERT cho bài toán che thông tin cá nhân. Hãy tiếp tục khám phá và sáng tạo với các kỹ thuật học máy và các bài toán thực tế nhiều hơn nữa để tạo thêm nhiều giá trị mới nhé.",https://viblo.asia/p/finetune-bert-cho-bai-toan-che-thong-tin-ca-nhan-phan-2-huan-luyen-va-danh-gia-mo-hinh-n1j4l6nGLwl
68,Sử dụng docker compose watch để tạo môi trường phát triển,"Docker compose watch là gì?
Trong docker compose, thuộc tính watch cho phép bạn có thể sử dụng docker-compose watch thay vì docker-compose up và tự động đồng bộ hóa mã nguồn cục bộ với mã trong vùng chứa Docker mà không cần sử dụng volumes.
Chúng ta sẽ thử tính năng này với project sau.
Các bạn clone project sample tại đây https://github.com/dockersamples/avatars/
git clone https://github.com/dockersamples/avatars.git
cd avatars
File compose.yaml sẽ như sau:
services:
  api:
    image: avatars-api
    build:
      context: .
      dockerfile: ./deploy/api.dockerfile
    ports:
      - 5734:80
    develop:
      watch:
        - path: api/requirements.txt
          action: rebuild
        - path: api/
          target: /app/api/
          action: sync

  web:
    image: avatars-web
    build:
      context: .
      dockerfile: ./deploy/web.dockerfile
    ports:
      - 5735:5173
    develop:
      watch:
        - path: web/package.json
          action: rebuild
        - path: web/yarn.lock
          action: rebuild
        - path: web/
          target: /app
          action: sync
Thuộc tính watch chứa danh sách các quy tắc, mỗi quy tắc chứa một đường dẫn mà chúng watch và một action được thực thi khi tệp trong đường dẫn thay đổi. Có 2 action có thể thực hiện:
sync -  Sao chép các tệp đã thay đổi vào các service container đang chạy.
rebuild - Kích hoạt image build và recreate service container đang chay.
Trong file compose.yaml, bất cứ khi nào file trong thư mục web/ được thay đổi, Compose sẽ sao chép tệp đó vào vị trí tương ứng trong /src/web bên trong container. Vì Webpack hỗ trợ Hot Module Reload nên các thay đổi sẽ tự động được phát hiện và áp dụng.
Việc thêm các thư viện mới không thể được thực hiện nhanh chóng, vì vậy bất cứ khi nào package.json được thay đổi, Compose sẽ build image và tạo lại container web.
Chạy các lệnh sau:
docker compose up -d
docker compose watch
Mở dường dẫn http://localhost:5735, ta sẽ thấy mỗi khi thay đổi file source code, các thay đổi tự động được áp dụng vào các container.
Việc sử dụng docker compose watch sẽ giúp tạo môi trường phát triển một cách nhanh chóng và thuận tiện. Nếu gặp khó khăn hãy để lại bình luận cho mình ở bên dưới.
Nguồn tham khảo: https://docs.docker.com/compose/file-watch/",https://viblo.asia/p/su-dung-docker-compose-watch-de-tao-moi-truong-phat-trien-018J2K7MLYK
69,Làm thế nào return về nhiều giá trị trong C#?,"Có bao giờ bạn thắc mắc làm thế nào để return nhiều giá trị về cùng 1 lúc trong C# như Javascript hay không. Thật ra, C# đã có tính năng này, và thậm chí có kha khá cách tiếp cận để giải quyết vấn đề này nữa là. Bây giờ bạn cùng mình, chúng ta cùng đi vào bài viết để tìm hiểu nhé.
Ví dụ chúng ta có bài toán, làm thế nào tìm ra số lớn nhất và nhỏ nhất từ list số nguyên bằng cách chỉ viết 1 hàm duy nhất? Cùng nhìn qua các cách tiếp cận dưới đây nhé.
Dùng Tuple
Đầu tiên ở JS có kỹ thuật destructuring thì C# có Tuple để làm điều tương tự. Một hàm thường chỉ return 1 kiểu giá trị thôi đúng không như với Tuple nó cho phép return nhiều hơn
SystemTuple
Vì phiên bản này dùng object và hơi dài dòng nên mình sẽ gọi nó là biến thể dài dòng Hải Phòng.
Cùng đi vào ví dụ nhé.
using System;
using System.Collections.Generic;
using System.Linq;

public class Program
{
  // return về Tuple object
  public static Tuple<int, int> FindMaxMin(List<int> numbers)
  {
    int max = numbers.Max();
    int min = numbers.Min();
    
    return Tuple.Create(max, min);
    // return new Tuple<int, int>(max, min); // như vầy cũng được nhá
  }

  public static void Main()
  {
    var numbers = new List<int>{ 1, 10, 100, 99, 44, -1, 33, 99 };
    var (max, min) = FindMaxMin(numbers);
    var t = FindMaxMin(numbers);

    Console.WriteLine($""Max = {max}, Min = {min}""); // Max = 100, Min = -1
    Console.WriteLine($""Max = {t.Item1}, Min = {t.Item2}""); // Max = 100, Min = -1
  }
}
ValueTuple
Vì nó dài dòng như vậy, nên các bác kỹ sư đã đẻ ra 1 cái cú pháp khác ngắn gọn hơn, biến thể đầu cắt moi. Ví dụ nhé.
using System;
using System.Collections.Generic;
using System.Linq;
          
public class Program
{
  // return về tuples
  public static (int max, int min) FindMaxMin(List<int> numbers)
  {
    int max = numbers.Max();
    int min = numbers.Min();
    
    return (max, min);
  }

  public static void Main()
  {
    var numbers = new List<int>{ 1, 10, 100, 99, 44, -1, 33, 99 };
    var (max, min) = FindMaxMin(numbers); // destructuring
    
    Console.WriteLine($""Max = {max}, Min = {min}""); // Max = 100, Min = -1
  }
}
Bonus cho phần này, mình muốn nói về cú pháp ngắn gọn hơn nữa, biến thể đầu trọc, nhưng bản chất là một với cái ở trên.
using System;
using System.Collections.Generic;
using System.Linq;
          
public class Program
{
  // không dùng tên đại diện nữa
  public static (int, int) FindMaxMin(List<int> numbers)
  {
    int max = numbers.Max();
    int min = numbers.Min();
    
    return (max, min);
  }
  
  public static void Main()
  {
    var numbers = new List<int>{ 1, 10, 100, 99, 44, -1, 33, 99 };
    var (max, min) = FindMaxMin(numbers); // destructuring
    var t1 = FindMaxMin(numbers);
    (int, int) t2 = FindMaxMin(numbers); // destructuring với kiểu tường minh
    (int max, int min) t3 = FindMaxMin(numbers); // destructuring với tên tường minh
    
    var t4 = (max, min); // gom lại (assignment)
    (int max, int min) t5 = (max, min); // gom lại với tên

    Console.WriteLine($""Max = {max}, Min = {min}""); // Max = 100, Min = -1
    Console.WriteLine($""Max = {t1.Item1}, Min = {t1.Item2}""); // Max = 100, Min = -1
    Console.WriteLine($""Max = {t2.Item1}, Min = {t2.Item2}""); // Max = 100, Min = -1
    
    Console.WriteLine($""Max = {t4.Item1}, Min = {t4.Item2}""); // Max = 100, Min = -1
    Console.WriteLine($""Max = {t5.max}, Min = {t5.min}""); // Max = 100, Min = -1
  }
}
Khác nhau
Phiên bản ValueTuple và SystemTuple thọat nhìn chúng ta dễ thấy rằng chúng giống nhau (kết quả nhận về), nên có thể bạn sẽ nghĩ chúng là 1 ha. Nhưng thực tình là chúng có điểm khác biệt, điểm khác biệt lớn nhất đó là ValueTuple là dạng tham trị, còn SystemTuple sẽ là dạng tham chiếu. Nên nếu đem compare, sẽ cho kết quả khác nhau. Ví dụ:
using System;
using System.Collections.Generic;
using System.Linq;
          
public class Program
{
  public static Tuple<int, int> FindMaxMin(List<int> numbers)
  {
    int max = numbers.Max();
    int min = numbers.Min();
    
    return Tuple.Create(max, min);
  }
  
  public static (int max, int min) FindMaxMinValueTuple(List<int> numbers)
  {
    int max = numbers.Max();
    int min = numbers.Min();
    
    return (max, min);
  }
  
  public static void Main()
  {
    var numbers = new List<int>{ 1, 10, 100, 99, 44, -1, 33, 99 };
    var t = FindMaxMin(numbers);
    var t1 = FindMaxMin(numbers);
    var t2 = FindMaxMinValueTuple(numbers);
    var t3 = FindMaxMinValueTuple(numbers);

    // so sánh địa chỉ object
    Console.WriteLine(t == t1); // False
    
    // so sánh giá trị
    Console.WriteLine(t2 == t3); // True
  }
}
Dùng kỹ thuật tham chiếu
Ủa mà khoan, dùng kỹ thuật tham chiếu, ý ông là dùng con trỏ á hả?
Thế quái nào trong C# lại có con trỏ? Đừng nói với tôi, ông tính truyền vào địa chỉ của biến sau đó update nó thông qua con trỏ nha, giống giống cách làm của C, C++ ấy. Uấy dà, thật ra cũng tựa tựa như vậy, nhưng không phải chính xác như vậy :v. Thật ra trong C# có cung cấp cho chúng ta các keyword, để giải quyết vấn đề này.
Đó là hai keyword ref và out, cả hai có thể truyền vào hàm thông qua tham số, riêng ref có thể kết hợp với kiểu trả về để tạo ra cách trả về địa chỉ tham chiếu (này mình sẽ nói ở phần khác). Bây giờ tiếp tục với ví dụ ở trên nhé, chúng ta sẽ giải quyết bài toán một hàm return nhiều giá trị.
ref
out
ref
Dùng ref
ref
Thêm từ khóa ref vào tham số là được.
using System;
using System.Collections.Generic;
using System.Linq;
          
public class Program
{
  public static void FindMaxMin(List<int> numbers, ref int max, ref int min)
  {
    max = numbers.Max();
    min = numbers.Min();
  }

  public static void Main()
  {
    var numbers = new List<int>{ 1, 10, 100, 99, 44, -1, 33, 99 };
    int max = 0;
    int min = 0;

    FindMaxMin(numbers, ref max, ref min);

    Console.WriteLine($""Max = {max}, Min = {min}""); // Max = 100, Min = -1
  }
}
Dùng out
out
Bằng cách thêm từ khóa out vào tham số, chúng ta có thể giải quyết được bài toán trên dễ đàng.
using System;
using System.Collections.Generic;
using System.Linq;
          
public class Program
{
  public static void FindMaxMin(List<int> numbers, out int max, out int min)
  {
    max = numbers.Max();
    min = numbers.Min();
  }

  public static void Main()
  {
    var numbers = new List<int>{ 1, 10, 100, 99, 44, -1, 33, 99 };
    int max;
    int min;
    FindMaxMin(numbers, out max, out min);

    Console.WriteLine($""Max = {max}, Min = {min}""); // Max = 100, Min = -1
  }
}
Khác nhau của ref và out
ref
out
Ủa rồi giống hệt nhau? Tại sao lại đẻ ra hai thằng cùng 1 mục đích. Why?
Thực ra có vài điểm khác nhau, khác nhau cơ bản nhất là khi dùng ref bạn phải init giá trị cho biến trước khi truyền vào, còn với out tuy không cần init giá trị trước khi truyền vào nhưng ở trong hàm thực thi, bạn phải gắn cho out 1 giá trị.
ref
out
out
Tóm lại
ref yêu cầu biến phải được khởi tạo trước khi truyền vào.
ref
out không yêu cầu khởi tạo trước, nhưng phải được gán giá trị trong hàm.
out
Qua đây chúng ta thấy một điều, để sử dụng return nhiều giá trị cùng lúc, chúng ta nên dùng out hơn là ref, vì nếu quan sát ta có thể thấy rằng ref giống như chúng ta đang binding data 2 chiều với hàm xử lý (init giá trị <-> thay đổi). Còn với out thì chỉ đơn giản thay đổi từ bên trong hàm xử lý (khởi tạo <- thay đổi), chúng ta chỉ việc nhận giá trị đã thay đổi mà thôi.
out
ref
ref
out
Dùng phương pháp đen tối (tham chiếu) - (không nên dùng)
Ở đây chúng ta sẽ lợi dụng tính tham chiếu để gán các giá trị cần return vô 1 list object, sau đó từ list object này chúng ta sẽ lấy item con ra. Xem ví dụ bên dưới nha.
using System;
using System.Collections.Generic;
using System.Linq;
          
public class Program
{
  public static void FindMaxMin(List<int> numbers, List<int> kq)
  {
    int max = numbers.Max();
    int min = numbers.Min();
    
    kq.Add(max);
    kq.Add(min);
  }

  public static void Main()
  {
    var numbers = new List<int>{ 1, 10, 100, 99, 44, -1, 33, 99 };
    var results = new List<int>();
    FindMaxMin(numbers, results);
    
    // results đang trỏ tới vùng nhớ chứ data kiểu List<int>, 
    // trong hàm FindMaxMin nhận được địa chỉ list y hệt vậy, 
    // nên  nếu trong FindMaxMin thực hiện lệnh .Add thực chất đang sửa trên cùng vùng nhớ results trỏ tới
    Console.WriteLine($""Max = {results[0]}, Min = {results[1]}""); // Max = 100, Min = -1
  }
}
Lưu ý: Để ví dụ chơi chơi thì được, chứ cách này không nên dùng tý nào, phần vì nhìn nó chuối lụi, phần vì nó vi phạm một trong bộ nguyên tắc pure function đó là ""thay đổi trạng thái bên ngoài phạm vi của nó"".
Dùng class object hoặc struct
Ở trong phạm vị section này, mình sẽ không so sánh khác biệt của class và struct nha, mà chúng ta ngầm hiểu mục đích ở đây là như nhau nhé.
Thật ra đây là cách mà chúng ta xài nhiều nhất và hầu như ai cũng đã xài, chẳng qua chừ dô bài viết, mình làm cho nó có vẻ bí ẩn tý mà thôi.
À vì sao cách này hay được xài, và nên xài lúc nào mà không xài mấy cách ở trên? Vâng, nếu dùng cách này thì phải chú ý rằng, chỉ khi nào bạn cần return quá nhiều giá trị về một lúc (tầm 4, 5 cái trở lên), thì lúc này cách này là best, không nói nhiều dùng ngay cho mình.
Cùng vô ví dụ giải quyết bài toán trên nha.
using System;
using System.Collections.Generic;
using System.Linq;

public class MaxMinResult {
  public int Max;
  public int Min;
}

//public struct MaxMinResult {
//	public int Max;
//	public int Min;
//}
          
public class Program
{
  public static MaxMinResult FindMaxMin(List<int> numbers)
  {
    return new MaxMinResult()
    {
      Max = numbers.Max(),
      Min = numbers.Min(),
    };
  }

  public static void Main()
  {
    var numbers = new List<int>{ 1, 10, 100, 99, 44, -1, 33, 99 };
    var result = FindMaxMin(numbers);

    Console.WriteLine($""Max = {result.Max}, Min = {result.Min}""); // Max = 100, Min = -1
  }
}
Thay từ khóa class bằng struct đều được nha.
class
struct
Tóm lại
Trên đây là một vài cách để giải quyết bài toán return nhiều giá trị cùng lúc trong C#. Tùy vào tình huống, bạn có thể chọn cách tiếp cận phù hợp. Theo quan điểm cá nhân, mình ưu tiên dùng out, ValueTuple, và class/struct.
Hy vọng bài viết hữu ích. Nếu thấy hay, hãy like, share và bookmark nhé. Cảm ơn bạn đã đọc!",https://viblo.asia/p/lam-the-nao-return-ve-nhieu-gia-tri-trong-c-m2vJPo3p4eK
70,Distributed Database,"1. Distributed Database

Cơ sở dữ liệu phân tán là một hệ thống lưu trữ dữ liệu trên nhiều nút trong một mạng máy tính. Điều này mang lại tính phân tán, tính nhất quán và hiệu suất cao. Hãy cùng điểm qua các thành phần và đặc điểm chính:
1.1 Thành Phần:
Nút (Nodes): Máy chủ hoặc nút trong mạng máy tính nơi dữ liệu được lưu trữ và xử lý.
Nút (Nodes): Máy chủ hoặc nút trong mạng máy tính nơi dữ liệu được lưu trữ và xử lý.
Mạng (Network): Liên kết các nút với nhau cho việc truyền thông và truy cập dữ liệu.
Mạng (Network): Liên kết các nút với nhau cho việc truyền thông và truy cập dữ liệu.
Distributed Database Management System (DDBMS): Phần mềm quản lý và điều phối truy cập và cập nhật dữ liệu trên các nút.
Distributed Database Management System (DDBMS): Phần mềm quản lý và điều phối truy cập và cập nhật dữ liệu trên các nút.
1.2 Đặc Điểm Chính:
Phân Tán: Dữ liệu được lưu trữ trên nhiều nút, giúp tăng hiệu suất và tính sẵn có.
Phân Tán: Dữ liệu được lưu trữ trên nhiều nút, giúp tăng hiệu suất và tính sẵn có.
Dự Phòng và Sao Lưu: Dữ liệu thường được sao chép trên nhiều nút để đảm bảo tính an toàn và sẵn có.
Dự Phòng và Sao Lưu: Dữ liệu thường được sao chép trên nhiều nút để đảm bảo tính an toàn và sẵn có.
Khả Năng Chịu Lỗi: Cơ sở dữ liệu phân tán có khả năng chịu lỗi cao hơn nhờ dữ liệu được sao chép trên nhiều nút.
Khả Năng Chịu Lỗi: Cơ sở dữ liệu phân tán có khả năng chịu lỗi cao hơn nhờ dữ liệu được sao chép trên nhiều nút.
Tính Nhất Quán và Đồng Nhất: Để đảm bảo dữ liệu nhất quán, hệ thống thường áp dụng các cơ chế đồng nhất.
Tính Nhất Quán và Đồng Nhất: Để đảm bảo dữ liệu nhất quán, hệ thống thường áp dụng các cơ chế đồng nhất.
Hiệu Suất: Cơ sở dữ liệu phân tán có thể mở rộng dễ dàng và tăng hiệu suất bằng cách thêm nút mới vào hệ thống.
Hiệu Suất: Cơ sở dữ liệu phân tán có thể mở rộng dễ dàng và tăng hiệu suất bằng cách thêm nút mới vào hệ thống.
Distributed Database không chỉ là một công nghệ, mà còn là một công cụ quan trọng giúp xử lý dữ liệu phức tạp và đảm bảo tính nhất quán và hiệu suất trong thế giới số hóa ngày nay.
2. Eventual Consistency

Eventual consistency là một mô hình đồng nhất trong các hệ thống phân tán, trong đó dữ liệu có thể không đồng nhất ngay lập tức sau khi các thay đổi được thực hiện, nhưng cuối cùng sẽ trở nên nhất quán trong một khoảng thời gian sau đó. Điều này có nghĩa là các thay đổi dữ liệu được thực hiện trên một nút có thể không được tức thì truyền đạt đến tất cả các nút khác trong hệ thống.
2.1 Lợi Ích và Thách Thức
Lợi Ích:

Tăng Hiệu Suất: Eventual consistency giúp tối ưu hiệu suất của hệ thống bằng cách cho phép các nút xử lý thay đổi dữ liệu một cách độc lập mà không cần phải chờ đợi sự đồng bộ ngay lập tức.
Tính Mở Rộng: Hệ thống có thể dễ dàng mở rộng bằng cách thêm nút mới mà không gặp phải các vấn đề đồng bộ hóa lớn.
Lợi Ích:
Tăng Hiệu Suất: Eventual consistency giúp tối ưu hiệu suất của hệ thống bằng cách cho phép các nút xử lý thay đổi dữ liệu một cách độc lập mà không cần phải chờ đợi sự đồng bộ ngay lập tức.
Tính Mở Rộng: Hệ thống có thể dễ dàng mở rộng bằng cách thêm nút mới mà không gặp phải các vấn đề đồng bộ hóa lớn.
Thách Thức:

Tính Nhất Quán: Trong một vài trường hợp, việc đảm bảo tính nhất quán của dữ liệu có thể trở nên phức tạp và đòi hỏi các cơ chế đồng bộ hóa phức tạp.
Xử Lý Xung Đột: Các hệ thống phải xử lý các xung đột dữ liệu khi dữ liệu không nhất quán và cần phải được hòa giải.
Thách Thức:
Tính Nhất Quán: Trong một vài trường hợp, việc đảm bảo tính nhất quán của dữ liệu có thể trở nên phức tạp và đòi hỏi các cơ chế đồng bộ hóa phức tạp.
Xử Lý Xung Đột: Các hệ thống phải xử lý các xung đột dữ liệu khi dữ liệu không nhất quán và cần phải được hòa giải.
2.2 Áp Dụng Trong Thực Tế
Eventual consistency thường được áp dụng trong các hệ thống phân tán lớn với dữ liệu có tính chất phức tạp và yêu cầu hiệu suất cao, như các hệ thống mạng xã hội, thương mại điện tử, và trò chơi trực tuyến. Trong các tình huống như vậy, tính linh hoạt của eventual consistency cho phép hệ thống mở rộng dễ dàng mà không cần phải hy sinh tính nhất quán của dữ liệu.
3. Strong Consistency

Strong consistency là một mô hình đồng nhất trong cơ sở dữ liệu phân tán, trong đó mọi truy cập đọc và ghi đều nhìn thấy dữ liệu được cập nhật gần nhất và đồng nhất trên toàn bộ hệ thống. Điều này đảm bảo rằng mọi thay đổi dữ liệu sẽ được phản ánh ngay lập tức và có thể truy cập từ bất kỳ nút nào trong hệ thống.
3.1 Lợi Ích và Thách Thức
Lợi Ích:

Tính Nhất Quán Tuyệt Đối: Mọi truy cập dữ liệu đều thấy được phiên bản mới nhất và đồng nhất của dữ liệu, giúp tránh xung đột và hiểu lỗi.
Đáng Tin Cậy: Dữ liệu luôn có tính nhất quán và tin cậy, giúp đảm bảo tính toàn vẹn của hệ thống.
Lợi Ích:
Tính Nhất Quán Tuyệt Đối: Mọi truy cập dữ liệu đều thấy được phiên bản mới nhất và đồng nhất của dữ liệu, giúp tránh xung đột và hiểu lỗi.
Đáng Tin Cậy: Dữ liệu luôn có tính nhất quán và tin cậy, giúp đảm bảo tính toàn vẹn của hệ thống.
Thách Thức:

Hiệu Suất: Strong consistency có thể gây ra độ trễ và giảm hiệu suất trong các hệ thống phân tán lớn hoặc có nhiều người dùng đồng thời.
Chi Phí: Để đạt được strong consistency, có thể cần phải sử dụng các cơ chế đồng bộ hóa phức tạp, làm tăng chi phí và phức tạp hóa hệ thống.
Thách Thức:
Hiệu Suất: Strong consistency có thể gây ra độ trễ và giảm hiệu suất trong các hệ thống phân tán lớn hoặc có nhiều người dùng đồng thời.
Chi Phí: Để đạt được strong consistency, có thể cần phải sử dụng các cơ chế đồng bộ hóa phức tạp, làm tăng chi phí và phức tạp hóa hệ thống.
3.2 Áp Dụng Trong Thực Tế
Strong consistency thường được ứng dụng trong các hệ thống nơi tính nhất quán và đáng tin cậy của dữ liệu là yếu tố quan trọng như ngân hàng trực tuyến, hệ thống đặt vé máy bay, và các ứng dụng y tế. Trong các tình huống như vậy, việc đảm bảo tính nhất quán của dữ liệu là hết sức quan trọng để đảm bảo tính toàn vẹn và đáng tin cậy của hệ thống.",https://viblo.asia/p/distributed-database-n1j4l68ALwl
71,Hệ thống lớn sẽ quản lý Cron Job như thế nào ??? - Bài học thực tế từ Slack,"Anh em đã biết đến Slack như là nền tảng làm việc hiện đại, nơi mọi người có thể giao tiếp, cộng tác và hoàn thành công việc một cách hiệu quả. Bằng cách kết hợp nhắn tin, công cụ và ứng dụng vào một không gian làm việc chung, Slack giúp các nhóm làm việc nhanh hơn, thông minh hơn và gắn kết hơn.
Một trong các thành phần quan trọng trong thiết kế của Slack đó chính là các Cron Job. Các Cron job sẽ đảm nhiệm thực thi các tác vụ quan trọng tại Slack như lập lịch và tự động gửi email, thông báo tin nhắn, nhắc lịch hẹn hay dọn dẹp các dữ liệu dư thừa.
Cùng với sự phát triển nhanh chóng của công ty và các nghiệp vụ liên quan trở lên phức tạp hơn thì số lượng các cron script và độ phức tạp các job cần chạy cũng tăng lên theo. Khiến cho việc bảo trì và quản lý việc chạy các Cron script trở lên vô cùng khó khăn, giảm độ tin cậy (reliability) của hệ thống
Cùng Sydexa tìm hiểu cách mà Slack phát triển một hệ thống ổn định hơn nha
Bạn cho chúng mình xin 1 upvote và comment để chúng mình nhận giải của viblo nha. Và chúng mình có động lực ra những bài viết thú vị hơn nữa 😄😄😄
Chúng mình có tạo Group cho các bạn cùng chia sẻ và học hỏi về thiết kế hệ thống nha 😄😄😄
Các bạn tham gia để gây dựng cộng đồng System Design Việt Nam thật lớn mạnh nhé 😍😍😍
Cộng Đồng System Design Việt Nam: https://www.facebook.com/groups/sydexa
Kênh TikTok: https://www.tiktok.com/@sydexa.com
1. Hiện trạng tại Slack lúc đó:
Hệ thống chạy các cron job của Slack gồm có 1 máy chủ đơn lẻ chứa các file scripts thực thi của các job và 1 crontab file với các thông tin về thời gian và cách lập lịch chạy cho các file script đó

Ban đầu, một máy chủ đơn lẻ này sẽ đảm nhiệm việc thực thi tất cả các tập lệnh cron tại Slack. Tuy nhiên, khi quy mô hệ thống tăng lên, giải pháp này trở nên khó duy trì và quản lý.
Một giải pháp tạm thời đã được đưa ra là mở rộng theo chiều dọc (Vertical scaling), đơn giản chỉ là tăng số lượng core CPU và Ram để node này có thể xử lý được nhiều script hơn, nhưng kéo theo đó là chi phí vận hành tăng lên.
Tuy nhiên, mô hình này vẫn tiềm ẩn nhiều rủi ro - chỉ cần node xử lý cronjob gặp sự cố thì toàn bộ dịch vụ sẽ bị đình trệ, kéo theo sự gián đoạn của một số chức năng quan trọng trên Slack.
Và họ đã đưa đến quyết định xây dựng một thiết kế khác ổn định hơn để có thể sẵn sàng cho số lượng cron job lớn hơn mà vẫn đảm bảo được sự ổn định của hệ thống
2. Giải pháp mới tăng độ ổn định một cách lâu dài:

Trên là sơ đồ kiến trúc mới để xử lý các cron job của slack, hệ thống sẽ có 3 thành phần chính:
Scheduled Job Conductor: Một service được viết bằng Golang và triển khai trên Bedrock (Một công cụ được xây dựng xung quanh K8S được phát triển nội bộ tại slack)
Job Queue: một thành phần hiện có tại Slack, phục vụ rất nhiều yêu cầu khác nhau. Đây là một nền tảng tính toán không đồng bộ, xử lý khoảng 9 tỷ tác vụ mỗi ngày và bao gồm nhiều ""hàng đợi"" khác nhau.
Vitess database table:  Giúp loại bỏ trùng lặp và giám sát công việc, nhằm tăng cường khả năng hiển thị xung quanh việc chạy và lỗi của các cron job.
Chúng ta cùng đi vào từng thành phần này để hiểu hơn bên trong đó có gì nhé
1. Scheduled Job Conductor
Service này bắt chước các tính năng của cron bằng cách sử dụng thư viện cron của ngôn ngữ lập trình Golang. Điều hay ho là thư viện này cho phép họ giữ lại được format của cron string, qua đó sẽ giảm thiểu việc thay đổi cấu trúc của crontab, đơn giản hoá quá trình migrate giảm thiểu những lỗi không đáng có.
Về triển khai thì Slack dùng một công cụ nội bộ được xây dựng bao quanh Kubernetes, giúp cho việc scale lên nhiều pods trở lên dễ dàng. Nhưng sẽ chỉ có 1 pod được dùng để xử lý các nhiệm vụ trong khi các pod còn lại sẽ ở trạng thái sẵn sàng chờ được kích hoạt và được dùng khi pod chính gặp sự cố.

Việc chọn lựa 1 pod thay vì nhiều pod cùng chạy là do nếu nhiều pod cùng chạy thì độ phực tạp sẽ tăng lên và phải xử lý xung đột giữa các pod.
Các pod chỉ đóng vai trò lên lịch còn những công việc đòi hỏi khối lượng tính toán lớn hơn sẽ được chuyển giao cho Job Queue thực hiện
2. Slack Job Queue:
Các hàng đợi này hoạt động như những đường dẫn logic, chuyển các job thông qua Kafka vào Redis - nơi lưu trữ các metadata của chúng (ví dụ: thông tin liên quan đến việc worker nào đang chạy job này). Từ Redis, các tác vụ cuối cùng được chuyển cho worker đảm nhiệm xử lý và thực thi các cron job.
Trong kiến trúc này, mỗi ""job"" tương ứng với một tập lệnh cron đơn lẻ. Job Queue có thể thực thi các tập lệnh này cực kỳ nhanh chóng nếu chúng được phân tách vào ""hàng đợi"" riêng biệt.
Nhờ tận dụng Job Queue có sẵn của Slack, các kỹ sữ đã không chỉ giảm tải được những lo ngại về tài nguyên tính toán và bộ nhớ mà còn giúp rút ngắn đáng kể thời gian xây dựng ban đầu và giảm thiểu nỗ lực bảo trì về sau – một mũi tên chúng hai đích

Vitess database table:
Cuối cùng, Slack sử dụng bảng Vitess - một cơ sở dữ liệu tương thích với MySQL có khả năng mở rộng cao trên nền tảng đám mây - để quản lý dữ liệu của các tác vụ (job). Bảng Vitess này có hai nhiệm vụ chính:
Xử lý vấn đề trùng lặp dữ liệu, đảm bảo tính chính xác và nhất quán của thông tin tác vụ.
Báo cáo và theo dõi trạng thái các tác vụ cho người dùng nội bộ, giúp họ có cái nhìn tổng quan về quá trình thực thi và phát hiện kịp thời các sự cố.

Hình ảnh trên minh họa dữ liệu trong Vitess database table
Mỗi lần thực thi một tác vụ (Job) sẽ được ghi lại dưới dạng một hàng mới trong bảng Vitess. Đồng thời, trạng thái của tác vụ cũng được cập nhật liên tục khi nó chuyển qua các giai đoạn khác nhau (được đưa vào hàng đợi, đang xử lý, hoàn thành).
Trước khi khởi động một lượt chạy mới của một tác vụ, hệ thống sẽ kiểm tra xem liệu có một phiên bản khác của tác vụ đó đang chạy hay không???. Bảng Vitess này còn đóng vai trò là nền tảng cho một trang web hiển thị thông tin thực thi của các tập lệnh cron.

Trang web này cho phép người dùng tra cứu trạng thái của các lần chạy tập lệnh và bất kỳ lỗi nào mà hệ thống gặp phải.
Và đó là hành trình Slack tạo ra một hệ thống mới giúp thực thi các cronjob một cách ổn định hơn rất nhiều, có khả năng mở rộng tốt hơn. Hệ thống này sẵn sàng cho rất nhiều cơ hội phát triển những tính năng phức tạp trong tương lai.
Nếu thấy bài viết này hay thì cho chúng mình xin 1 upvote và comment để chúng mình nhận giải của viblo nha 😄😄😄
Lời nhắn
Chúng mình có tạo Group cho các bạn cùng chia sẻ và học hỏi về thiết kế hệ thống nha 😄😄😄
Các bạn tham gia để gây dựng cộng đồng System Design Việt Nam thật lớn mạnh nhé 😍😍😍
Cộng Đồng System Design Việt Nam: https://www.facebook.com/groups/sydexa
Kênh TikTok: https://www.tiktok.com/@sydexa.com
Bạn có thể tìm hiểu thêm:
Chia sẻ từ chính Slack: https://slack.engineering/executing-cron-scripts-reliably-at-scale/
Bedrock: https://slack.engineering/applying-product-thinking-to-slacks-internal-compute-platform/
Slack’s Job Queue: https://slack.engineering/scaling-slacks-job-queue/
Vitess: https://vitess.io/",https://viblo.asia/p/he-thong-lon-se-quan-ly-cron-job-nhu-the-nao-bai-hoc-thuc-te-tu-slack-GyZJZjRkJjm
72,Cách sử dụng Codepush trong React Native - Android,"Trong quá trình phát triển một ứng dụng, đôi khi sẽ có một vài cập nhật bắt buộc bạn phải chỉnh sửa lại code trên app, nhưng mỗi lần như vậy bạn phải tiến hành build lại app và upload bản release đó lên trên store. Như vậy sẽ làm mất thời gian của dev cũng như người dùng lại phải bấm nút cập nhật để tải bản mới nhất, điều này mang lại trải nghiệm không hay cho người dùng. Codepush sinh ra để giải quyết vấn đề này
Cơ chế hoạt động của Codepush có thể hiểu đơn giản như sau, mỗi lần người dùng mở app, đầu tiên Codepush sẽ kiểm tra trên server xem app có bản cập nhật mới nào không, nếu có thì nó sẽ tự động kéo file bundle mới về app và cài đặt
Nhược điểm của codepush là chỉ áp dụng được đối với những thay đổi trên code Javascript, nếu như bạn có thay đổi nào đó trên Native code (Java hay ObjectiveC, Swift) thì Codepush không hoạt động. Bài viết này mình sẽ hướng dẫn các bạn cài đặt và sử dụng Codepush đổi với React Native trên Android.  Và giờ bắt đầu thôi nào
1. AppCenter
Appcenter là một dịch vụ được cung cấp và phát triển bởi Microsoft. Nó cho phép người dùng quản lý, giám sát các ứng dụng di động, tích hợp CI/CD để phân phối app và còn nhiều chức năng hữu ích khác, trong phạm vi bài viết này chúng ta chỉ tập trung nghiên cứu vào Codepush
Bạn cần phải tạo một tài khoản AppCenter sau đó đăng nhập vào trên web


Cài đặt AppCenter-cli trên máy tính bằng lệnh npm install -g appcenter-cli
npm install -g appcenter-cli
Trên Visual Studio Code, mở Terminal, liên kết với AppCenter bằng lệnh appcenter login
appcenter login
Lúc này trình duyệt trên máy bạn sẽ mở lên, bạn copy dòng token này và dán vào terminal là được

Trên web AppCenter, tiến hành tạo 1 dự án cho Android:
Điền các thông tin như tên app, nền tảng, icon cho app,...
2. Cấu hình react-native-codepush trên Android
Cài đặt codepush-cli bằng lệnh
npm install -g appcenter-cli
npm install -g appcenter-cli
Trong dự án React Native của các bạn, cài đặt thư viện react-native-codepush:
npm i react-native-code-push
npm i react-native-code-push
Thường từ bản React Native 0.70, các dependency khi cài vào thì nó sẽ tự linking. Nhưng ở đây mình cũng sẽ hướng dẫn cách Link bằng cơm, hoặc tham khảo link sau: https://github.com/microsoft/react-native-code-push/blob/master/docs/setup-android.md
Vào file setting.gradle, thêm 2 dòng này vào
include ':app', ':react-native-code-push' 
project(':react-native-code-push').projectDir = new File(rootProject.projectDir, '../node_modules/react-native-code-push/android/app')
Trong file android/app/build.gradle thêm dòng sau
apply from: '../../node_modules/react-native/react.gradle' 
apply from: '../../node_modules/react-native-code-push/android/codepush.gradle' // thêm dòng này vào vị trí này nhé các bạn
Trong file MainApplication.java thêm các dòng sau
import com.microsoft.codepush.react.CodePush; // dòng này
				
public class MainApplication extends Application implements ReactApplication {
    private final ReactNativeHost mReactNativeHost = new ReactNativeHost(this) {
    @Override
        protected String getJSBundleFile() {
            return CodePush.getJSBundleFile(); // dòng này
        }
    };
}
Trong file strings.xml (android\app\src\main\res\values\strings.xml) thêm dòng sau:
<resources>
    <string moduleConfig=""true"" name=""CodePushDeploymentKey"">{DeploymentKey}</string>
</resources>
Trong đó DeploymentKey chính là key lấy bằng cách sau:  vào Web AppCenter vào Distribute/CodePush sau đó chọn vào các nút được khoanh đỏ dưới đây
DeploymentKey
Lấy code tùy vào phiên bản rồi dán vào DeploymentKey ở trên là được, ở đây mình test trên bản Beta nên tự tạo ra bản Beta rồi lấy key Beta
3. Thêm vào trong code React Native
Trong màn hình nào cần gọi CodePush (thường là màn hình Splash) thì thêm code như sau:
import CodePush from ""react-native-code-push"";

const codePushOptions = {
    updateDialog: true,
    installMode: CodePush.InstallMode.IMMEDIATE,
    checkFrequency: CodePush.CheckFrequency.ON_APP_RESUME,
};

useEffect(() => {
    CodePush.sync({
        installMode: CodePush.InstallMode.IMMEDIATE,
    });
}, []);

export default CodePush(codePushOptions)(Splash);
Lúc này mỗi khi user mở app, vào tới màn Splash thì Codepush sẽ kiểm tra bản cập nhật từ trên AppCenter và tiến hành kéo về app, installMode ở đây là hành vi sau khi app cài đặt xong bundle mới, có thể là restart ngay lập tức hoặc là restart sau v.v..., ở đây chúng ta set  IMMEDIATE có nghĩa là nó sẽ cài đặt lại bundle mới nhất và restart app ngay lập tức.
IMMEDIATE
Thế là chúng ta đã xong phần thiết lập Codepush trên app
4. Codepush lên AppCenter
Chúng ta sẽ tiến hành đưa bản cập nhật đầu tiên lên trên AppCenter bằng cách gõ lệnh theo cú pháp sau
code-push release-react <tên app trên app center> android -d <tên phiên bản (Production|Staging|Beta|...)> -t '<số phiên bản>'
Nếu không biết tên app thì gõ lệnhappcenter apps list
appcenter apps list
Thiết lập phiên bản app ở file android/app/build.gradle
Ở đây chúng ta sẽ đưa phiên bản 1.0 lên AppCenter, gõ lệnh
code-push release-react Bell android -d Beta -t '1.0'
Chờ cho nó chạy và thông báo thế này là ok (nếu bị lỗi EPERM: operation not permitted, lstat thì tắt VS Code và mở lại bằng quyền Administrator)

Lên AppCenter kiểm tra (nhớ chọn đúng tên phiên bản ở góc trên bên phải)
Trước khi CodePush thì chưa có gì, sau khi CodePush thành công thì AppCenter sẽ hiển thị như sau
5. Kiểm thử
Mình sẽ build app ra file apk và cài đặt trên máy ảo để tiến hành kiểm thử. Ở đây app của mình sẽ có một màn hình chứa một tấm ảnh cái chuông và một button
Sau đó vào VSCode, chỉnh cho cái nút thành màu cam, sau đó chạy lệnh codepush lại để nó up bản cập nhật mới này lên AppCenter
code-push release-react bell android -d Beta -t '1.0'
Tắt App và mở App lại, nếu chưa có cập nhật thì tắt app chờ khoảng 30s đến 1 phút rồi mở app lại. Kết quả app đã được cập nhật mà không cần cài đặt lại
Bạn có thể kiểm tra api logger, có thể thấy có api được gọi lên AppCenter để kiểm tra Codepush kiểu vầy
Trên đây là những chia sẻ của mình về cách sử dụng Codepush trên app React Native dành cho Android. Hi vọng bài viết của mình sẽ giúp ích được cho các bạn trong quá trình xây dựng app. Cảm ơn các bạn đã theo dõi.",https://viblo.asia/p/cach-su-dung-codepush-trong-react-native-android-MG24BkgOJz3
73,Làm chủ không gian với InstanceDiffusion: Định nghĩa lại sự tự do cho các mô hình sinh ảnh?,"Mình xin phép giữ nguyên tên tiếng Anh của một vài từ mà mình không tìm ra từ tiếng Việt nào có thể lột tả hết được ý nghĩa.
Những text-to-image diffusion model hiện có gặp khó khăn trong việc sinh ra hình ảnh theo điều kiện/dense caption, mà ở đó text prompt mô tả chi tiết vật thể nào ở vị trí nào.
Đó là lý do mà cũng đã có 1 số lượng tương đối các nhà nghiên cứu cho ra mắt các layout+text-to-image diffusion model, nổi bật như SpaText, GLIGEN, ControlNet và DenseDiffusion... Tuy nhiên tất cả những model đó đều chỉ tận dụng một kiểu layout (hoặc là bounding boxes, hoặc là mask) trong 1 lần inference. Các bạn có thể kiểm tra ở mục tham khảo.
Câu hỏi đặt ra là liệu có phương pháp hay model nào không bị hạn chế bởi định dạng layout, và thực sự chạm đến tự do không? Câu trả lời nằm ở InstanceDiffusion - SOTA - CVPR 2024.

Tác giả cải thiện khả năng sinh ảnh theo điều kiện bằng cách tận dụng cả hai loại thông tin, text và location. Có thể viết model của ở dạng f(cg, {(c1, l1), ..., (cn, ln)}) với input là global text caption cg và (caption ci , location li )(ci, li) cho n điều kiện.

Lý do: Chắc chắn trong bài toán này, chúng ta nhất định phải có cách để chiếu các thông tin khác nhau về cùng một không gian và kết hợp chúng lại. UniFusion sẽ giúp chúng ta xử lý điều đó một cách hiệu quả.
UniFusion chiếu các loại điều kiện có định dạng khác nhau về cùng một không gian, đồng thời, tích hợp thông tin về location, caption với thông tin visual từ diffusion backbone.

Một điều cần chú ý nữa là UniFusion được tích hợp giữa 2 lớp self-attention và cross-attention.
a. Tham số hóa vị trí cho các điều kiện khác nhau (Location parameterization)

Tác giả chuyển đổi:
Instance masks —> một tập hợp các điểm trong mask.
Nét viết (scribble) —> Một tập hợp các điểm đều nhau.
Bounding boxes —> Điểm trên cùng bên trái và dưới cùng bên phải.
b. Mã hóa điều kiện (Instance Tokenizer)
Tác giả chuyển đổi tọa độ điểm 2D pi cho các vị trí sử dụng Fourier mapping γ(·) và mã hóa caption ci sử dụng bộ CLIP text encoder τθ(·).

Với mỗi định dạng điều kiện, tác giả dùng mạng MLP khác nhau, vì thế với 1 điều kiện i, chúng ta có
gscribblei, gboxi, gpointi, and gmaski.
Tuy nhiên nếu 1 điều kiện được thể hiện bởi 1 định dạng, tác giả sẽ dùng learnable null token ei cho những định dạng còn lại.
s là giá trị nhị phân thể hiện sự xuất hiện của 1 định dạng cụ thể.
c. Instance-Masked Attention và cơ chế kết hợp
Tác giả biểu thị các instance condition tokens cho từng vị trí nhưng cùng thuộc 1 định dạng là g, cho tất cả vị trí là G, và từng visual tokens là v, tất cả là V.
Tác giả quan sát thấy rằng vanilla self-attention dẫn tới việc rò rỉ thông tin của các điều kiện, (màu sắc của điều kiện này lại được thể hiện bởi điều kiện khác). Vì thế họ xây dựng mask M giúp tránh việc rò rỉ thông tin, việc này cũng khá tương tự với cơ chế masked attention của transformer.

Ivk = i nếu visual tokens vk thuộc cùng 1 vị trí với điều kiện i.
w là learnable parameter, khởi tạo bằng 0, mục đích là điều khiển tầm ảnh hưởng của điều kiện.
ScaleU giúp tinh chỉnh skip connection của UNet, có 2 learnable scaling vector: SbS_bSb​ cho feature chính và SsS_sSs​ cho skip-connected features.
FbF_bFb​ được scaled bởi một phép nhân theo channel:
Fb′=Fb⊗(tanh⁡(Sb)+1)F'_b = F_b \otimes (\tanh(S_b) + 1)Fb′​=Fb​⊗(tanh(Sb​)+1).
Đối với skip-connection features, họ chọn ra những feature có tần số xuất hiện thấp (ít hơn rthreshr_{\text{thresh}}rthresh​)  sử dụng frequency mask α\alphaα và scale chúng trong Fourier domain:
Fs′=IFFT(FFT(Fs)⊙α)F'_s = \text{IFFT}(\text{FFT}(F_s) \odot \alpha)Fs′​=IFFT(FFT(Fs​)⊙α).
FFT(.) là phép biến đổi Fast-Fourier và IFFT(.) là phép biến đổi Inverse-Fast-Fourier , ⊙\odot⊙ là phép nhân theo phần tử, và
α(r)=tanh⁡(Ss)+1\alpha(r) = \tanh(S_s)+1α(r)=tanh(Ss​)+1 if r<rthreshr < r_{\text{thresh}}r<rthresh​ otherwise =1=1=1,
rrr biểu thị tần số, và rthreshr_{\text{thresh}}rthresh​ là một ngưỡng. SbS_bSb​ và SsS_sSs​ đều được khởi tạo là vectors 0.
Tác giả cũng giới thiệu một cách nữa để tránh rò rỉ thông tin, đó là chiến lược Multi-instance Sampler trong khi inference.
Với mỗi nnn điều kiện, chúng ta sẽ có sự khử nhiễu riêng biệt trong MMM bước (ít hơn 10% của tổng số bước) để có được** latent instance embedding **LiL_iLi​.
Với mỗi nnn điều kiện, chúng ta sẽ có sự khử nhiễu riêng biệt trong MMM bước (ít hơn 10% của tổng số bước) để có được** latent instance embedding **LiL_iLi​.
Kết hợp denoised instance latents {L1,…,Ln}\{L_1, \ldots, L_n\}{L1​,…,Ln​} với global latent LGL_GLG​ bằng việc tính trung bình tất cả latents.
Kết hợp denoised instance latents {L1,…,Ln}\{L_1, \ldots, L_n\}{L1​,…,Ln​} với global latent LGL_GLG​ bằng việc tính trung bình tất cả latents.
Khử nhiễu với đầu vào là aggregated latent từ bước (2) trong T-M bước, tận dụng tất cả instance tokens và text prompts.
Khử nhiễu với đầu vào là aggregated latent từ bước (2) trong T-M bước, tận dụng tất cả instance tokens và text prompts.
Có được một dataset với domain lớn cho bài toán này là rất khó. Vậy nên để có được nhiều thông tin chi tiết về điều kiện (diều hâu) và đặc trưng của điều kiện (mắt của diều hâu), tác giả xây dựng bộ dữ liệu dựa vào nhiều model.
Image-level label generation: Tác giả dùng RAM, một model gắn thẻ (tagging) hình ảnh mạnh mẽ với vốn từ vựng mở, để tạo ra danh sách những thẻ (tag) cho hình ảnh.
Image-level label generation: Tác giả dùng RAM, một model gắn thẻ (tagging) hình ảnh mạnh mẽ với vốn từ vựng mở, để tạo ra danh sách những thẻ (tag) cho hình ảnh.
Bounding-box and mask generation: Tác giả dùng Grounded-SAM để có được bounding boxes và masks tương ứng với những thẻ. Các thẻ này có thể ở mức độ điều kiện hoặc đặc trưng của điều kiện.
Bounding-box and mask generation: Tác giả dùng Grounded-SAM để có được bounding boxes và masks tương ứng với những thẻ. Các thẻ này có thể ở mức độ điều kiện hoặc đặc trưng của điều kiện.
Instance-level text prompt generation: Để sinh ra văn bản có thể mô tả từng điều kiện, họ cắt điều kiện dựa vào tọa bộ bounding boxes và tạo ra captions cho những ảnh cắt đó sử dụng Vision-Language Model BLIP-V2.
Instance-level text prompt generation: Để sinh ra văn bản có thể mô tả từng điều kiện, họ cắt điều kiện dựa vào tọa bộ bounding boxes và tạo ra captions cho những ảnh cắt đó sử dụng Vision-Language Model BLIP-V2.
Dưới đây là phương pháp mà họ dùng để đo độ chính xác mà vật thể được tạo ta gắn với những định dạng location khác nhau
Bounding box:
So sánh bounding boxes mà model phát hiện được trên generated image với bounding boxes có sẵn từ input sử dụng phương pháp đánh giá chính thức của COCO (AP và AR).
Instance mask:
So sánh predicted mask trên generated image dùng YOLOv8m-Seg với masks có sẵn từ đầu vào bằng chỉ số IOU.
Scribble:
Do trước đây chưa xuất hiện phương pháp đánh giá cho Scribble nên họ đề xuất một chỉ số đánh giá mới sử dụng YOLOv8m-Seg. Đó là “Points in Mask” (PiM), đo lường có bao nhiêu điểm ngẫu nhiên trên input scribble nằm trong predicted mask.
Single-point:
Giống với scribble, độ chính xác của PiM là 1 nếu điểm này nằm trong predicted mask, 0 cho trường hợp còn lại. Sau đó họ tính trung bình điểm PiM.



Còn một vài kĩ thuật, thiết kế nữa cũng rất hay nhưng mình không nêu ở đây vì đó là gợi ý của tác giả. Các bạn có thể đọc thêm tại paper gốc nhé, mình sẽ gắn link ở phần tham khảo. Xin cảm ơn và hẹn gặp lại các bạn trong những bài tiếp theo.
GliGen: https://gligen.github.io/
SpaText: https://omriavrahami.com/spatext/
DenseDiffusion: https://arxiv.org/pdf/2308.12964
InstanceDiffusion: https://arxiv.org/pdf/2402.03290
🔗 Tìm hiểu thêm về Pixta Vietnam: http://bit.ly/3kdkzvW",https://viblo.asia/p/lam-chu-khong-gian-voi-instancediffusion-dinh-nghia-lai-su-tu-do-cho-cac-mo-hinh-sinh-anh-EbNVQw31JvR
74,Chế độ Doze và App Standby trong lập trình Android quan trọng như thế nào?,"Chế độ Doze và App Standby là hai tính năng quan trọng được giới thiệu trong Android 6.0 Marshmallow để cải thiện tuổi thọ pin của thiết bị. Chúng giúp hạn chế hoạt động của các ứng dụng khi thiết bị không được sử dụng, qua đó tiết kiệm pin.

Chế độ Doze là một tính năng hệ thống giúp giảm mức tiêu thụ điện năng khi thiết bị di động của bạn đang ở trạng thái nghỉ quá lâu mà không hoạt động. Khi kích hoạt, chế độ Doze sẽ làm giảm các hoạt động mạng và hoạt động của CPU cho các ứng dụng đang chạy. Tuy nhiên, nó vẫn cho phép các ứng dụng hoạt động trong những khoảng thời gian ngắn gọi là ""giai đoạn duy trì"".
App Standby, mặt khác, là một tính năng ứng dụng cụ thể. Nó giới hạn hoạt động của một ứng dụng khi ứng dụng đó không được sử dụng trong một thời gian dài. Khi một ứng dụng ở chế độ App Standby, nó sẽ không nhận được các bất cứ dữ liệu mạng và không thực hiện các công việc nền. Tuy nhiên, ứng dụng vẫn có thể hiển thị thông báo và đăng ký nhận các sự kiện như cuộc gọi thoại.
Cả hai tính năng này đều nhằm mục đích tối ưu hóa tuổi thọ pin của thiết bị Android. Tuy nhiên, chúng hoạt động với các cơ chế khác nhau và ảnh hưởng đến các ứng dụng theo cách riêng.
Dưới đây là một ví dụ demo bằng Android Kotlin để thực hiện một số tác vụ liên quan đến Chế độ Doze và App Standby:
import android.content.Context
import android.os.Build
import android.os.Bundle
import androidx.appcompat.app.AppCompatActivity
import kotlinx.android.synthetic.main.activity_main.*

class MainActivity : AppCompatActivity() {

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)

        // Kiểm tra xem thiết bị có hỗ trợ Chế độ Doze không
        val isDozeSupported = Build.VERSION.SDK_INT >= Build.VERSION_CODES.N

        // Kiểm tra xem ứng dụng có ở chế độ App Standby không
        val isAppStandby = isAppStandby(this)

        // Hiển thị thông tin trạng thái
        statusTextView.text = ""Doze Supported: $isDozeSupported\nApp Standby: $isAppStandby""
    }

    private fun isAppStandby(context: Context): Boolean {
        if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.M) {
            val powerManager = context.getSystemService(Context.POWER_SERVICE) as android.os.PowerManager
            return powerManager.isDeviceIdleMode
        }
        return false
    }
}
Trong ví dụ trên, chúng ta sử dụng các phương thức và thuộc tính của Android SDK để kiểm tra xem thiết bị có hỗ trợ Chế độ Doze hay không và xem ứng dụng có ở chế độ App Standby hay không. Sau đó, chúng ta hiển thị thông tin đó trên màn hình.
Phân tích
Cả Chế độ Doze và App Standby đều là những tính năng quan trọng để quản lý và tối ưu hóa tuổi thọ pin trên thiết bị Android. Tuy nhiên, chúng hoạt động với các cơ chế khác nhau và ảnh hưởng đến các ứng dụng theo những cách riêng biệt.
Chế độ Doze là một tính năng hệ thống được giới thiệu trong Android 6.0 Marshmallow. Nó được kích hoạt khi thiết bị của bạn ở trạng thái nghỉ, không di chuyển và không được cắm sạc trong một khoảng thời gian nhất định. Khi Chế độ Doze được bật, hệ thống sẽ hạn chế hoạt động của các ứng dụng bằng cách ngăn chặn chúng truy cập mạng và thực hiện các công việc nền. Tuy nhiên, Doze vẫn cho phép các ứng dụng hoạt động trong những khoảng thời gian ngắn gọi là ""giai đoạn duy trì"" (maintenance windows).
Trong giai đoạn duy trì, các ứng dụng có thể thực hiện các tác vụ như đồng bộ hóa dữ liệu, tải xuống nội dung và xử lý công việc nền. Ngoài ra, các ứng dụng vẫn có thể nhận được thông báo và đăng ký nhận các sự kiện quan trọng như cuộc gọi thoại. Giai đoạn duy trì xảy ra định kỳ, thường là mỗi vài giờ một lần, và kéo dài trong một khoảng thời gian ngắn.
Ngoài việc hạn chế hoạt động của các ứng dụng, Chế độ Doze cũng có thể giới hạn hoạt động của hệ thống bằng cách làm chậm tần suất cập nhật vị trí, đóng kết nối Wi-Fi và Bluetooth, và hạn chế các tác vụ nền khác.
App Standby, mặt khác, là một tính năng ứng dụng cụ thể được giới thiệu cùng với Chế độ Doze trong Android 6.0 Marshmallow. Nó được thiết kế để giới hạn hoạt động của các ứng dụng khi chúng không được sử dụng trong một thời gian dài.
Khi một ứng dụng ở chế độ App Standby, nó sẽ không nhận được bất kỳ dữ liệu mạng nào và không thực hiện các công việc nền. Tuy nhiên, ứng dụng vẫn có thể hiển thị thông báo và đăng ký nhận các sự kiện như cuộc gọi thoại, tin nhắn và đồng bộ hóa lịch.
App Standby được kích hoạt sau một khoảng thời gian nhất định khi ứng dụng không được sử dụng. Thời gian này có thể được điều chỉnh bởi người dùng hoặc do nhà sản xuất thiết bị quy định. Nếu người dùng mở ứng dụng, App Standby sẽ tự động tắt để cho phép ứng dụng hoạt động bình thường.
Cả Chế độ Doze và App Standby đều có thể được vô hiệu hóa bởi người dùng hoặc bởi các ứng dụng có quyền đặc biệt. Tuy nhiên, điều này có thể ảnh hưởng đáng kể đến tuổi thọ pin của thiết bị.
Để đạt được hiệu quả tối ưu về tiết kiệm pin, các nhà phát triển ứng dụng nên tuân thủ các hướng dẫn và khuyến nghị của Google về việc tương tác với Chế độ Doze và App Standby. Điều này bao gồm việc:
Sử dụng các API quản lý công việc nền (JobScheduler hoặc WorkManager) để đảm bảo rằng các công việc nền của ứng dụng được thực hiện hiệu quả và không gây ra tình trạng tiêu thụ pin không cần thiết.
Sử dụng các API quản lý công việc nền (JobScheduler hoặc WorkManager) để đảm bảo rằng các công việc nền của ứng dụng được thực hiện hiệu quả và không gây ra tình trạng tiêu thụ pin không cần thiết.
Tận dụng các giai đoạn duy trì của Chế độ Doze để thực hiện các tác vụ đồng bộ hóa và tải xuống dữ liệu.
Tận dụng các giai đoạn duy trì của Chế độ Doze để thực hiện các tác vụ đồng bộ hóa và tải xuống dữ liệu.
Tối ưu hóa việc sử dụng GPS, Wi-Fi và Bluetooth để giảm thiểu tiêu thụ pin khi ứng dụng không được sử dụng.
Tối ưu hóa việc sử dụng GPS, Wi-Fi và Bluetooth để giảm thiểu tiêu thụ pin khi ứng dụng không được sử dụng.
Sử dụng các thông báo thông minh và có ý nghĩa để đảm bảo rằng ứng dụng vẫn có thể giao tiếp với người dùng ngay cả khi ở chế độ App Standby.
Sử dụng các thông báo thông minh và có ý nghĩa để đảm bảo rằng ứng dụng vẫn có thể giao tiếp với người dùng ngay cả khi ở chế độ App Standby.
Cung cấp cho người dùng các tùy chọn để điều chỉnh cách ứng dụng tương tác với Chế độ Doze và App Standby.
Cung cấp cho người dùng các tùy chọn để điều chỉnh cách ứng dụng tương tác với Chế độ Doze và App Standby.
Ngoài việc tuân thủ các hướng dẫn của Google, các nhà phát triển cũng cần hiểu rõ cách thức hoạt động của Chế độ Doze và App Standby để tối ưu hóa hiệu suất và trải nghiệm người dùng của ứng dụng.
Chế độ Doze hoạt động dựa trên một số điều kiện nhất định. Trước tiên, thiết bị phải ở trạng thái nghỉ, có nghĩa là màn hình phải tắt và không có hoạt động của người dùng trong một khoảng thời gian nhất định (thường là 30 phút). Thứ hai, thiết bị phải không di chuyển, được xác định bằng cảm biến chuyển động. Cuối cùng, thiết bị không được cắm sạc.
Khi tất cả các điều kiện này được đáp ứng, Chế độ Doze sẽ được kích hoạt và hệ thống sẽ bắt đầu hạn chế hoạt động của các ứng dụng. Trong giai đoạn đầu tiên của Chế độ Doze, hệ thống sẽ ngăn chặn các ứng dụng truy cập mạng và thực hiện công việc nền. Tuy nhiên, các ứng dụng vẫn có thể nhận được thông báo và đăng ký nhận các sự kiện quan trọng như cuộc gọi thoại.
Sau một khoảng thời gian nhất định (thường là vài tiếng), Chế độ Doze sẽ chuyển sang giai đoạn thứ hai, gọi là ""Doze nâng cao"". Trong giai đoạn này, hệ thống sẽ hạn chế hoạt động của các ứng dụng một cách nghiêm ngặt hơn, bao gồm việc ngăn chắn các thông báo và sự kiện quan trọng. Tuy nhiên, các ứng dụng vẫn có thể hoạt động trong các giai đoạn duy trì định kỳ.
Về phần App Standby, nó cũng được kích hoạt dựa trên một số điều kiện nhất định. Khi một ứng dụng không được sử dụng trong một khoảng thời gian nhất định (thường là từ 2-3 ngày), hệ thống sẽ đưa ứng dụng đó vào chế độ App Standby. Trong chế độ này, ứng dụng sẽ không nhận được dữ liệu mạng và không thực hiện công việc nền.
Tuy nhiên, ứng dụng vẫn có thể nhận được thông báo và đăng ký nhận các sự kiện quan trọng như cuộc gọi thoại, tin nhắn và đồng bộ hóa lịch. Nếu người dùng mở ứng dụng, App Standby sẽ tự động tắt và ứng dụng sẽ hoạt động bình thường.
Cả Chế độ Doze và App Standby đều có thể được điều chỉnh hoặc vô hiệu hóa bởi người dùng hoặc bởi các ứng dụng có quyền đặc biệt. Trong trường hợp vô hiệu hóa, các ứng dụng sẽ hoạt động như bình thường nhưng có thể ảnh hưởng đáng kể đến tuổi thọ pin của thiết bị.
Để giúp người dùng quản lý các tính năng này, Android cung cấp một số tùy chọn trong phần cài đặt Pin và Tuổi thọ Pin. Người dùng có thể xem các ứng dụng đang sử dụng nhiều pin nhất và có thể chọn vô hiệu hóa Chế độ Doze hoặc App Standby cho các ứng dụng cụ thể.
Ngoài ra, các nhà sản xuất thiết bị cũng có thể tùy chỉnh hoặc bổ sung các tính năng của riêng họ để quản lý và tối ưu hóa tuổi thọ pin trên thiết bị của mình. Ví dụ, Samsung có tính năng ""Chế độ tiết kiệm pin"" riêng, trong khi Huawei có ""Quản lý pin thông minh"".
Chế độ Doze và App Standby là những tính năng quan trọng trong Android để giúp kéo dài tuổi thọ pin của thiết bị. Chúng hoạt động dựa trên các điều kiện nhất định và có thể được điều chỉnh hoặc vô hiệu hóa bởi người dùng hoặc các ứng dụng có quyền đặc biệt. Để đạt được hiệu quả tối ưu, các nhà phát triển cần tuân thủ các hướng dẫn của Google và tối ưu hóa ứng dụng của mình để tương tác tốt với các tính năng này.",https://viblo.asia/p/che-do-doze-va-app-standby-trong-lap-trinh-android-quan-trong-nhu-the-nao-MG24BkM5Jz3
75,TOP 10 REPOSITORIES GITHUB HAY NHẤT ĐỂ HỌC SECURITY (Part 1),"Số liệu được updated ngày 21/05/2024
Top 1. Hacker Scripts - 47.2k ⭐️
Mặc dù đây không phải repo nhiều sao nhất về chủ đề Security, nhưng cá nhân mình thì thấy nó hấp dẫn nhất, nên mình xếp nó lên đầu. Các bạn có thể truy cập nó và so sánh với những repo phía sau, xem có suy nghĩ giống như mình không nhé 🙌
Đập vào mắt phần README đọc đã thấy hấp dẫn rồi. Repo này dựa trên một câu chuyện có thật của một anh hacker Nga ""không ngố"" 🇷🇺🤪
Anh này đã nghỉ việc ở công ty X này để chuyển sang một công ty khác. Theo mô tả của đồng nghiệp cũ thì anh này là một người yêu thích sử dụng Vim, thích vẽ diagram bằng DOT (một graph description language, các bạn tò mò có thể lên wiki xem thử nhé), ... Nói chung là bất kỳ việc gì lãng phí nhiều thời gian, anh ta sẽ đều tìm cách viết script để tự động hóa nó 👨‍💻
Một số ""di sản"" mà anh ấy để lại có thể kể đến như:
Script tự động gửi tin nhắn báo cho vợ ""tối nay anh về trễ"" với 1 lý do ngẫu nhiên trong mảng có sẵn.
Script tự động quét email mà 1 ông làm vị trí Database Administrator của công ty khách hàng gửi đến (ông này tên là Kumar, nên file anh ấy đặt có tên là kumar-4sshole), tìm kiếm các tự khóa như ""help"", ""trouble"", ""sorry"", ... Nếu tìm thấy các từ khóa trên thì tự động SSH vào client server và rollback bản backup DB gần nhất của họ. Sau đó tự động reply là no worries mate, be careful next time"" - ""đừng lo bạn hiền, lần sau cẩn thận nhé"" 😏
Hay một cron-job khác được thiết lập để vào những ngày được chỉ định sẽ tự động gửi tin nhắn đến sếp với lý do ""nay em không khỏe lắm, sếp cho em work from home nhé"", hoặc ""nay em ngộ độc thức ăn, sếp cho em xin nghỉ một hôm nhé"", ... Muốn bao nhiêu lý do khác nhau thì cứ soạn sẵn vào 1 mảng rồi random.
Đặc sắc hơn nữa là anh này phát hiện ra máy pha cà phê có thể kết nối internet, chạy bằng Linux. Anh ấy mở 1 telnet session tới máy pha cà phê, tự động pha 1 nửa cốc latte cỡ vừa, đợi thêm 24 giây trước khi đổ nó vào cốc - vừa zin với khoảng thời gian anh ấy đi từ bàn làm việc đến chỗ máy pha cà phê.
Đọc xong lại nhớ tới câu nói của Bill Gates: ""Tôi luôn chọn những người lười biếng cho những công việc khó khăn. Bởi vì họ luôn biết cách tìm ra những con đường dễ dàng nhất để thực hiện nó""
Về chuyện thật giả của những câu chuyện trên thì chưa có gì kiểm chứng. Nhưng những file script thực hiện những việc trên thì có ở trong repo này thật, các bạn có thể truy cập theo đường link GitHub dưới đây để vọc vạch nhé!
👉️ Link Github: https://github.com/NARKOZ/hacker-scripts

Top 2. The Book Of Secret Knowledge - 132k ⭐️
Mở đầu repo, bạn sẽ gặp câu châm ngôn ""Knowledge is powerful, be careful how you use it!"". Đây cũng có thể coi là lời nhắc nhở với toàn bộ anh em muốn theo hoặc đang theo mảng security. Hãy sử dụng những kiến thức mình học được một cách chính đáng, đừng để sa chân vào những con đường tội lỗi rồi hối hận không kịp 🥲
Repo này rất nhiều các tài liệu và công cụ hữu ích khác nhau mà tác giả sử dụng hàng ngày trong công việc của mình. Bản thân tác giả cũng chia sẻ rằng, đây là nguồn kiến thức vô giá mà tác giả thường xuyên nhìn lại.
Repo này tập trung hướng đến các vị trí như System and Network administrators, DevOps, Pentesters, và Security Researchers. Đây chính là những vị trí mà tác giả đã từng làm và có nhiều kinh nghiệm để chia sẻ lại cho mọi người thông qua repo The Book Of Secret Knowledge này 👨‍🏫
👉️ Link Github: https://github.com/trimstray/the-book-of-secret-knowledge

Top 3. Awesome Hacking - 78.2k ⭐️
Bạn nào dạo này thường xuyên đọc loạt bài về ""Top 10 ..."" của mình, thì chắc sẽ đều công nhận rằng: rất nhiều repo trên GitHub bắt đầu với từ ""awesome"" đều ""tuyệt vời"" đúng như cái tên của nó 🤩
Repo Awesome Hacking là tập hợp rất nhiều tài liệu bổ ích cho anh em hackers, pentesters và security researchers.
Danh sách này rất dài và đa dạng nên mình không mô tả chi tiết được ở đây, các bạn truy cập đường link dưới đây để tra cứu cụ thể nhé!
👉️ Link Github: https://github.com/Hack-with-Github/Awesome-Hacking

Top 4. Payloads All The Things - 57.3k ⭐️
Repo này cung cấp cho bạn danh sách hàng loạt những payloads và bypasses, phục vụ cho việc bảo mật ứng dụng Web và công việc Pentest, cũng như các cuộc thi CTF.
Các điểm nổi bật của repo:
Payloads đa dạng: Bao gồm nhiều loại payloads phục vụ cho việc khai thác các lỗ hổng khác nhau.
Bypass techniques: Cung cấp các phương pháp bypass cho những cơ chế bảo mật thông thường mà các ứng dụng web sử dụng.
Hướng dẫn chi tiết: Có các ví dụ minh họa và giải thích chi tiết về cách sử dụng từng payload một cách hiệu quả.
Cập nhật thường xuyên: Được cập nhật với các kỹ thuật và phương pháp mới nhất từ cộng đồng bảo mật.
Tài liệu hữu ích: Chứa nhiều tài liệu học tập và tham khảo, giúp nâng cao kiến thức và kỹ năng về bảo mật ứng dụng web và pentest.
👉️ Link Github: https://github.com/swisskyrepo/PayloadsAllTheThings

Top 5. SecLists - 54.2k ⭐️
Repo này là tập hợp nhiều loại danh sách được sử dụng trong quá trình đánh giá bảo mật.
Các loại danh sách bao gồm: usernames, passwords, URLs, những data patterns nhạy cảm, fuzzing payloads, web shells, ...
Repo này nặng ""sương sương"" có 1.2GB thôi, anh em có ý định clone về thì chịu khó đợi một xíu nhá.
👉️ Link Github: https://github.com/danielmiessler/SecLists

Trên đây là phần 1 của bài viết ""TOP 10 REPOSITORIES GITHUB HAY NHẤT ĐỂ HỌC SECURITY""
Mình đang gấp rút hoàn thiện nốt phần 2 với 5 project còn lại từ Top 6 đến Top 10.
Nếu các bạn thấy bài viết này hữu ích, thì hãy tặng mình 1 upvote và 1 comment để mình có động lực viết thêm những series bài viết thú vị về lập trình tiếp theo nha 🌻
Các bạn cũng có thể tham khảo series video ""Lên trình Thuật toán - Lập trình thi đấu 🏆"" mà mình đang làm trên Youtube tại đây:


Hẹn gặp lại 👋",https://viblo.asia/p/top-10-repositories-github-hay-nhat-de-hoc-security-part-1-W13VMyAWVY7
76,"TOP 10 PROJECT GITHUB GIÚP BẠN LÊN TRÌNH C#, ASP.NET, ASP.NET CORE 💎 (Phần 2)","Số liệu được updated ngày 19/05/2024
Tiếp nối phần 1 của ""TOP 10 PROJECT GITHUB GIÚP BẠN LÊN TRÌNH C#, ASP.NET, ASP.NET CORE 💎"", hôm nay mình sẽ chia sẻ với các bạn 5 project còn lại từ Top 6 đến Top 10.
Top 6. ASP.NET Boilerplate - 11.6k ⭐️
Repo này là mẫu thiết kế của một ứng dụng web sử dụng ASP.NET hoàn chỉnh 🌐
Anh em nào mà hay xem trên mạng các video nói về nguyên tắc SOLID, thì có thể vào đây để tìm hiểu chi tiết về cách áp dụng SOLID vào code của một dự án sẽ như thế nào 👨‍💻
Repo này được xây dựng theo dạng multi-tanancy, và có chia thành các module để bạn dễ dạng mở rộng chức năng nếu muốn.
Bên cạnh đó, họ cũng cung cấp documentation rất chi tiết, để kể cả những bạn mới học cũng có thể dễ dàng tiếp cận và tìm hiểu 📝
👉️ Link Github: https://github.com/aspnetboilerplate/aspnetboilerplate

Top 7. C# Language Design - 11k ⭐️
Đây là một repo đến từ chính chủ nhà Microsoft, giúp team phát triển ngôn ngữ lập trình C# của họ có thể dễ dàng kết nối với những lập trình viên khác trên thế giới 🌍
Repo này lưu trữ đầy đủ về các tính năng đang có, những tính năng được đề xuất, ghi chú từ các buổi meeting và bản nháp spec của team phát triển ngôn ngữ lập trình C# 🗂️
Và đương nhiên, khi đã open như vậy, thì bạn cũng hoàn toàn có thể đề xuất tính năng mới hoặc fix bug, rồi tạo pull request đề team C# của Microsoft xem xét merge vào source code của họ 📬
👉️ Link Github: https://github.com/dotnet/csharplang

Top 8. practical-aspnetcore - 8.9k ⭐️
Repo này tổng hợp rất nhiều những mini projects để các bạn có thể thực hành với ASP.NET Core 👨‍💻
Ngoài phiên bản ASP.NET Core 8.0, thì còn có các các sample cho phiên bản 6.0, 5.0, 3.1 LTS, 2.1 LTS nữa.
Học phải đi đôi với hành mà. Nên nếu bạn đang thiếu ý tưởng để code mini project với ASP.NET Core thì hãy tham khảo ngay repo này nhé!
👉️ Link Github: https://github.com/dodyg/practical-aspnetcore

Top 9. clean-code-dotnet - 6.8k ⭐️
Clean Code là cuốn sách rất nổi tiếng - được mệnh danh là cuốn sách gối đầu giường của các lập trình viên 📖
Đây là cuốn sách xếp Top 1 Amazon Best Seller của thể loại sách Software Design & Engineering 🥇
Tác giả của cuốn sách này là Robert C. Martin, hay còn được biết đến rộng rãi với danh xưng ""Uncle Bob"" 👨‍🦳
Tuy nhiên, các ví dụ trong bản gốc của cuốn sách được code bằng Java, có thể gây đôi chút bất tiện cho những bạn mới học lập trình và code .NET/.NET Core. Vậy nên mục đích của repo này chính là xóa đi những bất tiện đó.
👉️ Link Github: https://github.com/thangchung/clean-code-dotnet

Top 10. The Algorithms - C# - 6.4k ⭐️
Trang GitHub của ""The Algorithms"" có lẽ quá nổi tiếng đối với những anh em đam mê thuật toán rồi 💫
Repo ""The Algorithms - C#"" này là một trong những repo con của họ, giúp các bạn hiểu và implement được các cấu trúc dữ liệu và giải thuật bằng cách sử dụng ngôn ngữ C#.
👉️ Link Github: https://github.com/TheAlgorithms/C-Sharp

Trên đây là phần 2 của bài viết ""TOP 10 PROJECT GITHUB GIÚP BẠN LÊN TRÌNH C#, ASP.NET, ASP.NET CORE 💎""
Nếu các bạn thấy bài viết này hữu ích, thì hãy tặng mình 1 upvote và 1 comment để mình có động lực viết thêm những series bài viết thú vị về lập trình tiếp theo nha 🌻
Các bạn cũng có thể tham khảo series video ""Lên trình Thuật toán - Lập trình thi đấu 🏆"" mà mình đang làm trên Youtube tại đây:


Hẹn gặp lại 👋",https://viblo.asia/p/top-10-project-github-giup-ban-len-trinh-c-aspnet-aspnet-core-phan-2-EvbLbadlJnk
77,[Docker] HomeLab triển khai Reverse Proxy server sử dụng Zoraxy,"Chào các bạn,
Hôm nay mình quay trở lại với một bài viết về Docker. Hi vọng giúp ích cho ai đó lang thang vào đọc bài viết này. Chủ đề mình muốn chia sẻ với các bạn trong bài viết này là tự  triển khai Reverse Proxy server sử dụng Zoraxy với HomeLab tự có và không tốn chi phí. Nó bình dân là phờ ri 😄😄
Bắt đầu thôi !!!
Máy ảo
Server
VPS
Cài Linux Ubuntu, Debian, CentOS...
Bạn nào chưa biết các khái niệm này thì hỏi chị Google hoặc anh chatGPT nhé!
Sau khi đã có các công cụ ở phần đầu mục 1.
Các bạn tiến hành cài Docker và Docker Compose. Phần cài mình để link tại đây.
Sau khi hoàn thành việc cài đặt Docker và Docker Compose. Chúng ta sẽ đi cài ứng dụng Zoraxy để làm  Reverse Proxy
Zoraxy là gì? Các bạn đọc thêm tài liệu tại đây
Một vài tính năng chính của Zoraxy mình liệt kêt ở đây:
Reverse Proxy (HTTP/2)
Reverse Proxy (HTTP/2)
Redirection Rules
Redirection Rules
TLS / SSL setup and deploy
TLS / SSL setup and deploy
Blacklist / Whitelist by country or IP address (single IP, CIDR or wildcard for beginners)
Blacklist / Whitelist by country or IP address (single IP, CIDR or wildcard for beginners)
Integrated Up-time Monitor
Integrated Up-time Monitor
Để cài đặt Zoraxy các bạn tạo một file docker-compose như bên dưới
version: '3.3'
services:
  zoraxy-docker:
    image: zoraxydocker/zoraxy:latest
    container_name: zoraxy
    ports:
      - 80:80
      - 443:443
      - 8005:8000/tcp
    volumes:
      - /home/docker/Containers/Zoraxy:/opt/zoraxy/config/
    environment:
      ARGS: '-noauth=false'
Lưu lại và thoát
Tiếp đến chạy lệnh docker-compose up -d để khởi tạo
docker-compose up -d
Đợi ít phút khởi tạo xong các bạn login và khởi tạo user để đăng nhập. Mở trình duyệt lên và nhập

Đây là giao diện tổng quan của Zoraxy.
Cấu hình Reverse Proxy. Mình ví dụ ở đây mình khai báo cho ứng dụng tên uptime-kuma thì các bạn khai báo như bên dưới. Ở đây có 2 thông số chính là Domain và IP target.
uptime-kuma

Domain là tên miền bạn khai báo cho ứng dụng khi người dùng truy cập vào
IP target là IP và port backend bên trong server ứng dụng đang chạy
Khai báo xong các bạn qua tab HTTP proxy sẽ thấy domain xuất hiện ở đây
Để Host chạy được HTTP bạn cấu hình lại cho Zoraxy chạy port 443 thay vì chạy port 80 default. Trong tab status i các bạn kéo xuống bên dưới và đổi Port 80 -> 443 là xong. Hình minh hoạ mình chưa đổi nhé 😃
Tiếp theo các bạn tạo cho domain ứng dụng tên uptime-kuma một cái SSL. Các bạn qua tab TLS/ SSL certificates
uptime-kuma
Mở cái ACME Tool lên

Nhập email và domain của ứng dụng và bấm Get certificate
Nếu muốn auto renew cert thì enable lên.
Sau đó qua lại tab HTTP Proxy thấy ổ khoá từ vàng -> xanh. Vậy là ngon lành rồi. Truy cập vào domain ứng dụng coi xem sao và mở xem chứng chỉ cert.

Vậy là xong rồi đó
Toàn bộ quá trình thực hiện các bạn có thể xem video mình hướng dẫn bên dưới

Follow các kênh mình chia sẻ
Vibo chia sẻ
Youtube
Blog chia sẻ",https://viblo.asia/p/docker-homelab-trien-khai-reverse-proxy-server-su-dung-zoraxy-PAoJeQ3pJ1j
78,Project Idx -  môi trường phát triển tích hợp trực tuyến được phát triển bởi Google,"Project IDX là một môi trường phát triển tích hợp hoàn toàn dựa trên web. IDX có thể giúp bạn xây dựng và triển khai các ứng dụng trong nhiều nhóm công nghệ khác nhau mà không cần phải thiết lập môi trường phát triển tẻ nhạt cho chính bạn và những người khác trong nhóm của bạn. IDX chạy trên các máy ảo được cấu hình sẵn trên Google Cloud để giữ cho môi trường phát triển của bạn luôn đáng tin cậy, an toàn và có thể tùy chỉnh hoàn toàn, giống như môi trường phát triển cục bộ.
Các mẫu dự án mới trên IDX hỗ trợ phát triển nhiều loại ứng dụng sử dụng các khung như Angular, Flutter, Next.js, React, Svelte và Vue. Các mẫu IDX giúp bạn dễ dàng bắt đầu phát triển ứng dụng mình muốn, hoàn toàn từ trình duyệt.

Tại hội nghị nhà phát triển Google I/O 2024 thường niên vào thứ Ba, Google đã thông báo rằng Project IDX , môi trường phát triển dựa trên trình duyệt tập trung vào AI thế hệ tiếp theo của công ty, hiện đang ở giai đoạn thử nghiệm mở. Công ty lần đầu tiên ra mắt dịch vụ này dưới dạng dịch vụ chỉ dành cho những người được mời được đưa vào danh sách chờ vào tháng 8.
Google cho biết hơn 100.000 nhà phát triển đã dùng thử dịch vụ này.
Jeanine Banks, Phó chủ tịch Google kiêm tổng giám đốc Nhà phát triển X và người đứng đầu bộ phận phát triển của công ty cho biết: “Khi AI trở nên phổ biến hơn, sự phức tạp đi kèm với việc triển khai tất cả những thứ đó thực sự trở nên khó khăn hơn, lớn hơn và chúng tôi muốn giúp giải quyết thách thức đó”. quan hệ của nhà phát triển. “Đó là lý do tại sao chúng tôi xây dựng dự án IDX, một trải nghiệm phát triển đa nền tảng giúp việc xây dựng ứng dụng trở nên nhanh chóng và dễ dàng. Project IDX giúp bạn thực sự dễ dàng sử dụng framework hoặc ngôn ngữ ưa thích của mình bằng các mẫu dễ sử dụng như Next.js, Astro, Flutter, Dart, Angular, Go, v.v.
Với bản cập nhật này, Google sẽ bổ sung tính năng tích hợp với Nền tảng Google Maps vào IDE, giúp bổ sung các tính năng định vị địa lý cho các ứng dụng của mình, cũng như tích hợp với Chrome Dev Tools và Lighthouse để giúp gỡ lỗi ứng dụng. Sắp tới, Google cũng sẽ cho phép triển khai ứng dụng lên Cloud Run , nền tảng serverless của Google Cloud để chạy các dịch vụ front-end và back-end.
Môi trường phát triển cũng sẽ tích hợp với Checks , nền tảng tuân thủ do AI cung cấp của Google, nền tảng này sẽ chuyển từ phiên bản beta sang tính khả dụng rộng rãi vào thứ Ba.
Nhưng tất nhiên, IDX không chỉ là xây dựng các ứng dụng hỗ trợ AI - mà còn là sử dụng AI trong quá trình mã hóa. Để kích hoạt tính năng này, IDX bao gồm nhiều tính năng hiện đã trở thành tính năng tiêu chuẩn như hoàn thành mã và thanh bên trợ lý trò chuyện, cũng như các tính năng cải tiến như khả năng đánh dấu một đoạn mã và — tương tự như tính năng điền tổng quát trong Photoshop — hãy hỏi mô hình Gemini của Google để thay đổi một đoạn mã.
Bất cứ khi nào Gemini đề xuất mã, nó sẽ liên kết ngược lại nguồn ban đầu và giấy phép liên quan của nó.
Project IDX, được Google xây dựng với mã nguồn mở Visual Studio Code làm cốt lõi, cũng tích hợp với GitHub, giúp dễ dàng tích hợp với các quy trình công việc hiện có. Trong một trong những bản phát hành mới nhất của IDX, Google cũng đã thêm trình giả lập iOS và Android tích hợp sẵn dành cho nhà phát triển thiết bị di động ngay vào IDE.
Lợi ích chính của Project IDX là nó cung cấp một môi trường phát triển nhất quán có thể truy cập được từ mọi thiết bị, ở mọi lúc, mọi nơi. Điều này giúp loại bỏ đáng kể những vấn đề khó khăn phổ biến như đảm bảo môi trường nhà phát triển đồng bộ giữa các thiết bị/hệ thống. IDE dựa trên đám mây cũng cung cấp quyền truy cập vào các tài nguyên điện toán mạnh mẽ, không khả dụng cục bộ trên máy của nhà phát triển.
Google cho biết Project IDX có thể sử dụng các máy macOS trên đám mây để xây dựng ứng dụng iOS và xử lý iOS Simulator. Thông tin chi tiết về khả năng hỗ trợ giả lập Android sẽ được thông báo sau.
Chưa hết, Project IDX còn được tích hợp sâu với GitHub để kiểm soát nguồn và nền tảng Firebase của Google để lưu trữ, chức năng đám mây, v.v. Mỗi không gian làm việc cung cấp một máy ảo dựa trên Linux.
Project IDX hiện đang ở giai đoạn early preview. Google có kế hoạch bổ sung nhiều ngôn ngữ, framework và tích hợp thêm những dịch vụ đám mây khác. Công ty kỳ vọng dự án này sẽ mở đường cho một cuộc cách mạng hóa trong cách thức viết code của các nhà phát triển.

Bước 1: Truy cập vào trang https://idx.dev/
Bước 2: Chọn GetStarted, ở đây sẽ có rất nhiều loại mẫu của nhiều dự án khác như Flutter, Html, Python,...(trong phần này mình sẽ chọn Flutter)
Bước 3: Khi chọn mẫu xong chờ vài giây để dự án load thành công. Sau khi tải thành công giao diện sẽ như trong ảnh, trông rất giống IDE Visual Studio Code.
Lợi ích của Projext IDx là không cần phải cái máy ảo, mà ở đó đã hỗ trợ luôn cho chúng ta
Chúng ta chỉ cần mở web lên và code và build dự án trực tiếp trên đó.
Project IDX đã tạo không gian làm việc được hỗ trợ bởi AI để phát triển ứng dụng đa nền tảng, toàn diện trên đám mây. Với sự hỗ trợ cho nhiều khung, ngôn ngữ và dịch vụ, cùng với khả năng tích hợp với các sản phẩm Google yêu thích của bạn, IDX hợp lý hóa quy trình phát triển của bạn để bạn có thể xây dựng và phân phối ứng dụng trên nhiều nền tảng một cách nhanh chóng, dễ dàng và chất lượng. Hãy cùng khám phá phiên bản beta của nó nhé.",https://viblo.asia/p/project-idx-moi-truong-phat-trien-tich-hop-truc-tuyen-duoc-phat-trien-boi-google-pgjLN2X9L32
79,Cách sử dụng Browserstack,"Browserstack không còn quá xa lạ với các lập trình viên, các tester. Đây là nền tảng để kiểm thử Cross Browser Testing để xác minh khả năng tương thích của ứng dụng web với các trình duyệt khác nhau. Bài viết này sẽ giới thiệu một cách chi tiết về các tính năng của nền tảng này.
Giới thiệu về BrowserStack
BrowserStack là một nền tảng thử nghiệm di động và web trên nền tảng đám mây cho phép các nhà phát triển kiểm tra các trang web và ứng dụng di động của họ trên các trình duyệt theo yêu cầu, hệ điều hành và thiết bị di động thực mà không yêu cầu người dùng cài đặt máy ảo, thiết bị hoặc trình giả lập.
Sử dụng Browserstack người dùng có thể chọn từ trên hơn 3500 kết hợp trình duyệt trên máy tính và thiết bị di động thật, được cập nhật liên tục và có sẵn ngay lập tức theo yêu cầu và dựa trên cơ sở hạ tầng an toàn, ổn định và có thể mở rộng để hỗ trợ hàng ngàn thử nghiệm tự động và thủ công đồng thời. Nó có thể cung cấp tất cả các phiên bản macOS & Windows, từ Snow Leopard đến Sonoma & Windows XP đến 11. Một điều nổi bật là các thử nghiệm đều sử dụng các thiết bị Android và IOS thực, trên các điện thoại và Android như iPhone, iPad, Google, OnePlus, Motorola, Xiaomi, Huawei, Motorola, Vivo và Samsung, đủ các trình duyệt với đủ các phiên bản,...
Giới thiệu về BrowserStack
BrowserStack là một nền tảng thử nghiệm di động và web trên nền tảng đám mây cho phép các nhà phát triển kiểm tra các trang web và ứng dụng di động của họ trên các trình duyệt theo yêu cầu, hệ điều hành và thiết bị di động thực mà không yêu cầu người dùng cài đặt máy ảo, thiết bị hoặc trình giả lập.
Sử dụng Browserstack người dùng có thể chọn từ trên hơn 3500 kết hợp trình duyệt trên máy tính và thiết bị di động thật, được cập nhật liên tục và có sẵn ngay lập tức theo yêu cầu và dựa trên cơ sở hạ tầng an toàn, ổn định và có thể mở rộng để hỗ trợ hàng ngàn thử nghiệm tự động và thủ công đồng thời. Nó có thể cung cấp tất cả các phiên bản macOS & Windows, từ Snow Leopard đến Sonoma & Windows XP đến 11. Một điều nổi bật là các thử nghiệm đều sử dụng các thiết bị Android và IOS thực, trên các điện thoại và Android như iPhone, iPad, Google, OnePlus, Motorola, Xiaomi, Huawei, Motorola, Vivo và Samsung, đủ các trình duyệt với đủ các phiên bản,...
Cách sử dụng Browserstack
Cách sử dụng Browserstack
BrowserStack bao gồm các tính năng: Live, App Live, Automate và App Automate.
Live: Test tương tác trên thiết bị và trình duyệt thực tế.
Automate: tự chạy một số bài test trên thiết bị smartphone.
App Live: Test app thực tế trên các thiết bị smartphone.
App Automate: kiểm tra một số bài test tự động trên smartphone.
Ở đây mình sẽ thử Live để demo về test tương tác trên các thiết bị
2.1. Sử dụng Browserstack để test tương tác trên các thiết bị và trình duyệt thực tế
Click vào Manual testing -> Live sẽ xuất hiện giao diện về các thiết bị cũng như version của các thiết bị. Để test các thiết bị trên iOS, có thể chọn các loại điện thoại từ iPhone 3 đến iPhone 15, từ các verion 3 đến các verion mới nhất của iOS, có thể hoàn toàn chọn test trên iPhone hoặc iPad

Tương tự như thế, với các thiết bị sử dụng hệ điều hành Android, ta có thể chọn các thiết bị của Samsung, Google, Xiaomi, Realme,...

Để sử dụng thiết bị nào, chỉ cần hover vào tên thiết bị và click chọn trình duyệt cần để kiểm thử

Ở đây mình chọn iPhone 15 Promax verion 17.0

Như vậy chỉ cần với 1 click chuột có thể chọn được thiết bị, verion muốn kiểm thử, giúp tối ưu chi phí mua thiết bị thật, setup thiết bị để thực hiện test một cách dễ dàng.
Để đổi sang thiết bị khác chỉ cần click chọn Switch Browser.
Một điều lưu ý ở đây thiết bị hiển thị trên Browserstack hoàn toàn là thiết bị thật, không phải thiết bị giả lập, chúng ta hoàn toàn có thể test được tới các tính năng như Zoom in, Zoom out, Multi Touch, ...
2.2. Sử dụng Browserstack để capture bug
Một trong những tính năng mình khá thích ở Browserstack là có thể Capture trực tiếp bug trên thiết bị tại trình duyệt, mà không cần sử dụng đến bên thứ 3

Sau khi click vào Capture a bug sẽ hiển thị hình ảnh đã lưu để gửi trực tiếp đến Jira, Github, Trello, Slack,... hoặc lưu lại hình ảnh để log bug
Tương tự như khi capture bug bằng hình ảnh, chúng ta có thể sử dụng tính năng Record Session để ghi lại quá trình tái hiện bug
Sử dụng tính năng Rotate Device để xoay thiết bị
2.3. Sử dụng Browserstack để Debug
Dù thiết bị là máy tính hay điện thoại, chúng ta vẫn có thể sử dụng tính năng DevTools để debug trên Chrome hoặc trên trình duyệt đang sử dụng
2.4. Sử dụng Browserstack để Change Location -  Giả lập một vị trí bất kỳ
Với tính năng Change Location, chúng ta hoàn toàn có thể giả lập thiết bị tại vị trí bất kỳ muốn kiểm thử, sử dụng Change Device GPS

Có thể chọn tính năng IP Location để đổi địa chỉ IP tại bất cứ quốc gia nào
2.5. Sử dụng tính năng Throttle Network - Giả lập tốc độc 3G, 4G… để kiểm tra tốc độ
Với Browserstack, người dùng có thể giả laoaj các tốc độ mạng Edge, 3G, 4G, custom tốc độ mạng để có thể đảm bảo phần mềm của mình hoạt động được tại mọi điều kiện mạng khác nhau
Sử dụng Browserstack để thực hiện automation test trên các thiết bị
Browserstack cung cấp môi trường, thiết bị để có thể chạy được automations test.  Nó có thể sử dụng chạy Automate các trang web sử dụng Selenium, Cypress, JS Testing API, Playwright,... Có thể sử dụng để kiểm thử tự động Mobile apps trên các thiết bị thật sử dụng Appium, Espresso, ...

Phần này khá dài nên mình sẽ trình bày tại một bài viết khác, mọi người có thể tham khảo tại: https://www.browserstack.com/docs/
Sử dụng Browserstack để thực hiện automation test trên các thiết bị
Browserstack cung cấp môi trường, thiết bị để có thể chạy được automations test.  Nó có thể sử dụng chạy Automate các trang web sử dụng Selenium, Cypress, JS Testing API, Playwright,... Có thể sử dụng để kiểm thử tự động Mobile apps trên các thiết bị thật sử dụng Appium, Espresso, ...

Phần này khá dài nên mình sẽ trình bày tại một bài viết khác, mọi người có thể tham khảo tại: https://www.browserstack.com/docs/
Sử dụng Browserstack để Screenshots màn hình tại nhiều thiết bị cùng lúc
Với Browserstack bạn hoàn toàn có thể kiểm thử nhanh các trang tĩnh trên nhiều thiết bị bằng cách truy cập https://www.browserstack.com/screenshots, nhập url trang web, chọn các thiết bị muốn screenshot sau đấy click Generate và chờ đợi. Như vậy chỉ với một cú click chuột có thể gen ra trang web mình muốn truy cập hiển thị ở các thiết bị mobile ( iOS, Android) , desktop, ipad với các version khác nhau như thế nào


Trên đây mình mới chỉ trình bày được một số ít trong các tác dụng của Browserstack, hi vọng sẽ giúp ích được cho mọi người ^^
Sử dụng Browserstack để Screenshots màn hình tại nhiều thiết bị cùng lúc
Với Browserstack bạn hoàn toàn có thể kiểm thử nhanh các trang tĩnh trên nhiều thiết bị bằng cách truy cập https://www.browserstack.com/screenshots, nhập url trang web, chọn các thiết bị muốn screenshot sau đấy click Generate và chờ đợi. Như vậy chỉ với một cú click chuột có thể gen ra trang web mình muốn truy cập hiển thị ở các thiết bị mobile ( iOS, Android) , desktop, ipad với các version khác nhau như thế nào

Trên đây mình mới chỉ trình bày được một số ít trong các tác dụng của Browserstack, hi vọng sẽ giúp ích được cho mọi người ^^",https://viblo.asia/p/cach-su-dung-browserstack-y37Ld1QDVov
80,[Software Architect][Thiết kế và Kiến trúc] Những kĩ năng cần học (Bài viết #1),"Nội dung bài viết được dịch từ https://roadmap.sh/, ở khía cạnh của một người đang làm Frontend Developer, đang học để trở thành Software Architect, mỗi vài viết là 1 kĩ năng. Vào vấn đề, ...
Để trở thành một software architect, bạn cần có các kĩ năng sau:
Thiết kế và Kiến trúc
Ra quyết định
Đơn giản hoá vấn đề
Biết code
Viết tài liệu
Giao tiếp
Ước lượng và đánh giá
Cân bằng
Tư vấn và hướng dẫn
Kĩ năng marketing
Sau đây là kĩ năng số 1, Thiết kế và Kiến trúc.
Biết các nguyên tắc thiết kế cơ bản (Design Principles):
Nguyên tắc là những gì mà chúng ta cần tuân thủ, nó giống như việc bạn cần dùng chén để đựng thức ăn và dùng muỗng để lấy thức ăn. Ví dụ:
Tạo ra các thành phần rồi gộp chúng lại sẽ hay hơn là kế thừa
Gộp những phần dễ thay đổi thành một nhóm, gộp những phần không thay đổi thành một nhóm
Lập trình theo hướng trừu tượng, tạo ra tên của hành vi, nhưng không chi tiết hành vi đó. Chỉ chi tiết khi hành vi đó cần triển khai.
Nguyên tắc Hollywood, ""Đừng gọi chúng tôi, chúng tôi gọi cho bạn"", component-sử-dụng sẽ truyền callback vào component-được-sử-dụng, component-được-sử-dụng sẽ gọi callback này khi xử lí
SOLID, DRY, KISS, YAGNI, ...
Có rất nhiều nguyên tắc, chúng có thể trùng lặp nhau hoặc chứa đựng một phần của nhau. Phạm vi bài viết này sẽ không liệt kê và chi tiết hết được. Hãy tìm đọc nó!
Biết các mẫu thiết kế cơ bản (Design Patterns):
Khác biệt với Nguyên tắc, cái mà chúng ta cần tuân thủ. Mẫu là cái có sẵn, việc của chúng ta là lựa chọn. Mẫu là một trong những công cụ quan trọng nhất mà một kiến trúc sư cần có để phát triển các hệ thống có thể bảo trì. Với các mẫu, bạn có thể tái sử dụng thiết kế để giải quyết các vấn đề phổ biến với các giải pháp đã được chứng minh. Thông thường, các mẫu này sẽ được phản ánh trong cách sử dụng một Framework cụ thể, bạn có thể sử dụng nó trong cách viết mã của mình. Hai nguồn cơ bản để tiếp cận là:
GoF (Gang of Four): Cuốn sách ""Design Patterns: Elements of Reusable Object-Oriented Software"" do John Vlissides, Ralph Johnson, Richard Helm, Erich Gamma viết là cuốn sách bắt-buộc-đọc cho tất cả những ai làm trong ngành phát triển phần mềm. Mặc dù các mẫu đã được xuất bản hơn 20 năm trước nhưng chúng vẫn là cơ sở của kiến trúc phần mềm hiện đại. Ví dụ, mẫu Model-View-Controller (MVC) đã được mô tả trong cuốn sách này, được áp dụng trong nhiều lĩnh vực hoặc là cơ sở cho các mẫu mới hơn, ví dụ như Model-View-ViewModel (MVVM).
POSA (Pattern-Oriented Software Architecture): Kiến trúc phần mềm hướng mẫu là một tập hợp các mẫu thiết kế để phát triển hệ thống phần mềm có thể mở rộng và thích ứng với các yêu cầu thay đổi. Những mô hình này lần đầu tiên được mô tả trong cuốn sách “Patterns of Scalable, Reliable Services” của Kevin Hoffman.
Tìm hiểu sâu hơn về các mẫu và phản mẫu (Patterns và Anti-Patterns):
Nếu bạn đã biết tất cả các mẫu cơ bản, hãy mở rộng kiến thức của bạn với nhiều mẫu thiết kế phần mềm hơn hoặc tìm hiểu sâu hơn về lĩnh vực quan tâm của bạn. Một trong những cuốn sách yêu thích của tôi về tích hợp ứng dụng là ""Enterprise Integration Patterns"" do Gregor Hohpe viết. Cuốn sách này áp dụng trong nhiều lĩnh vực bất cứ khi nào hai ứng dụng cần trao đổi dữ liệu, dù là trao đổi tệp tin kiểu cũ từ một số hệ thống cũ hay một kiến trúc microservice hiện đại.
Biết về các nguyên tắc kiến trúc (Architectural Principles):
Nguyên tắc kiến trúc đề cập đến một bộ hướng dẫn hoặc quy tắc được sử dụng để hướng dẫn kiến trúc phần mềm. Những nguyên tắc này nhằm đảm bảo rằng kết quả có thể duy trì được, có thể mở rộng, dễ hiểu và sửa đổi. Chúng có thể là:
Nguyên tắc Component (Component Principles): chia nhỏ những phần liên quan thành component, để module hoá, dễ sử dụng, dễ hiểu, dễ test, dễ duy trì,...
Chính sách và chi tiết (Policy vs Detail): Chính sách là những định nghĩa ở cấp cao, tổng thể, kiến trúc do architect hoặc designer quyết định; Chi tiết là những phần cấp thấp, cụ thể, thuật toán, cấu trúc dữ liệu do developer sẽ thực hiện.
Khớp nối và Gắn kết (Coupling vs Cohesion): Khớp nối là sự kết hợp các component lại với nhau, càng lõng lẽo càng tốt (Component PDF có thể sử dụng với Component GetViewDocument hoặc Component GetPrintDocument); Gắn kết là sự kết hợp trong nội hàm component, càng liên quan càng tốt (Trong component PDF chỉ nên xử lí PDF, không nên xử lí gọi API để lấy document).
Ranh giới (Boundaries): Các ranh giới này có thể là vật lý, chẳng hạn như giữa các vi-dịch-vụ khác nhau trong hệ thống phân tán; có thể là phi vật lý như các lớp khác nhau trong một ứng dụng.
Biết về các mẫu kiến trúc (Architectural Patterns):
Các mẫu kiến trúc là một tập hợp các giải pháp đã được chứng minh là hoạt động tốt cho các loại hệ thống phần mềm cụ thể. Chúng cung cấp những thuật ngữ chung và tập hợp các phương pháp hay nhất để thiết kế và xây dựng hệ thống phần mềm, đồng thời có thể giúp các nhà phát triển đưa ra quyết định thiết kế tốt hơn. Một số mẫu kiến trúc phổ biến bao gồm:
Service-Oriented Architecture (SOA)
Microservices
Serverless
Message Queues Streams
Command Query Responsibility Segregation (CQRS)
Domain-Driven Design (DDD)
Model-View-Controller (MVC)
Blackboard
Microkernel
Event Sourcing
Biết về các kiểu kiến trúc (Architectural Styles):
Kiểu kiến trúc trong phần mềm đề cập đến thiết kế và tổ chức tổng thể của hệ thống phần mềm cũng như các nguyên tắc và mẫu được sử dụng để hướng dẫn thiết kế. Những kiểu này cung cấp một khuôn khổ chung cho việc thiết kế một hệ thống và có thể được sử dụng để đảm bảo rằng hệ thống có cấu trúc tốt, có thể bảo trì và có thể mở rộng.
Kiểu sự kiện (Messaging): Event-driven, Publish-Subscribe
Kiểu phân phối (Distributed): Client-Server, Peer-to-Peer
Kiểu cấu trúc (Structural): Component-based, Monolithic, Layered
Biết các biện pháp chất lượng:
Tạo ra kiến trúc, chưa hẵng là xong, cần phải định lượng xem chúng hoạt động thế nào. Đây là cơ sở để đánh giá một kiến trúc tốt. Tại sao các hướng dẫn và tiêu chuẩn mã hóa phải được định nghĩa, áp dụng và kiểm soát? Bởi vì chất lượng (liên quan đến yêu cầu chức năng) và các yêu cầu phi chức năng. Và một phần để đạt được tất cả các thuộc tính chất lượng này là áp dụng công việc kiến trúc tốt. Bạn có thể bắt đầu tìm hiểu thêm về các biện pháp chất lượng trên Wikipedia. Lý thuyết là quan trọng. Thực hành cũng quan trọng — thậm chí quan trọng hơn — nếu bạn không muốn trở thành một Ivory Tower Architect. Vậy nên, ngay cả khi lên ý tưởng và cả khi thực hiện hay đã hoàn thành, hãy luôn quan tâm đến chất lượng của nó. Các thuộc tính có thể là:  có thể bảo trì, đáng tin cậy, có thể thích nghi, an toàn, có thể kiểm tra, có thể mở rộng, có thể sử dụng, v.v.
Thử nghiệm và hiểu các nhóm công nghệ khác nhau:
Tôi nghĩ đây là hoạt động quan trọng nhất nếu bạn muốn trở thành một kiến trúc sư giỏi hơn. Thử nghiệm các nhóm công nghệ và học được điểm mạnh/yếu của chúng. Công nghệ khác nhau sẽ đi kèm với các khía cạnh và mẫu thiết kế khác nhau. Bạn có thể không học được gì từ việc chỉ lướt qua các slide trừu tượng. Hãy thử nghiệm và cảm nhận sự đau đớn (fix bug sml) hoặc sự nhẹ nhõm.
Một kiến trúc sư không chỉ cần có kiến thức rộng ở nhiều lĩnh vực mà còn cần thêm kiến thức sâu ở một lĩnh vực (T-shape, tôi sẽ có một bài viết về điều này).
Không quan trọng phải thành thạo tất cả các nhóm công nghệ nhưng phải có sự hiểu biết vững chắc về những điều quan trọng nhất trong lĩnh vực của bạn.
Cũng nên thử công nghệ không thuộc lĩnh vực của bạn.
Ví dụ, nếu bạn sâu vào SAP R/3, bạn cũng nên thử JavaScript và ngược lại. Dù vậy, cả hai bên sẽ ngạc nhiên về những tiến bộ mới nhất trong SAP S/4 Hana. Ví dụ, bạn có thể tự thử và tham gia một khóa học miễn phí tại openSAP.
Hãy tò mò và thử nghiệm những điều mới. Cũng thử những thứ mà bạn không thích vài năm trước.
Phân tích và hiểu các mẫu được áp dụng:
Hãy xem bất kỳ framework hiện tại nào, ví dụ, Angular. Bạn có thể nghiên cứu rất nhiều mẫu trong thực hành, ví dụ, Observables. Cố gắng hiểu cách nó được áp dụng trong framework, tại sao nó được thực hiện. Và nếu bạn thực sự tận tâm, hãy xem xét sâu hơn vào mã và hiểu cách nó được thực hiện.
Hãy tò mò và tham gia Cộng đồng:
Tham gia các hội nhóm, diễn đàn trên Facebook, X, Telegram, Meetup, ...",https://viblo.asia/p/software-architectthiet-ke-va-kien-truc-nhung-ki-nang-can-hoc-bai-viet-1-aAY4q7gQLPw
81,Wing - Ngôn ngữ sinh ra cho Cloud,"Lâu rồi mới viết lại, âu cũng do KPI đã đặt thì phải thực hiện thôi 🥲. Ok, hôm nay tôi sẽ giới thiệu cho các bạn ngôn ngữ Wing mới ra gần đây.
Bài viết này hướng tới người đọc:
Đang làm việc với Cloud: AWS, Azure, Google, ...
Đang nghiên cứu Cloud
Đang gặp khó khăn trong việc tiếp cận Cloud
Thời điểm viết bài này là 9:31 pm ngày mùng 5/9/2024. Sau khoảng thời gian đó mà repo có bước phát triển mới thì bài viết của tôi sẽ bị outdated, vậy nên mong mọi người thông cảm.
Giới thiệu
Trước khi đi vào khái niệm của Wing thì tôi sẽ kể cho bạn nghe trải nghiệm của tôi sau khi vọc vạch Google Cloud Platform. Đầu tiên, tôi phải công nhận là Cloud tiện thật, nhưng bất tiện cũng nhiều 😀. Dưới đây là ưu và khuyết của Cloud mà tôi ngộ ra trong quá trình làm việc.
Ưu điểm:
Với những doanh nghiệp vừa và nhỏ có mong muốn phát hành sản phẩm thì Cloud là một lựa chọn tối ưu cả về chi phí và thời gian: họ đỡ phải quan tâm tới việc xây dựng cơ sở hạ tầng (infrastructure), xây dựng web server, hệ thống network, ...
Tính bảo mật cao: rule, policy, IAM service, login, authentication, ...
Cloud cung cấp nhiều dịch vụ, phục vụ nhu cầu riêng biệt của mỗi doanh nghiệp
...
Khuyết điểm:
Lằng nhằng, phức tạp, khó tùy chỉnh hệ thống theo nhu cầu (giới hạn request, quotas, limit, ...)
Chi phí cao, gia tăng theo số lượng dịch vụ, cấu hình máy ảo, thời lượng sử dụng máy ảo, số lượng request, ...
Khó khăn trong việc kiểm thử, thí nghiệm, ...
...
Với tôi, một tay mơ về Cloud, làm việc trên Cloud vừa là cực hình, vừa là sung sướng. Có khuynh hướng tự ngược vãi 😁
Còn với một chuyên gia về Cloud, họ cần phải có nhiều kinh nghiệm (master), thông hiểu về các lớp trong cloud stack, IAM roles, network, các công cụ cần thiết, test và debug, ...
Tuy vậy, điểm chung của cả tay mơ và chuyên gia là thời gian phát triển. Trong một dự án dài hơi, thời gian phát triển lâu đi kèm chu kỳ lặp lại nhiều (test, debug, ...) sẽ dẫn đến sức sáng tạo của lập trình viên dần dần mất đi.
Vì vậy, một ngôn ngữ lập trình dành cho Cloud được sinh ra: Wing. Wing sẽ ""chữa lành"" cho bạn bằng cách giả lập Cloud tại local cũng như cung cấp các công cụ cần thiết để kiểm tra code. Nói chung là thay vì tốn quá nhiều thời gian đọc tài liệu được cung cấp bởi Cloud Provider và làm quen với Cloud Platform thì bạn chỉ cần tập trung cho logic, nghiệp vụ của bạn.
Khái niệm
Về cơ bản, ứng dụng trên Cloud khác với ứng dụng trên một máy do nó là một hệ thống phân tán (distributed systems) phụ thuộc vào cơ sở hạ tầng trên cloud.
Vì vậy để trở thành một ngôn ngữ dành cho Cloud, Wing phải thực thi 2 giai đoạn: preflight cơ sở hạ tầng (infrastructure) và inflight runtime code.
preflight
inflight
Preflight được thực thi trong quá trình biên dịch và cung cấp cấu hình của cơ sở hạ tầng cho ứng dụng (ví dụ: Terraform, CloudFormation, ...)
Preflight
Inflight được biên dịch thành JavaScript và được thực thi trong môi trường giả lập Cloud của Node.js
Inflight
Ví dụ về 1 đoạn code của Wing:
bring cloud;

let queue = new cloud.Queue();
let counter = new cloud.Counter();
let bucket = new cloud.Bucket();

queue.setConsumer(inflight (message) => {
  let i = counter.inc();
  bucket.put(""file-{i}.txt"", message);
});
Ok, đoạn code này chia ra 2 phần:
Preflight objects: cloud.Queue, cloud.Counter và cloud.Bucket. Các objects này đại diện cho tài nguyên cơ sở hạ tầng (infrastructure resources). Khi được biên dịch tới một cloud provider chỉ định (chẳng hạn như AWS), một file Terraform sẽ được tạo ra. Phương thức queue.setConsumer() là một phương thức preflight cấu hình cơ sở hạ tầng, phương thức này sẽ gọi tới một hàm inflight cho mỗi một message có trong queue.
Preflight
cloud.Queue
cloud.Counter
cloud.Bucket
queue.setConsumer()
preflight
inflight
Inflight function: để tương tác với counter và bucket objects của cloud thì chúng ta có thể dùng các phương thức inflight: counter.inc() và bucket.put(). Những phương thức này chỉ có thể gọi trong phạm vi của hàm inflight.
Inflight
counter
bucket
inflight
counter.inc()
bucket.put()
inflight
Well, cái ví dụ trên cho thấy cách vận hành đơn giản của Wing. Trong việc phát triển ứng dụng trên cloud, Wing có vẻ là một công cụ hỗ trợ rất tốt, làm giảm đi mức độ khó khăn khi tiếp cận với Cloud:
Biến các Cloud Services về dạng class để tiện gọi, phân tách ra 2 giai đoạn: cơ sở hạ tầng và runtime code.
Cung cấp thư viện Wing Cloud Library  tiện cho việc viết code tập trung một chỗ và linh hoạt trong việc chuyển giao code.
Tùy chỉnh Cloud Platforms : tùy chỉnh cơ sở hạ tầng và kiểm tra các chính sách.
Tương thích với Terraform
Tương thích với JavaScript
Tự động sinh các chính sách IAM (IAM policies): role, principal, ...
Cung cấp Wing Console giúp bạn có thể dùng các lệnh CLI trên terminal
Cung cấp một trình giả lập (simulator giúp bạn thử nghiệm và sửa lỗi nhanh.
Hỗ trợ Json format
...
Tiếp theo, chúng ta cùng dạo một vòng Wing qua một khóa học của họ: https://www.winglang.io/learn/
Thực hành
Mục tiêu: Xây dựng một ""smart queue"". Mỗi một message nằm trong queue sẽ được lưu vào file latest.txt và đẩy lên bucket của AWS.
latest.txt
Khởi tạo tài nguyên Cloud

Ở đây tôi sẽ tạo 1 object cloud.Queue bằng từ khóa new. Kết quả sẽ hiện lên trình giả lập của Wing
cloud.Queue
new

Như vậy, tôi đã tạo được một ứng dụng cloud đầu tiên với Wing
Đẩy message vào queue
Trong trình giả lập ở hình trên, tôi có thể tương tác với node màu xanh lá cây Queue bằng cách ấn vào. Khi ấn vào sẽ hiện lên panel phía bên tay phải.
Queue

Điền một cái message chà bá vào cái khung Push Message rồi ấn nút Push để đẩy message vào queue. Ở đây tôi điền 2 messages thôi: ""First time"" và ""Second time"". Để ý nó có timeout 30s, sau 30s queue sẽ tự động xóa message.
Push Message
Push
Gọi hàm và hiển thị log
Ở bước này, tôi sẽ tạo cloud.Function. Đây là một tài nguyên của cloud, thực thi code và phản hồi kết quả. Tôi sẽ dùng function này để đẩy message vào queue. Trong hàm dưới đây có từ khóa inflight biểu thị là đoạn code này sẽ được thực thi sau khi hệ thống được deploy
cloud.Function
inflight

Node xanh nước biển Function được tạo ra. Khi ấn vào sẽ hiện lên panel bên tay phải, sau đó ấn vào nút Invokesẽ thực thi hàm và hiển thị log bên dưới.
Function
Invoke

Đẩy message vào queue bằng function
Trong hàm mà tôi đã nêu bên trên, thêm dòng q.push(s) nhằm gửi payload của hàm (message) tới queue.
q.push(s)

Sau khi thêm dòng code kia thì trình giả lập sẽ hiển thị mối quan hệ giữa node Function và node Queue. Điều này rất quan trọng khi bạn làm việc với Cloud, bởi vì các dịch vụ của Cloud kết nối với nhau và tạo ra các stack layer, không những thế mối quan hệ tự động suy ra chính sách bảo mật (IAM).
Function
Queue

Lưu message cuối tới bucket
Ở bước gần cuối này, tôi sẽ dùng phương thức queue.setConsumer() để đăng ký một handler. handler này sẽ được gọi mỗi khi một message mới được đẩy vào queue.
queue.setConsumer()
Tôi cũng sẽ tạo một bucket mới và đẩy file chứa message cuối cùng lên bucket này.


Sau khi viết xong đoạn code trên hãy kiểm tra xem trình giả lập thay đổi như thế nào. Sơ đồ sẽ hiển thị mối quan hệ của Function, Queue và Bucket.
Khi bạn điền message vào payload của Function rồi ấn invoke thì hãy kiểm tra Bucket bằng cách ấn vào latest.txt, file này sẽ hiển thị message mà mình vừa điền ở Function.
Function
Queue
Bucket
Function
Bucket
latest.txt
Function
Compile bằng Terraform
Sau khi xây dựng và thử nghiệm code xong, tôi sẽ compile nó lại để deploy lên cloud.

Như các bạn thấy thì đoạn code vừa rồi đã được trình giả lập biên dịch lại thành tài nguyên (assets) trong Terraform. Sau đó thì bạn có thể dùng Terraform để cấu hình cơ sở hạ tầng và thực thi code trên Cloud Platform.
Cám ơn các bạn đã đọc đến đây.
Bài viết này chỉ giới thiệu đơn giản về Wing. Để đào sâu thêm thì cần phải thực hành nhiều hơn và đọc tài liệu do nhà phát triển Wing cung cấp. Biết đâu tôi lại cho ra lò thêm bài viết về Wing trong tương lai. Mà thực ra bí ý tưởng viết bài thì tôi cũng quay lại chủ đề này mà thôi 🤣
Tôi thấy ngôn ngữ này tiềm năng phết, nếu ông nào cảm thấy khó khăn trong việc thử nghiệm và sửa lỗi trên Cloud Platform thì có thể thử trên trình giả lập của Wing trước xem sao.
Tóm lại, ai đi qua cho tôi xin 1 upvote, tuy không biết upvote được cái gì không nhưng thấy mọi người like bài viết là tâm trạng tốt hơn hôm sau rồi 🙇
https://github.com/winglang/wing
https://www.winglang.io/docs/",https://viblo.asia/p/wing-ngon-ngu-sinh-ra-cho-cloud-vlZL98leJQK
82,Unit Of Work với Repository Pattern - C#,"Trong bài viết này, tôi sẽ thảo luận về cách thực hiện Đơn vị Công việc (Unit Of Work) trong Mẫu Repository Pattern trong C# với các Ví dụ. Đơn vị công việc trong C# quản lý các hoạt động CRUD trên cơ sở dữ liệu trong bộ nhớ như một giao dịch. Nói một cách đơn giản, chúng ta có thể nói rằng nếu chúng ta muốn triển khai Giao dịch khi sử dụng Entity Framework và Mẫu Thiết kế Repository, thì chúng ta cần sử dụng đơn vị công việc. Vì vậy, nếu một trong những hoạt động thất bại như một phần của giao dịch, tất cả các hoạt động cơ sở dữ liệu sẽ được quay trở lại. Điều đó có nghĩa là hoặc tất cả các hoạt động cơ sở dữ liệu thành công hoặc không có hoạt động nào.
Đơn vị công việc trong C# là một khái niệm liên quan đến việc thực hiện hiệu quả Mẫu Thiết kế Repository. Vì vậy, để hiểu khái niệm này, quan trọng là phải hiểu khái niệm Mẫu Thiết kế Repository trong C#.
Repository là gì?
Như đã thảo luận trước đó, repository là một lớp được xác định cho một thực thể với tất cả các hoạt động cơ sở dữ liệu có thể có. Ví dụ, một repository cho một thực thể Nhân viên sẽ có các hoạt động CRUD cơ bản và bất kỳ hoạt động khác nào có thể liên quan đến thực thể Nhân viên. Mẫu Repository có thể được triển khai theo hai cách, tức là sử dụng Một Repository Cho Mỗi Thực Thể và Một Repository Cho Tất Cả Thực Thể. Cũng có thể bao gồm cả hai trong ứng dụng của chúng ta.
Một Repository Cho Mỗi Thực Thể (Repository Không Generic):
Trong trường hợp này, chúng ta phải tạo một repository riêng cho mỗi thực thể. Ví dụ, nếu chúng ta có hai thực thể, Nhân viên và Khách hàng, trong ứng dụng của chúng ta, chúng ta phải tạo hai repository. Repository Nhân viên sẽ có các hoạt động liên quan đến Thực thể Nhân viên và Repository Khách hàng sẽ chỉ có các hoạt động liên quan đến Thực thể Khách hàng.
Repository Generic (Một Repository Cho Tất Cả Thực Thể):
Một Repository Generic có thể được sử dụng cho tất cả các thực thể. Nói cách khác, Repository Generic có thể được sử dụng cho Thực thể Nhân viên, Thực thể Khách hàng hoặc bất kỳ thực thể nào khác. Vì vậy, tất cả các hoạt động tiêu chuẩn của tất cả các thực thể sẽ được đặt trong Repository Generic.
Lưu Ý: Chúng ta sử dụng cả Repository Generic và Không Generic trong hầu hết Ứng dụng Thời Gian Thực. Repository Generic chứa các phương thức chung cho tất cả các thực thể. Nhưng nếu bạn muốn một số hoạt động cụ thể cho một số thực thể cụ thể. Sau đó, bạn cần tạo một repository cụ thể với các hoạt động cần thiết.
Unit of work trong Mẫu Repository Pattern
Mẫu Đơn vị Công việc trong C# nhóm một hoặc nhiều hoạt động (thường là hoạt động CRUD cơ sở dữ liệu) vào một giao dịch duy nhất và thực hiện chúng bằng cách áp dụng nguyên tắc làm tất cả hoặc không làm gì cả. Điều đó có nghĩa là nếu bất kỳ hoạt động nào của giao dịch thất bại, nó sẽ quay trở lại giao dịch. Nếu tất cả các hoạt động đều thành công, thì nó sẽ xác nhận giao dịch. Vì vậy, nó sẽ thực hiện tất cả các hoạt động cơ sở dữ liệu như một đơn vị.
Chúng ta sẽ làm việc với cùng một ví dụ mà chúng ta đã sử dụng trong bài viết trước của chúng tôi. Dưới đây là đoạn mã cho lớp Repository Generic.
// Đoạn mã cho lớp Repository Generic
using RepositoryUsingEFinMVC.DAL;
using System.Collections.Generic;
using System.Data.Entity;
using System.Linq;

namespace RepositoryUsingEFinMVC.GenericRepository
{
    public class GenericRepository<T> : IGenericRepository<T> where T : class
    {
        public EmployeeDBContext _context = null;
        public DbSet<T> table = null;

        public GenericRepository()
        {
            this._context = new EmployeeDBContext();
            table = _context.Set<T>();
        }

        public GenericRepository(EmployeeDBContext _context)
        {
            this._context = _context;
            table = _context.Set<T>();
        }

        public IEnumerable<T> GetAll()
        {
            return table.ToList();
        }

        public T GetById(object id)
        {
            return table.Find(id);
        }

        public void Insert(T obj)
        {
            table.Add(obj);
        }

        public void Update(T obj)
        {
            table.Attach(obj);
            _context.Entry(obj).State = EntityState.Modified;
        }

        public void Delete(object id)
        {
            T existing = table.Find(id);
            table.Remove(existing);
        }

        public void Save()
        {
            _context.SaveChanges();
        }
    }
}
Vấn đề với mã Triển khai trên:
Vấn đề phát sinh khi chúng ta làm việc với nhiều repository. Ví dụ, nếu chúng ta làm việc với hai repository, hãy nói Nhân viên và Sản phẩm, thì trong trường hợp này, cả hai repository sẽ tạo ra và duy trì một phiên bản riêng của lớp DbContext. Điều này có thể dẫn đến vấn đề trong tương lai vì mỗi đối tượng DbContext sẽ có danh sách thay đổi trong bộ nhớ riêng của nó, tức là duy trì trạng thái của các thực thể, tức là Thêm / Sửa đổi / Xóa. Trong trường hợp như vậy, nếu phương thức SaveChanges của một trong các repository thất bại và phương thức SaveChanges của một trong các repository khác thành công, điều này sẽ dẫn đến sự không nhất quán trong cơ sở dữ liệu. Trong ví dụ của chúng tôi, trong khi thêm dữ liệu cho các thực thể Nhân viên và Sản phẩm, cả hai đều sử dụng một phiên bản riêng của lớp DbContext. Vì vậy, nếu phương thức SaveChanges của repository Nhân viên thất bại, nhưng phương thức SaveChanges của repository Sản phẩm thành công, dữ liệu Sản phẩm sẽ được thêm vào cơ sở dữ liệu nhưng dữ liệu Nhân viên sẽ không được thêm vào cơ sở dữ liệu.
Để giải quyết vấn đề này, chúng ta cần sử dụng đơn vị công việc.
Triển Khai Đơn vị Công việc:
Đơn vị công việc sẽ duy trì một phiên bản của lớp DbContext trong một ứng dụng và sẽ sử dụng nó để thực hiện các hoạt động của cơ sở dữ liệu cho tất cả các repository trong ứng dụng. Điều này đảm bảo rằng tất cả các repository sẽ làm việc với cùng một phiên bản của DbContext và do đó, tất cả các hoạt động cơ sở dữ liệu sẽ được thực hiện trong một giao dịch. Điều này đảm bảo rằng nếu bất kỳ hoạt động nào của giao dịch thất bại, nó sẽ quay trở lại giao dịch. Nếu tất cả các hoạt động đều thành công, thì nó sẽ xác nhận giao dịch.
Dưới đây là mã cho lớp Unit of work.
using RepositoryUsingEFinMVC.DAL;

namespace RepositoryUsingEFinMVC.UnitOfWork
{
    public class UnitOfWork : IDisposable
    {
        private EmployeeDBContext _context = new EmployeeDBContext();
        private GenericRepository<Employee> employeeRepository;
        private GenericRepository<Product> productRepository;

        public GenericRepository<Employee> EmployeeRepository
        {
            get
            {
                if (this.employeeRepository == null)
                    this.employeeRepository = new GenericRepository<Employee>(_context);
                return employeeRepository;
            }
        }

        public GenericRepository<Product> ProductRepository
        {
            get
            {
                if (this.productRepository == null)
                    this.productRepository = new GenericRepository<Product>(_context);
                return productRepository;
            }
        }

        public void Save()
        {
            _context.SaveChanges();
        }

        private bool disposed = false;

        protected virtual void Dispose(bool disposing)
        {
            if (!this.disposed)
            {
                if (disposing)
                {
                    _context.Dispose();
                }
            }
            this.disposed = true;
        }

        public void Dispose()
        {
            Dispose(true);
            System.GC.SuppressFinalize(this);
        }
    }
}
Như bạn có thể thấy trong đoạn mã trên, lớp UnitOfWork sử dụng lớp DbContext (_context) để tạo ra các repository và thực hiện hoạt động cơ sở dữ liệu. Lớp UnitOfWork này sẽ đảm bảo rằng tất cả các repository trong ứng dụng sẽ làm việc với cùng một phiên bản của DbContext và do đó, tất cả các hoạt động cơ sở dữ liệu sẽ được thực hiện trong một giao dịch.
Kết luận:
Trong bài viết này, chúng ta đã thảo luận về cách thực hiện Đơn vị Công việc (Unit Of Work) trong Mẫu Repository Pattern trong C# với các Ví dụ. Như bạn đã thấy, Đơn vị Công việc trong C# nhóm một hoặc nhiều hoạt động (thường là hoạt động CRUD cơ sở dữ liệu) vào một giao dịch duy nhất và thực hiện chúng bằng cách áp dụng nguyên tắc làm tất cả hoặc không làm gì cả. Điều đó có nghĩa là nếu bất kỳ hoạt động nào của giao dịch thất bại, nó sẽ quay trở lại giao dịch. Nếu tất cả các hoạt động đều thành công, thì nó sẽ xác nhận giao dịch. Vì vậy, nó sẽ thực hiện tất cả các hoạt động cơ sở dữ liệu như một đơn vị.",https://viblo.asia/p/unit-of-work-voi-repository-pattern-c-x7Z4DAE2JnX
83,"Nếu được quay trở lại, mình sẽ bắt đầu tự học lập trình như thế nào?","Mở đầu
Nếu được quay trở lại, mình sẽ không học lập trình nữaaaaaa 🐧
Nếu được quay trở lại, mình sẽ không học lập trình nữaaaaaa 🐧
I'm just kidding 😅 Học chứ, càng ngày càng cảm thấy có vô vàn kiến thức hay ho để mình học trong ngành này.
I'm just kidding 😅 Học chứ, càng ngày càng cảm thấy có vô vàn kiến thức hay ho để mình học trong ngành này.
Mục đích chính của bài viết này là để tiếp thêm động lực cho những anh em đang là học sinh, sinh viên, đang đôi lúc cảm thấy mông lung về ngành IT. Đương nhiên là quá khứ của mình đã trải qua rồi, có bao nhiêu cái giá như thì cũng không thể quay lại được. Chỉ có thể bước tiếp về phía trước mà thôi.
Mục đích chính của bài viết này là để tiếp thêm động lực cho những anh em đang là học sinh, sinh viên, đang đôi lúc cảm thấy mông lung về ngành IT. Đương nhiên là quá khứ của mình đã trải qua rồi, có bao nhiêu cái giá như thì cũng không thể quay lại được. Chỉ có thể bước tiếp về phía trước mà thôi.
Mặc dù bài viết chỉ dựa trên kinh nghiệm cá nhân của mình, nhưng nếu bạn đang ở một trong những thời điểm mình nhắc đến ở bên dưới, thì hi vọng bài viết này sẽ phần nào có ích với bạn.
Mặc dù bài viết chỉ dựa trên kinh nghiệm cá nhân của mình, nhưng nếu bạn đang ở một trong những thời điểm mình nhắc đến ở bên dưới, thì hi vọng bài viết này sẽ phần nào có ích với bạn.
Chọn thời điểm bắt đầu
Nếu được quay lại, chắc chắn mình sẽ lựa chọn thời điểm bắt đầu học lập trình là thời học sinh cấp 3.
Nếu được quay lại, chắc chắn mình sẽ lựa chọn thời điểm bắt đầu học lập trình là thời học sinh cấp 3.
Thực tế năm lớp 12 mình lựa chọn thi ĐH ngành IT đơn thuần chỉ là cảm tính, khi xem phim thấy các anh hacker áo đen gõ phím ngầu ngầu.
Thực tế năm lớp 12 mình lựa chọn thi ĐH ngành IT đơn thuần chỉ là cảm tính, khi xem phim thấy các anh hacker áo đen gõ phím ngầu ngầu.
Mình không hề có kiến thức lập trình, cũng không hề quan tâm tìm hiểu cơ hội việc làm hay khó khăn, thách thức gì sẽ có thể phải đối diện ở ngành này. Có thể nói là “bàng quan với tương lai” của chính bản thân mình.
Mình không hề có kiến thức lập trình, cũng không hề quan tâm tìm hiểu cơ hội việc làm hay khó khăn, thách thức gì sẽ có thể phải đối diện ở ngành này. Có thể nói là “bàng quan với tương lai” của chính bản thân mình.
“Giá như” mình không ham chơi trong những tiết Tin học, giá như mình chịu khó tham gia các diễn đàn công nghệ, học hỏi từ những anh chị, bạn bè khác trên internet, giá như mình có trách nhiệm hơn với tương lai của mình, giá như ...
“Giá như” mình không ham chơi trong những tiết Tin học, giá như mình chịu khó tham gia các diễn đàn công nghệ, học hỏi từ những anh chị, bạn bè khác trên internet, giá như mình có trách nhiệm hơn với tương lai của mình, giá như ...
Tiếng Anh
Ngày xưa vì HỌC DỐT HÓA, nên sang năm lớp 11 mình chuyển qua học tiếng Anh để thi khối A1.
Ngày xưa vì HỌC DỐT HÓA, nên sang năm lớp 11 mình chuyển qua học tiếng Anh để thi khối A1.
Thế nhưng mình cũng chỉ học chống chế, với tư duy thiển cận rằng: sau này không ra nước ngoài làm việc thì cần học tốt tiếng Anh làm gì!
Thế nhưng mình cũng chỉ học chống chế, với tư duy thiển cận rằng: sau này không ra nước ngoài làm việc thì cần học tốt tiếng Anh làm gì!
Bây giờ mới thấy, việc không cố gắng học tiếng Anh nghiêm túc từ sớm, đã khiến mình bỏ lỡ rất nhiều cơ hội tiếp cận với những tài liệu học lập trình, những cộng đồng lập trình viên trên thế giới.
Bây giờ mới thấy, việc không cố gắng học tiếng Anh nghiêm túc từ sớm, đã khiến mình bỏ lỡ rất nhiều cơ hội tiếp cận với những tài liệu học lập trình, những cộng đồng lập trình viên trên thế giới.
Những quyển sách chuyên ngành dày cộm, những video dạy lập trình siêu cấp pro từ các pháp sư Ấn Độ mà mình “ngấu nghiến” bây giờ, là những thứ mình chưa từng nghĩ sẽ chạm tay vào khi còn học cấp 3.
Những quyển sách chuyên ngành dày cộm, những video dạy lập trình siêu cấp pro từ các pháp sư Ấn Độ mà mình “ngấu nghiến” bây giờ, là những thứ mình chưa từng nghĩ sẽ chạm tay vào khi còn học cấp 3.
Học C/C++ cơ bản
Việc học lập trình C/C++ sẽ giúp mình hiểu rõ hơn về cách máy tính hoạt động, cùng các khái niệm cơ bản như biến, mảng, con trỏ, quản lý bộ nhớ và phân tích thời gian chạy của chương trình.
Việc học lập trình C/C++ sẽ giúp mình hiểu rõ hơn về cách máy tính hoạt động, cùng các khái niệm cơ bản như biến, mảng, con trỏ, quản lý bộ nhớ và phân tích thời gian chạy của chương trình.
Những kiến thức này sẽ cung cấp một nền tảng vững chắc cho việc học các ngôn ngữ và công nghệ khác sau này.
Những kiến thức này sẽ cung cấp một nền tảng vững chắc cho việc học các ngôn ngữ và công nghệ khác sau này.
Tư duy giải quyết vấn đề. Cấu trúc dữ liệu & Giải thuật
Tư duy giải quyết vấn đề là một kỹ năng RẤT QUAN TRỌNG mà mọi lập trình viên đều cần phải có. Nó giúp mình có thể phân tích và hiểu rõ vấn đề, sau đó tìm ra cách tiếp cận hiệu quả để giải quyết bằng việc lập trình.
Tư duy giải quyết vấn đề là một kỹ năng RẤT QUAN TRỌNG mà mọi lập trình viên đều cần phải có. Nó giúp mình có thể phân tích và hiểu rõ vấn đề, sau đó tìm ra cách tiếp cận hiệu quả để giải quyết bằng việc lập trình.
Trong môi trường làm việc thực tế, các vấn đề thường phức tạp và đòi hỏi một phương pháp có hệ thống để giải quyết. Tư duy giải quyết vấn đề sẽ giúp mình xây dựng được các ứng dụng hữu ích, giải quyết được “điểm đau” của khách hàng.
Trong môi trường làm việc thực tế, các vấn đề thường phức tạp và đòi hỏi một phương pháp có hệ thống để giải quyết. Tư duy giải quyết vấn đề sẽ giúp mình xây dựng được các ứng dụng hữu ích, giải quyết được “điểm đau” của khách hàng.
Nhiều bạn hay nói rằng học lập trình web thì cần gì biết CTDL & GT. Nhưng theo quan điểm của cá nhân mình, học CTDL & GT chính là cách hiệu quả để mình phát triển tư duy giải quyết vấn đề.
Nhiều bạn hay nói rằng học lập trình web thì cần gì biết CTDL & GT. Nhưng theo quan điểm của cá nhân mình, học CTDL & GT chính là cách hiệu quả để mình phát triển tư duy giải quyết vấn đề.
Kỹ năng Debug
Thời sinh viên mình RẤT SỢ BUG. Chạy chương trình mà gặp lỗi là “chân tay bủn rủn, xây xẩm mặt mày”
Thời sinh viên mình RẤT SỢ BUG. Chạy chương trình mà gặp lỗi là “chân tay bủn rủn, xây xẩm mặt mày”
Và để debug, mình luôn sử dụng phương pháp KINH ĐIỂN NHẤT, đó là console log/print những dòng mà mình “nghi nghi” là gây ra lỗi. Và đương nhiên cách này không hề hiệu quả trong hầu hết các trường hợp mình gặp phải.
Và để debug, mình luôn sử dụng phương pháp KINH ĐIỂN NHẤT, đó là console log/print những dòng mà mình “nghi nghi” là gây ra lỗi. Và đương nhiên cách này không hề hiệu quả trong hầu hết các trường hợp mình gặp phải.
Mãi sau này, khi đi thực tập, mình mới được các anh trong công ty training theo kiểu “cầm tay chỉ việc” kỹ năng debug sử dụng công cụ có sẵn trên IDE hoặc Chrome Dev Tools, một bầu trời chân lý lúc đó mới thực sự mở ra trước mắt mình.
Mãi sau này, khi đi thực tập, mình mới được các anh trong công ty training theo kiểu “cầm tay chỉ việc” kỹ năng debug sử dụng công cụ có sẵn trên IDE hoặc Chrome Dev Tools, một bầu trời chân lý lúc đó mới thực sự mở ra trước mắt mình.
Một lý do QUAN TRỌNG nữa, đó là sau này đi làm, có thể mình sẽ debug code không phải do mình viết ra, mà do đồng nghiệp của mình viết, khi ấy mà không có công cụ hỗ trợ thì quả thực là một cực hình 🥴
Một lý do QUAN TRỌNG nữa, đó là sau này đi làm, có thể mình sẽ debug code không phải do mình viết ra, mà do đồng nghiệp của mình viết, khi ấy mà không có công cụ hỗ trợ thì quả thực là một cực hình 🥴
Version Control (Git)
Lại nhớ về hồi mình sinh viên, khi làm bài tập lớn theo nhóm, mình và anh em trong nhóm thường sẽ phân chia mỗi người code 1 phần, rồi gần đến hạn nộp sẽ hẹn nhau ra Circle K Ngõ 1 Ao Sen để ... ghép code.
Lại nhớ về hồi mình sinh viên, khi làm bài tập lớn theo nhóm, mình và anh em trong nhóm thường sẽ phân chia mỗi người code 1 phần, rồi gần đến hạn nộp sẽ hẹn nhau ra Circle K Ngõ 1 Ao Sen để ... ghép code.
Mỗi ông 1 chiếc USB, chọt chọt vào máy thằng trưởng nhóm để copy code sang. Và đương nhiên là khi ghép xong thì code ... không chạy.
Mỗi ông 1 chiếc USB, chọt chọt vào máy thằng trưởng nhóm để copy code sang. Và đương nhiên là khi ghép xong thì code ... không chạy.
Chưa kể xui rủi mà USB ông nào dính virus là thôi thằng trưởng nhóm xác định dính chưởng cùng. Kết quả là chỉ tốn tiền kem cho Circle K chứ chẳng làm được gì ra hồn (nhưng mà kem công nhận ngon nha 🤤)
Chưa kể xui rủi mà USB ông nào dính virus là thôi thằng trưởng nhóm xác định dính chưởng cùng. Kết quả là chỉ tốn tiền kem cho Circle K chứ chẳng làm được gì ra hồn (nhưng mà kem công nhận ngon nha 🤤)
Vậy mới thấu hiểu tầm quan trọng của Version Control như Git là như thế nào.
Vậy mới thấu hiểu tầm quan trọng của Version Control như Git là như thế nào.
Kết bài
Học lập trình là một quá trình liên tục và cần rất nhiều sự kiên trì.
Học lập trình là một quá trình liên tục và cần rất nhiều sự kiên trì.
Đừng ngần ngại tìm kiếm các tài liệu học tập (kể cả là tiếng Anh), và quan trọng là thực hành thường xuyên, tìm cách cải thiện kỹ năng của mình.
Đừng ngần ngại tìm kiếm các tài liệu học tập (kể cả là tiếng Anh), và quan trọng là thực hành thường xuyên, tìm cách cải thiện kỹ năng của mình.
Chúc bạn thành công trên con đường bản thân đã chọn ❤️
Chúc bạn thành công trên con đường bản thân đã chọn ❤️",https://viblo.asia/p/neu-duoc-quay-tro-lai-minh-se-bat-dau-tu-hoc-lap-trinh-nhu-the-nao-oK9VyZX54QR
84,Lập trình bot Viblo battle như thế nào? (phần 1),"Tôi đã tham gia một vài lần các sự kiện của nền tảng Viblo, tuy nhiên lần này là lần đầu tiên xuất hiện thử thách trên nền tảng Viblo battle. Nghe tên khá ngầu đó, tôi bắt đầu tìm hiểu cách chơi của nền tảng này. Sau khi đọc qua hướng dẫn tại https://battle.viblo.asia/games/matrix thì ... khá là mông lung. Do bài viết hướng dẫn chưa phải quá chi tiết (quan điểm cá nhân), cộng thêm bản thân tôi là một người không quá thông minh, gặp những thứ mới phải đọc kỹ, nghiên cứu đi nghiên cứu lại mới hiểu được chứ không tiếp thu nhanh, haizzzz.
Tôi tin rằng sẽ có một số bạn (chắc là thiểu số) cũng khá ""lơ mơ"" khi tiếp xúc tới nền tảng mới lạ này (Chắc là tôi không cô đơn đâu) - cũng chính là lý do bài viết này ra đời. Bài viết sẽ giải thích chi tiết hơn về nền tảng, luật chơi, cách lập trình bot, ... theo góc nhìn trực diện của tôi. Mong rằng bài viết sẽ giúp ích cho các bạn trong sự kiện này!
Lời khuyên: Hãy đọc kỹ luật chơi tại https://battle.viblo.asia/games/matrix, nếu chưa hiểu lắm, bài viết này có lẽ sẽ giúp bạn đó! Ngược lại, vui lòng bỏ qua để không lãng phí thời gian =)))
Các bạn có thể chơi thử chế độ người vs bot để trải nghiệm trước.

Người chơi được cung cấp một bảng 9x9, mỗi lượt có một bộ 1x3 với 3 số ngẫu nhiên (từ 7 đến 10) đưa ra, người chơi được xếp vào một trong 27 vị trí, ví dụ xếp vào vị trí 13 như dưới:

Con bot cũng làm việc tương tự, và trò chơi tính điểm nếu có 3 số liền nhau giống nhau (theo 3 hàng ngang, dọc hoặc chéo):

Sau 27 lượt, ai được điểm cao hơn sẽ thắng. Đối với người vs người hoặc người vs bot thì cứ kéo thả là OK! Hãy chơi thử và đưa ra chiến thuật hợp lý nhé!
Đây là phần chính của bài viết, tập trung vào cách lập trình tự động để 2 con bot solo với nhau. Trong phần 1 tôi sẽ giải thích chi tiết các giá trị input trong mỗi turn, phần 2 sẽ hướng dẫn lập trình một bot đơn giản.
Trong mỗi turn, chúng ta có input (với 2 người chơi) như sau:
2
7 8 10
9 9 8 9 -1 9 9 -1 -1 7 10 7 7 -1 9 7 -1 -1 9 10 8 9 -1 10 10 -1 -1 -1 10 7 9 9 -1 -1 -1 8 -1 10 9 9 8 -1 -1 -1 10 -1 8 8 8 8 -1 -1 -1 7 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
7 9 9 9 -1 7 9 -1 -1 10 10 7 10 -1 9 7 -1 -1 9 10 8 9 -1 10 10 -1 -1 -1 10 7 9 9 -1 -1 -1 8 -1 10 9 9 8 -1 -1 -1 10 -1 8 8 7 8 -1 -1 -1 7 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
Đối với 2 người chơi, input gồm 4 dòng:
Dòng 1: số lượng người chơi (tính bot là 1 người)
Dòng 2: Thẻ 1x3 và 3 giá trị trên thẻ (theo thứ tự từ trên xuống dưới)
Dòng 3: Trạng thái bảng 9x9 hiện tại của người chơi, số -1 nghĩa là ô trống đó đang không có giá trị nào
Dòng 4: Trạng thái bảng 9x9 của đối thủ
Như vậy chương trình của chúng ta cần đọc hết 4 dòng giá trị này. Tôi sử dụng ngôn ngữ C++ quen thuộc
Khai báo một biến N để lưu số người chơi, mảng now[3] để lưu 3 giá trị hiện tại trên thẻ 1x3 của turn, mảng userboard[81] lưu 81 giá trị, do chúng ta chưa cần quan tâm đến bảng hiện tại của đối thủ, nên tạm thời sử dụng một biến temp để nhập vào 81 lần lưu các giá trị trong bảng hiện tại của đối thủ.
N
now[3]
userboard[81]
temp
Giải thích thêm về biến temp: Thực ra, trong dự liệu input có số liệu của bảng đối thủ là để chúng ta tạo ra chiến thuật hợp lý dựa vào ""tình hình hiện tại"". Đây là phần nâng cao hơn, tôi sẽ nhắc đến sau. Hiện tại chúng ta chỉ cần quan tâm làm sao đọc được và đọc đúng dữ liệu thôi =))))
temp
#include <iostream>
using namespace std;

int main() {
    int N, now[3], userboard[81], temp;
    cin >> N;
    cin >> now[0] >> now[1] >> now[2];
    for (int i = 0; i < 81; i++) {
        cin >> userboard[i];
    }
    for (int i = 0; i < 81; i++) {
        cin >> temp;
    }
}
Tuy nhiên, hãy để ý rằng thực tế các thẻ của chúng ta có kích thước 1x3. Trong số 27 turn, chúng ta đặt vào bảng 9x9 là 27 thẻ. Để thống nhất từng thẻ gồm 3 giá trị không bị rời rạc (vì sau này chúng ta còn sử dụng đến các thẻ thì khó điều khiển giá trị). Chúng ta nên tạo biến mảng lưu giá trị bảng hiện tại là mảng 2 chiều userboard[27][3] gồm 27 biến, mỗi biến lưu được 3 giá trị.
userboard[27][3]
Khi đó sửa lại chương trình thành:
#include <iostream>
using namespace std;

int main() {
    int N, now[3], userboard[27][3], temp;
    cin >> N;
    cin >> now[0] >> now[1] >> now[2];
    for (int j = 0; j < 3; j++)
        for (int i = 0; i < 9; i++)
            cin >> userboard[i][0];
    for (int j = 0; j < 3; j++)
        for (int i = 9; i < 18; i++)
            cin >> userboard[i][0];
    for (int j = 0; j < 3; j++)
        for (int i = 18; i < 27; i++)
            cin >> userboard[i][0];
    for (int i = 0; i < 81; i++) {
        cin >> temp;
    }
    return 0;
}
Tạm thời phần 1 tới đây đã đi, phần sau chúng ta sẽ cùng xem xét phương thức xử lý output và phân tích một số chiến thuật đơn giản nhé!",https://viblo.asia/p/lap-trinh-bot-viblo-battle-nhu-the-nao-phan-1-y37LdvdR4ov
85,Publisher in Combine: Unveiling the Publisher Role,"Continue with the Combine series, today we will discuss Publisher. Combine needs something that can model a data stream. This is the role of the Publisher protocol.
Publisher
If you haven’t read my article about Combine in general, let’s check it out: Introduction to Combine in iOS. What is Combine
Now we will focus on types of Publisher regularly used and how to use them
Publishers
Publisher from a Sequence
A data stream can be viewed as a sequence of events over time. Therefore a Sequence is a great source for a simple publisher. Like this example above, Array conforms to Sequence. Sequence has a property called publisher that creates a publisher that emits the element from the source sequence
publisher
let arrayPublisher = [1, 2, 3, 4, 5].publisher
let arrayPublisher = [1, 2, 3, 4, 5].publisher
Another example:
let stringPublisher = ""Huy Pham"".publisher
let stringPublisher = ""Huy Pham"".publisher
The initial value is a String, after transforming to a publisher, its emitted values are characters
The characteristic of this Publisher type is that it never makes errors, so the data type for Failure is Never
Publisher from transformation
We can create a new publisher by using transform operators
[1, 2, 3].publisher // publisher of integers
    .map(String.init) // now a publisher of strings
    .sink {
        print($0)
    }
    .store(in: &cancellable)
The map function takes the values emitted by the upstream publisher and passes them as input to the String.init function. So the result Publisher now emits a String rather than an Int
String.init
Besides, we can create publishers from other sealed struct of Publishers, be able to transform upstream data to your expected publisher. See the example below:
Publishers
// combine latest operator
let publisher1 = PassthroughSubject<Int, Never>()
let publisher2 = PassthroughSubject<String, Never>()
let combined = Publishers.CombineLatest(publisher1, publisher2)

// create a sequence by sequence operator 
let numbers = [1, 2, 3, 4, 5]
let sequencePublisher = Publishers.Sequence<[Int], Never>(sequence: numbers)
It’s we simulate the operators. I will have an article next week giving more detail about Combine’s operators.
Publisher from the Class’s property
@Published is a property wrapper that makes any property using it Publisher
@Published
Publisher
This wrapper is class-constrained, meaning that you can only use it in instances of a class
class ViewModel {
    @Published var title = ""the first title""
}
@Published doesn’t need to call send() method or access .value. When you directly change its value, it will update the value. Note that we’re using the dollar sign to access the projected value. If you’re not familiar with this technique, check Apple’s document about Property Wrapper and projected value
@Published
send()
.value
var cancellable = viewModel.$title.sink(receiveValue: { newTitle in
    print(""Title changed to \(newTitle)"")
})
viewModel.title = ""the second title""
// Prints:
// Title changed to the first title
// Title changed to the second title
This approach is very efficient with UIKit and SwiftUI in case you don’t want to change the architecture of your project
Just
Just struct creates a straightforward publisher that will emit one value and then complete
Just
Just(""A"")
Just(""A"")
This is handy for use as the return type of a function or if you just want to emit just 1 value
Future
Future means a publisher that eventually produces a single value and then finishes or fails. It provides closure as Future.Promise has Result type parameter without a return value. In the successful case, the future downstream subscriber receives the element before the publishing stream finishes normally. If the result is an error, publishing terminates with that error.
Future
Future.Promise
Result
func generateAsyncRandomNumberFromFuture() -> Future <Int, Never> {
    return Future() { promise in
        DispatchQueue.main.asyncAfter(deadline: .now() + 2) {
            let number = Int.random(in: 1...10)
            promise(Result.success(number))
        }
    }
}
cancellable = generateAsyncRandomNumberFromFuture()
    .sink { number in print(""Got random number \\(number)."") }
    
// Prints
// Got random number 9
Future can replace the callback of the function, which can allow you to express asynchronous behavior without deeply nested callbacks (callback hell)
Future
Subject
PassthroughSubject
PassthroughSubject can be used to emit values to downstream subscribers. However, it doesn’t store or cache the most recent value
PassthroughSubject
CurrentValueSubject
Like PassThroughSubject, it can emit values to downstream subscribers. However, unlike PassThroughSubject, CurrentValueSubject maintains and provides access to the most recent value it has received. When a new subscriber attaches to a CurrentValueSubject, it immediately receives the current value (if available) before getting any subsequent updates
PassThroughSubject
PassThroughSubject
CurrentValueSubject
CurrentValueSubject
import Combine
class DataManager {
    // Create a PassThroughSubject to emit updates
    let passThroughSubject = PassthroughSubject<String, Never>()
    
    // Create a CurrentValueSubject with an initial value
    var currentValueSubject = CurrentValueSubject<Int, Never>(0)
    
    func updateData(value: String) {
        passThroughSubject.send(value)
    }
    
    func updateCurrentValue(value: Int) {
        currentValueSubject.send(value)
    }
}

let dataManager = DataManager()

let passThroughSubscription = dataManager.passThroughSubject.sink { value in
    print(""Received value from PassThroughSubject: \\(value)"")
}
let currentValueSubscription = dataManager.currentValueSubject.sink { value in
    print(""Received value from CurrentValueSubject: \\(value)"")
}
// subjects emit data
dataManager.updateData(value: ""Hello, World!"")
dataManager.updateCurrentValue(value: 42)
// Prints
// Received value from PassThroughSubject: Hello, World!
// Received value from CurrentValueSubject: 0
// Received value from CurrentValueSubject: 42
The main difference between both subjects is that a PassthroughSubject doesn’t have an initial value or a reference to the most recently published element. Therefore, new subscribers will only receive newly emitted events.
PassthroughSubject
A PassthroughSubject is like a doorbell push button When someone rings the bell, you’re only notified when you’re at home
PassthroughSubject
A CurrentValueSubject is like a light switch When a light is turned on while you’re away, you’ll still notice it was turned on when you get back home.
CurrentValueSubject
Type Erasure
Sometimes you want to subscribe to the publisher without knowing too much about its details or the chain operators create complicated nested type
let publisher = Fail<Int, Error>(error: ErrorDomain.example)
    .replaceError(with: 0)
    .map { _ in ""Now I am a string"" }
    .filter { $0.contains(""Now"") }
The type of this publisher is:
Publishers.Filter<Publishers.Map<Publishers.ReplaceError<Fail<Int, Error>>, String>>
Publishers.Filter<Publishers.Map<Publishers.ReplaceError<Fail<Int, Error>>, String>>
To manage these nested types publishers have the eraseToAnyPublisher() method. This is a form of “type erasure”. This method erases the complex nested types and makes the publisher appear as a simpler AnyPublisher<String, Never> to any downstream subscribers.
eraseToAnyPublisher()
AnyPublisher<String, Never>
With AnyPublisher, can’t call send(_:) method, it’s important to wrap a subject with AnyPublisher to prevent the view from sending events through it. When you use type erasure in the MVVM way, you can change the underlying publisher implementation over time without affecting existing clients.
send(_:)
AnyPublisher
Conclusion
In summary, Combine’s Publishers provides iOS developers with a powerful solution for managing asynchronous events and data streams. Mastering Publishers is essential for creating robust, scalable iOS applications with reactive capabilities.
Thanks for Reading! ✌️
If you have any questions or corrections, please leave a comment below or contact me via my LinkedIn account Pham Trung Huy.
Happy coding 🍻",https://viblo.asia/p/publisher-in-combine-unveiling-the-publisher-role-Rk74a1Xl4eO
86,Cơ bản về Docker,"Docker là một nền tảng công nghệ giúp đơn giản hóa việc xây dựng, triển khai và quản lý ứng dụng bằng cách sử dụng các containers. Nó cho phép bạn đóng gói ứng dụng và tất cả các phụ thuộc của nó vào một container độc lập với môi trường, giúp đảm bảo rằng ứng dụng sẽ hoạt động đúng cách trên mọi máy tính hoặc môi trường máy chủ.
Lợi ích:

Đơn giản hóa việc triển khai ứng dụng
Tiết kiệm tài nguyên
Dễ dàng thiết lập môi trường làm việc.
Tạo ra một môi trường độc lập để phát triển.
Đơn giản hóa việc triển khai ứng dụng
Tiết kiệm tài nguyên
Dễ dàng thiết lập môi trường làm việc.
Tạo ra một môi trường độc lập để phát triển.
Docker hỗ trợ nhiều hệ điều hành khác nhau nên bạn có thể tham khảo việc cài đặt docker cho hệ điều hành của bạn ở đây
Docker hỗ trợ nhiều hệ điều hành khác nhau nên bạn có thể tham khảo việc cài đặt docker cho hệ điều hành của bạn ở đây
Sau khi cài đặt xong bạn có thể kiểm trả bằng cách kiểm tra trên command line:
Sau khi cài đặt xong bạn có thể kiểm trả bằng cách kiểm tra trên command line:
docker version
// or
docker info
Docker Client: là cách mà bạn tương tác với docker thông qua command.
Docker Daemon: lắng nghe các yêu cầu từ Docker Engine thông qua giao thức HTTP hoặc Unix socket và thực hiện yêu cầu.
HTTP
Unix socket
Docker Volumes: để lưu trữ dữ liệu cho việc sử dụng và tạo apps.
Docker Registry: nơi lưu trữ riêng của Docker Images. Images được push vào registry và client sẽ pull images từ registry.
Docker Hub: là một dịch vụ đám mây cho phép  lưu trữ và chia sẻ các container Docker.
Docker Repository: là tập hợp các Docker Images cùng tên nhưng khác tag.
Docker Networking: là tính năng cho phép các container Docker tương tác và giao tiếp với nhau và với mạng bên ngoài một cách linh hoạt.
Docker Compose: Là công cụ để quản lý và triển khai nhiều ứng dụng (docker container) một cách dễ dàng.
Docker Swarm: để phối hợp triển khai container.
Docker Services: hiểu đơn giản là gồm các conntainer của ứng dụng và mỗi service chạy 1 image đã được định nghĩa.
Docker Images
docker image là một file bất biến - không thay đổi, chứa các source code, libraries, dependencies, tools và các files khác cần thiết cho một ứng dụng để chạy.
Do việc chỉ có thể đọc (read-only) nên những images này còn được gọi là snapshots. Chúng đại diện cho một application và virtual environment của nó tại một thời điểm cụ thể. Nó cho phép các developers sử dụng một môi trường thống nhất để phát triển.
Do việc chỉ có thể đọc (read-only) nên những images này còn được gọi là snapshots. Chúng đại diện cho một application và virtual environment của nó tại một thời điểm cụ thể. Nó cho phép các developers sử dụng một môi trường thống nhất để phát triển.
snapshots
Trên bản chất images chỉ là các mẫu nên bạn không thể start hoặc run chúng mà nó chỉ có thể làm mẫu để xây dựng 1 container.
Trên bản chất images chỉ là các mẫu nên bạn không thể start hoặc run chúng mà nó chỉ có thể làm mẫu để xây dựng 1 container.

Container là một thể hiện thực của một image. Nó bao gồm một môi trường chạy độc lập, chứa tất cả các thành phần cần thiết để chạy ứng dụng, bao gồm cả các thay đổi và cấu hình thêm.
Nó là các thực thể di động, có thể tạo, khởi động, dừng và xóa một cách nhanh chóng. Các container độc lập với nhau và với hệ thống host của mình. Nó còn giúp cho việc cài đặt cho các thành viên một cách nhanh chóng.
Các container không giống như máy ảo cần ảo hóa (virtualization) xảy ra ở tầng phần cứng mà chúng ảo hóa ở lớp ứng dụng. Nó dùng 1 máy chia sẻ kernel và làm giả môi trường để chạy một cách độc lập nên không hề tốn tài nguyên như máy ảo.
virtualization

Containers và Images là hai khái niệm cơ bản nhưng rất quan trọng trong Docker, là nền tảng cho việc đóng gói, triển khai và quản lý ứng dụng trong môi trường container. Mỗi container được tạo ra từ một image cụ thể. Một image có thể được sử dụng để tạo nhiều containers khác nhau, mỗi container có thể chạy với một tập hợp cấu hình và môi trường khác nhau.
Và Image có thể tồn tại mà không cần container, trong khi container chạy thì cần có image đã tồn tại. Vì vậy, container phụ thuộc vào image và sử dụng nó để tạo ra run-time environment và chạy ứng dụng trên đó.
Dockerfile là một tệp văn bản chứa một loạt các chỉ thị (instructions) cho quá trình xây dựng một Docker image. Nó được sử dụng để định nghĩa cách mà image sẽ được tạo ra, bao gồm các bước để cài đặt các phần mềm, cấu hình môi trường, và chạy các lệnh khởi động.
instructions
Cấu trúc file:

FROM: Chỉ định base image.
RUN: Chạy một lệnh trong môi trường của container, thường được sử dụng để cài đặt các gói phần mềm hoặc thực thi các lệnh cần thiết.
COPY hoặc ADD: Sao chép các tệp và thư mục từ máy host vào container.
WORKDIR: Xác định thư mục làm việc mặc định cho các lệnh tiếp theo trong Dockerfile.
EXPOSE: Khai báo các cổng mà container sẽ lắng nghe các kết nối từ bên ngoài.
CMD/ENTRYPOINT: Xác định lệnh mặc định sẽ được thực thi khi container được khởi chạy.
FROM: Chỉ định base image.
RUN: Chạy một lệnh trong môi trường của container, thường được sử dụng để cài đặt các gói phần mềm hoặc thực thi các lệnh cần thiết.
COPY hoặc ADD: Sao chép các tệp và thư mục từ máy host vào container.
WORKDIR: Xác định thư mục làm việc mặc định cho các lệnh tiếp theo trong Dockerfile.
EXPOSE: Khai báo các cổng mà container sẽ lắng nghe các kết nối từ bên ngoài.
CMD/ENTRYPOINT: Xác định lệnh mặc định sẽ được thực thi khi container được khởi chạy.
Ví dụ: Tạo ra một image chứa một ứng dụng web Node.js
FROM node:alpine

# Đặt thư mục làm việc mặc định
WORKDIR /app

# Sao chép package.json và package-lock.json vào container
COPY package*.json ./

# Cài đặt dependencies
RUN npm install

# Sao chép các file trong thư mục nguồn vào container
COPY . .

# Mở cổng 3000 để truy cập ứng dụng web
EXPOSE 3000

# Chạy lệnh để khởi động ứng dụng web
CMD [""node"", ""app.js""]
Để chạy file trên bạn cần cd tới app của mình sau đó chạy lệnh sau trên teminal:
docker build -t myapp .

docker run -p 3000:3000 myapp
trong đó:
docker build dùng để build docker image từ dockerfile.
docker build
-t myapp: đặt tên và tag cho image là myapp
-t myapp
myapp
. chỉ đường dẫn tới thư mục chứa Dockerfile (trong trường hợp Dockerfile được đặt cùng với thư mục bạn đang ở).
.
docker run để chạy docker container từ image vừa tạo.
docker run
-p 3000:3000 mở cổng 3000 trên máy host và liên kết với cổng 3000 trong container.
-p 3000:3000
myapp là tên của image bạn đã build trước đó.
myapp
Sau khi đã chạy docker lên bạn chỉ cần sh vào container để chạy các app của mình như bình thường.
sh
docker exec -it myapp sh
Trên đây là một vài khái niệm cơ bản về docker. Mong rằng chúng giúp cho bạn hiểu rõ hơn về docker. Tìm hiểu thêm về docker tại đây.",https://viblo.asia/p/co-ban-ve-docker-2oKLnG2gVQO
87,Deploy ELK Stack với Docker,"Cập nhật gần nhất: 10/11/2024
Hello các bạn lại là mình đây 👋👋 Chúc các bạn có kì nghỉ 30/4-1/5 vui vẻ và an toàn 🇻🇳🇻🇳🇻🇳🇻🇳🇻🇳
Tiếp tục series học Docker và CICD của mình, hôm nay ta sẽ cùng nhau làm một bài ""tàu nhanh"" setup ELK Stack bao gồm Elastic Search, Logstash, Kibana và Filebeat để monitor log của ứng dụng NodeJS. Và từ đó ta thấy được ứng dụng Docker vào thực tế sẽ làm giảm cực kì đáng kể thời gian setup infra (infrastructure) nhiều như thế nào nhé
infra
Lên tàu thôi nào anh em ơi 🛳️🛳️🛳️🛳️🛳️
Như thường lệ thì đầu tiên các bạn clone source code của mình ở đây, nhánh master và folder docker-elk-node nhé
Đầu tiên ta chạy thử lên xem project có gì nha:
docker compose up -d
Sau đó ta mở trình duyệt ở địa chỉ http://localhost:3000 hoặc http://localhost:3000/users sẽ thấy như sau:
http://localhost:3000
http://localhost:3000/users

Ta thử F5 vài cái nhé.
Giờ xem tổng quan project có gì nha:
Ở bài này ta có app NodeJS, rất đơn giản
Mỗi khi có request vào server (load trang, CSS/JS, API...) thì sẽ ghi vào file log trên server, nằm ở folder logs
logs
{""level"":""\u001b[32minfo\u001b[39m"",""message"":""::ffff:192.168.65.1 - - [29/Apr/2024:14:31:57 +0000] \""GET / HTTP/1.1\"" 200 204 \""-\"" \""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\""\n"",""timestamp"":""2024-04-29T14:31:57.843Z""}
{""level"":""\u001b[32minfo\u001b[39m"",""message"":""::ffff:192.168.65.1 - - [29/Apr/2024:14:31:57 +0000] \""GET /stylesheets/style.css HTTP/1.1\"" 200 111 \""http://localhost:3000/\"" \""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\""\n"",""timestamp"":""2024-04-29T14:31:57.853Z""}
{""level"":""\u001b[32minfo\u001b[39m"",""message"":""::ffff:192.168.65.1 - - [29/Apr/2024:14:31:58 +0000] \""GET / HTTP/1.1\"" 304 - \""-\"" \""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\""\n"",""timestamp"":""2024-04-29T14:31:58.407Z""}
Ở trên mỗi dòng log của chúng ta là 1 JSON object được cấu hình mặc định bởi library morgan, bạn có thể xem thêm ở file app.js nhé
morgan
app.js
Trong bài hôm nay ta sẽ setup để đẩy phần log kia sang ELK và query/monitor nhé. Triển thôi nào 🚀🚀
Kiến thức vỡ lòng
Cho bạn nào chưa biết thì ELK là một set bao gồm 3 thành phần:
Elastic Search: distributed, chuyên để lưu, search và phân tích data
Logstash: là 1 cái data processing pipeline, chuyên nhận data từ nhiều nguồn, transform sau đó gửi nó đi - thường là tới Elasticsearch
Kibana: là tool chuyên để visualize data trên web UI
Filebeat thì là một thanh niên shipper, chuyên đi ship log, đẩy đi các nơi, trong bài này là đẩy sang Logstash
Bộ ELK rất phổ biến và được nhiều người dùng vì độ mạnh mẽ của nó, khả năng scale tốt và rất nhiều tính năng 💪💪💪
Trong bài này thì ta sẽ setup ELK với flow như sau:

User truy cập app NodeJS -> Ghi ra log file -> mount log file vào Filebeat -> đẩy log qua Logstash, xử lý log chút -> đẩy tiếp qua Elastic -> từ Kibana truy vấn vào Elastic lấy data và hiển thị lên UI
Phần chính
Đầu tiên các bạn tạo cho mình folder elk ở root folder project để lưu cấu hình cho bài này và mount volume nữa nhé
elk
Trong folder elk đầu tiên các bạn tạo mình file filebeat.yml với nội dung như sau:
elk
filebeat.yml
filebeat.inputs:
  - type: filestream
    id: my-log-input
    paths:
      - ""/var/log/server/*.log""

output.logstash:
  hosts: [""logstash:5044""]
Ở trên ta có cấu hình cho Filebeat với nội dung như sau:
Ta định nghĩa 1 input với type là filestream, với type này thì filebeat sẽ đọc từng dòng (line by line từ các file log của ta)
input
filestream
filestream yêu cầu ta cần có id nên ta sẽ cho nó 1 cái id nào đó, cần phải unique nhé các bạn
id
Tiếp đó là ta có paths khai báo đường dẫn mà ta lưu file log để Filebeat có thể đọc
paths
Cuối cùng là khai báo địa chỉ của Logstash để Filebeat gửi log sang
Tiếp theo, vẫn ở folder elk ta tạo file logstash.conf với nội dung như sau:
elk
logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
	json {
		source => ""message""
		target => ""parseJson""
	}
	grok {
		match => {
			""message"" => ""%{IPV6:ipv6}:%{IPV4:ipv4} - - \[%{DATA:parsed_timestamp}\] \\""%{WORD:verb} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}\\"" %{NUMBER:response}""
		}
	}
	date {
		match => [ ""parsed_timestamp"" , ""dd/MMM/yyyy:HH:mm:ss Z"" ]
	}
}

output {
	elasticsearch {
		hosts => ""${ELASTIC_HOSTS}""
		user => ""${ELASTIC_USER}""
		password => ""${ELASTIC_PASSWORD}""
		index => ""my-log-%{+YYYY.MM.dd}""
	}
}
Ở trên, đầu tiên ta khai báo Beats plugin để nhận log từ Filebeat, Logstash sẽ listen ở port 5044 và Filebeat sẽ gửi log vào đây
5044
Tiếp đó ta có filter để xử lý data chút trước khi gửi sang Elasticsearch
filter
Đầu tiên ta có filter json, dùng để parse từng dòng log nhận được thành JSON object phía nodejs lưu là object rồi nhưng vì nó là log nên gửi sang Logstash nó chỉ như 1 text bình thường thôi nên cần phải parse. Lúc parse thì đọc lấy trường message và lưu nó vào trường parseJson (mình lấy tên khác để khi demo cho các bạn được rõ 😁
json
message
parseJson
Tiếp theo ta có grok > match đây là một cái matching tool dùng khá ổn để ta có thể extract lấy thông tin từ trong message theo 1 cái pattern mà ta mong muốn. Kiểu đưa dữ liệu từ ""unstructure"" thành ""structure""
grok
match
message
Sau đó ta có filter date, nó sẽ parse trường parsed_timestamp mà ta lấy được từ cái grok > match bên trên, parse từ string thành date, để truy vấn cho tiện
date
parsed_timestamp
grok > match
Cuối cùng là ta khai báo địa chỉ của elasticsearch để Logstash gửi log sau khi xử lý sang. Log khi gửi tới Elastic sẽ được index với tên theo dạng my-log-{thời_gian}, mỗi ngày sẽ có một index riêng. Ở đây ta cũng cần phải có thông tin user/pass của user elastic để xác thực với elastic search, thông tin này ta sẽ lấy ở biến môi trường tí nữa ta truyền vào
my-log-{thời_gian}
Sau đó vẫn ở trong elk các bạn tạo cho mình folder data để lát lưu data cho elastic nha
elk
data
Oke rồi đó, hiện tại thì folder elk của ta có 2 file như sau:
elk

Giờ ta tạo file docker-compose.yml ở root folder project nha:
docker-compose.yml
services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 3000:3000
    volumes:
      - ./logs:/app/logs

  elastic:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.2
    environment:
      - ELASTIC_PASSWORD=myelasticpass
      - KIBANA_PASSWORD=kibanapass
      - discovery.type=single-node
      - xpack.security.http.ssl.enabled=false
      - cluster.routing.allocation.disk.threshold_enabled=false
    volumes:
      - ./elk/data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:8.12.2
    volumes:
      - ./elk/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    environment:
      - ELASTIC_USER=elastic
      - ELASTIC_PASSWORD=myelasticpass
      - ELASTIC_HOSTS=http://elastic:9200
    depends_on:
      - elastic

  kibana:
    image: docker.elastic.co/kibana/kibana:8.12.2
    environment:
      SERVER_NAME: kibana
      ELASTICSEARCH_HOSTS: '[""http://elastic:9200""]'
      ELASTICSEARCH_USERNAME: kibana_system
      ELASTICSEARCH_PASSWORD: kibanapass
    ports:
      - ""5601:5601""
    depends_on:
      - elastic
      
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.12.2
    volumes:
      - ./elk/filebeat.yml:/usr/share/filebeat/filebeat.yml
      - ./logs:/var/log/server
Một trong những điều đầu tiên ta chú ý là giờ khi viết docker-compose.yml thì ở đầu file ta không cần khai báo version: '3.x' nữa rồi 😎
docker-compose.yml
version: '3.x'
Ở trên ta có những thứ sau:
service elastic có limit RAM là 1G, có các biến môi trường để set password cho elastic, cho kibana (tí nữa ta phải chui vào để đổi pass cho kibana), và ở bài này ta chỉ có 1 node elastic nên ta phải có discovery.type=single-node. Ta cũng cần phải có xpack.security.http.ssl.enabled=false nếu không thì khi kibana gọi tới elastic nó sẽ báo lỗi SSL received plaintext http traffic on an https channel hoặc http client did not trust this server's certificate. Ta cũng set thêm cluster.routing.allocation.disk.threshold_enabled=false bởi vì nếu chẳng may máy các bạn mà disk sắp đầy (> 90%) thì cũng sẽ gặp lỗi high disk watermark [90%] exceeded on
elastic
discovery.type=single-node
xpack.security.http.ssl.enabled=false
received plaintext http traffic on an https channel
http client did not trust this server's certificate
cluster.routing.allocation.disk.threshold_enabled=false
high disk watermark [90%] exceeded on
service logstash, không có gì đặc biệt lắm mình đã giải thích hết ở trên phần cấu hình Logstash rồi, các bạn tự thẩm nha
logstash
kibana: ta cần một số biến môi trường như ELASTICSEARCH_HOSTS để cho kibana biết nơi nó cần gọi tới để query lấy data, kibana sẽ cần xác thực với elastic và ta sẽ dùng tới user kibana_system. Chú ý rằng ta phải dùng user kibana_system này nhé, đó là user mà kibana dùng để giao tiếp với elastic
kibana
ELASTICSEARCH_HOSTS
kibana_system
kibana_system
cuối cùng là Filebeat: ta mount cấu hình filebeat và folder logs được ghi bởi NodeJS vào
Chú ý rằng user kibana_system chưa có pass và tí nữa sau khi start elastic ta phải chui vào đó để set pass cho nó nha
kibana_system
Lằng nhằng quá, start lên thôi nào 😂😂
docker compose up -d
Chú ý: với các bạn đang chạy Docker trên Linux thì khi start Filebeat có thể gặp ra lỗi Exiting: error loading config file: config file (""filebeat.yml"") can only be writable by the owner but the permissions are ""-rw-rw-r--"" (to fix the permissions use: 'chmod go-w /usr/share/filebeat/filebeat.yml'). Thì các bạn sửa lại docker-compose.yml service filebeat cho mình chút nhé:
Exiting: error loading config file: config file (""filebeat.yml"") can only be writable by the owner but the permissions are ""-rw-rw-r--"" (to fix the permissions use: 'chmod go-w /usr/share/filebeat/filebeat.yml')
docker-compose.yml
filebeat
filebeat:
  image: docker.elastic.co/beats/filebeat:8.12.2
  command: filebeat -e -strict.perms=false
  volumes:
    - ./elk/filebeat.yml:/usr/share/filebeat/filebeat.yml
    - ./logs:/var/log/server
Oke tiếp thôi nào, nếu giờ ta truy cập luôn vào Kibana ở http://localhost:5601 sẽ thấy như sau:
http://localhost:5601

Check log Kibana sẽ thấy như sau:
docker compose logs kibana
Kibana không xác thực được với Elastic 😭😭😭
Lí do thì như bên trên mình đã nói, ta cần set password cho user kibana_system😊
kibana_system
Ta chui vào container elastic và set pass cho kibana_system nha:
elastic
kibana_system
docker compose exec elastic sh

curl -X POST -u ""elastic:${ELASTIC_PASSWORD}"" -H ""Content-Type: application/json"" http://localhost:9200/_security/user/kibana_system/_password -d ""{ \""password\"": \""${KIBANA_PASSWORD}\"" }""
Sau khi đổi pass xong thì ta F5 lại phía Kibana thấy như sau là oke rồi nhé 😎:

Ta login với account elastic/myelasticpassnha. Sau khi login thành công thì ta vào được giao diện chính:
elastic/myelasticpass

Ở đó ta có ti tỉ thứ, nhưng bài này ta làm đơn giản thôi nha 😁
Ở phía tay trái các bạn bấm menu và chọn Discover:

Sau đó ta bấm Create data view:
Create data view

Tiếp theo ta nhập thông tin như trong hình:

Name: là tên dashboard của bạn, ta thích đặt như nào cũng được
Name
Index pattern: là pattern  Elastic index của chúng ta, lấy từ file logstash.conf mà ta đã làm từ trước. Ở đây ta để giá trị là my-log-*
Index pattern
logstash.conf
my-log-*
Timestamp field thì ta lấy từ parseJson.timestamp để thời gian khớp với bên NodeJS, vì @timestamp thì nó lại là thời gian phía logstash nhận được event - thời gian đó có thể không khớp với NodeJS vì Filebeat thường batch event và gửi 1 loạt sau mỗi khoảng thời gian
Timestamp field
parseJson.timestamp
@timestamp
logstash
batch
Cuối cùng là ta bấm Save data view to Kibana. Ngay sau đó ta tới trang query của dashboard ta vừa tạo:
Save data view to Kibana

Hiện tại thì chưa có tí data nào, các bạn quay trở lại trình duyệt ở địa chỉ app của chúng ta http://localhost:3000 và F5 nhiều nhiều vào tí để lấy log nhé. Sau đó ta chờ 1 chút để Filebeat gửi log đi và ta quay trở lại Kibana F5 sẽ thấy như sau:
http://localhost:3000

Yeah data lên rồi 🎉🎉🎉🎉🎉
Từ đây ta có thể query tuỳ ý theo các field mà ta đã xử lý trước từ phía logstash nhé, ví dụ như ở đây mình query theo IPV4:

Bài này các bạn tự vọc vạch nha, vì nó có nhiều thứ quá hay ho quá không biết vọc từ đâu 🤣🤣🤣 (và giữ cho bài này ngắn không lê thê như các bài khác 😂😂)
Nếu có vấn đề gì các bạn cứ comment cho mình biết nha
bài này có chữ Deploy nhưng không phải deploy lên server HTTPS các thứ đâu nhé 😁
Deploy
Ta để ý rằng với Docker thì việc triển khai cả 1 stack ELK - Filebeat - Application (nodejs) đã đơn giản đi rất rất rất nhiều. Nếu ta cài trực tiếp vào máy gốc theo cách truyền thống thì chắc máy của chúng ta nát trước khi thành công 🥲
Phewwww, một bài cực ngắn tráng miệng sau những bài lê thê từ đầu series. Qua bài này ta đã biết cách setup ELK với Filebeat để lấy log từ ứng dụng của chúng ta để từ đó ta có thể query/monitor một cách rất trực quan qua Kibana
Thực tế là mình cũng đang dùng cách này để monitor cho app chạy trên production của mình và thấy khá là hiệu quả, setup không nhiều và đủ phục vụ mục đích của mình
Chúc các bạn ngày mới vui vẻ và hẹn gặp lại các bạn vào những bài sau 😇😇😇",https://viblo.asia/p/deploy-elk-stack-voi-docker-Ny0VG7a5VPA
88,ZeroOps with Auto Remediation for K8S,"Trong quá trình vận hành K8s (mà thực ra chỉ là GKE) với vai trò là SRE, mình có phát triển 1 vài tool nhỏ để phục vụ công việc của mình. Hầu hết các tool để giải quyết các vấn đề về cost, automation và đảm bảo độ ổn định cho K8S workload,… . Nhân tiện có chủ đề, mình có dịp kết hợp bộ tool vào thành 1 giải pháp khá hoàn chỉnh
Hiện tại, mình đang quản lý một vài K8s Cluster, trong đó có hệ thống data platform đang sử dụng Spark. Với số lượng lớn Spark job chạy hàng ngày và cần monitoring liên tục để đảm bảo tối thiểu số lượng workload/job fail nhất mà vẫn phải đảm bảo cân bằng về cost. Với công việc chỉ có ngồi monitor với fix job fail trông có vẻ nhàm chán, và hệ thống ngày càng scale thì cần có gì đó để thay đổi việc này?
Mình quyết định phát triển 1 hệ thống gọi là “Auto Remediation” để tự động fix một số lỗi cơ bản cần vận hành trong hệ thống data platform và phát triển thêm để hoạt động với các workload khác trên K8S. Hệ thống này ban đầu để xử lý các vấn đề gặp phải về memory đối với workload chạy trên K8S, đặc biệt là lỗi Out-Of-Memory(OOM). Đây cũng có thể coi là 1 phần của bài toán rightsizing cho workload, khi giải quyết được vấn đề này có thể đem lại:
Giảm infra cost
Giảm nhân lực vận hành hệ thống
Giảm số lượng lỗi trên production, đồng thời giảm các chi phí phát sinh khi lỗi xảy ra
Và mình có thêm thời gian ngủ mỗi ngày 😂
Về tổng quan, Auto Remediation dựa vào log và metrics kết hợp với bộ rule-based classifier để đưa ra các quyết định cụ thể nhắm giải quyết các vấn đề gặp phải. Hệ thống có thể giải quyết được rất nhiều case khác nữa khi phát triển thêm rule-based classifier từ SRE để tự động hóa 1 phần hệ thống. Nội dung tiếp theo mình sẽ giới thiệu vài tool nhỏ mà mình phát triển để giải quyết từng phần.
Từ năm 2023, khi bắt đầu với công việc làm SRE, mình đã phát triển tool này để giải quyết vấn đề recommend cho K8S workload gọi là GRR( GKE resource recommendation - vì mình phát triển cho GKE).
Bạn có thể coi GRR hoạt động như VPA nhưng được mình custom lại cách tính toán recommend dựa vào metrics nhằm phù hợp hơn với nhu cầu sử dụng và update thêm 1 vài tính năng để đảm chạy ổn định khi hệ thống scale, tính chính xác hơn phần memory. Tool này mình có đem đi trình bày tại Viet-OpenInfra 2023, so với version hiện tại, mình có update thêm để xác định thêm phần memory cần để start pod (phục vụ cho Java app)
Tuy có thể giải quyết được vấn đề workload rightsizing nhưng 1 ngày nào đó pod cần thêm memory cũng hơi khó để rollout lại.
Lỗi Out-Of-Memory(OOM) là lỗi khá phổ biến đối với chúng ta khi dùng K8S. Để giới hạn lượng memory của container sử dụng trong K8S, người dùng có thể sử dụng resource limit để. Nhưng nếu memory usage của container vượt limit → Lỗi OOMKiled xuất hiện và app của bạn sẽ bị downtime.
limit
Vậy set limit cao cao chút để không bị OOM là được, khi worker node bị memory pressure → evict pod là được, quá đơn giản 🙂
Thật không may là đám Spark app chạy trên K8S khi submit job sẽ set memory request = memory limit, do đó việc bạn set memory limit đồng thời cũng làm tốn thêm resource. Mà chắc gì 1 ngày đẹp trời nào đó, lượng data tăng đột biến thì config ban đầu cũng không còn đúng nữa và vẫn bị OOM thôi. Ngoài ra app Java chạy trên K8S thường config -XX:MaxRAMPercentage để khai báo max Heap theo memory limit.
-XX:MaxRAMPercentage
Vậy là ngoài đảm bảo memory usage của container không được chạm resource limit còn phải lo thêm Heap memory không vượt ngưỡng max, dính 1 trong 2 case là container restart rồi.
Để giải quyết vấn đề này, mình có phát triển tool để chuyên đi xóa pod khi memory usage của container gần chạm ngưỡng limit và cũng support Java Spring khi Heap memory gần chạm ngưỡng max. Mình tận dụng luôn K8S metrics server để lấy metrics của container memory, đối với Java Spring bạn cần enable thêm Spring Boot Actuator. Thông tin về tool: https://github.com/phamngocsonls/k8s-oom-killer
Hơi lan man về 2 tool 😄, khi kết hợp cả 2 lại và thêm 1 vài luồng mới để để tích hợp với hệ thống hiện tại thành 1 luồng khá đầy đủ như dưới đây:
Architecture
Về mặt kiến trúc của hệ thống hiện tại, mình đang sử dụng trên Google Cloud nên sẽ tận dụng các dịch vụ của Google Cloud.
Mình có 2 GKE cluster:
GKE cluster 1: Triển khai các workload cơ bản, trên GKE cluster này được cài OOM Killer để kiểm soát memory limit của container, tránh bị lỗi OOM xảy ra.
GKE cluster 2: Triển khai Spark cho hệ thống data platform
Rule Execution Engine: Nhận trigger từ Pub/Sub topics được route từ log của các GKE cluster, sau đó xử lý với bộ rule có sẵn. Rule có thể được phát triển từ nhiều SRE tùy thuộc vào các case cụ thể. Rule nhằm phân loại log bao gồm source đến từ đâu, nội dung log và các action cụ thể. Một số lỗi có thể không cần làm gì hoặc cần tăng resource. Sau khi xử lý, kết quả sẽ được đẩy vào database Operational storage. VD một case cụ thể: Log đẩy về có thể là log pod bị xóa bởi tool OOM Killer, trong trường hợp này mình sẽ log lại để luồng tiếp theo xử lý (Nếu việc này lặp lại nhiều có thể cần nâng memory cho workload này.
Config Service: Chứa thông tin về recommend resource cho các workload, được lấy từ tool K8S workload rightsizing
Pensive: Được trigger định kì từ từ Cloud Scheduler, Pensive đọc thông tin từ Operational storage để đưa ra quyết định cụ thể. Nếu workload cần tăng resource, Pensive gọi tới Config Service để lấy config mới cho service và commit vào
GitOps repo để Argo CD thực hiện rollout với cấu hình mới. Đối với Apache Spark job, Airflow sẽ call định kì tới Pensive lấy resource config mới cho Spark job.

Auto Remediation có thể giải quyết được nhiều vấn đề phát sinh khi vận hành K8S trong tương lai khi SRE tham gia phát triển bộ Rule Engine để ngày càng hoàn thiện khi hệ thống ngày càng scale và phát sinh ra nhiều lỗi lặp đi lặp lại. Thời gian đầu phát triển có thể gặp nhiều false positive ảnh hưởng tới production nên mọi người có thể xem xét trước khi triển khai.",https://viblo.asia/p/zeroops-with-auto-remediation-for-k8s-aNj4vDb8L6r
89,Xây dựng Micro-service với gRPC,"Tiếp nối series về DDD, tôi xin phép được giới thiệu với bạn đọc một series nữa, lần này không phải về DDD mà là về Micro-service.
Trong series này tôi muốn chia sẻ với bạn đọc cách xây dựng micro-service bằng giao thức gRPC thông qua một project nho nhỏ. Về cơ bản series sẽ gồm các bài viết như sau:
Bài 1. Tổng quan về gRPC.
Bài 2. Giới thiệu về cấu trúc project.
Bài 3,4,5. Triển khai các services.
Bài 6. Tổng kết.
Rất mong được bạn đọc đón nhận nồng nhiệt. Không chần chừ nữa, sau đây tôi sẽ đi vào bài đầu tiên đó là ""Tổng quan về gRPC"".
RPC là gì?
RPC là tên viết tắt của ""Remote Procedure Call"" - lời gọi thủ tục từ xa. Có thể hiểu nó giống như việc gọi một hàm của server từ phía client.
RPC chỉ sử dụng POST và GET method. Tương tác giữa client và server sẽ thông qua Server-side Interface và Client-side stub.
Server-side Interface
Client-side stub
Khác biệt giữa RPC và REST
REST được thiết kế hướng ""tài nguyên"" - tức là mọi thao tác của REST (thêm mới, sửa, xoá) đều tác động lên tài nguyên (resource).
Và đương nhiên kết quả mà REST sẽ trả về đó là ""resource""
Còn RPC sẽ chỉ thực hiện việc gọi một hàm với 1 chức năng nhất định của phía server và dữ liệu RPC trả về chỉ tuần tuý là giá trị trả về của hàm phía server.
Lấy ví dụ:
Với REST ta có:
GET /posts/1
/posts/1
POST /posts/1
/posts/1
PUT /posts/1
/posts/1
Với RPC ta có:
/posts/1/calculate_score
/posts/1/calculate_score
/posts/1/average_score
/posts/1/average_score
Trong thực tế thì RPC sẽ được triển khai dựa trên REST nên ta có khái niệm RPC-base APIs.

gRPC là gì? Nó có gì khác biệt
gRPC là một open source remote procedure call (RPC) framework có thể chạy ở bất kì đâu. Nó cho phép client và server app có thể tương tác một cách ""minh bạch"".
gRPC là một framework được phát triển bởi Google. Giống như các hệ thống RPCm gRPC cũng định nghĩa interface - bản thân interface này cũng định nghĩa các methods mà client có thể truy cập.
Mặc định gRPC sử dụng protocol buffers như một Interface Definition Language & data exchange format. gRPC hỗ trợ các format như JSON.
Interface Definition Language
File Interface Defintiion Language trong gRPC được định nghĩa bởi file có extension là .prot.
Interface Defintiion Language
.prot
Chúng ta hoàn toàn có thể quản lí file này dựa theo version.
Các loại RPC mà gRPC hỗ trợ gồm:
Unary: Single request từ client - Single response từ server.
Server streaming: Single request từ client, stream messages trả về từ server.
Client Streaming: Stream messages từ client và trả về single response từ server.
Bidirectional streaming: Cả client và server đều gửi stream messages.
gRPC được áp dụng trong thực tế như thế nào
Với hệ thống microservices với nhiều service liên lạc với nhau, việc sử dụng REST sẽ phát sinh một vấn đề đó là encoding/ decoding dữ liệu (JSON data), còn RPC thì không cần phải encoding hay decoding dữ liệu.
Do đó gRPC thường được sử dụng trong các hệ thống microservice cần chịu tải lớn.
Hơn nữa gRPC được xây dựng trên nền tảng HTTP/2 hỗ trợ stream nên nó sẽ thích hợp cho việc stream event trong event sourcing, việc HTTP/2 sử dụng binary data thay vì text cũng sẽ làm tăng tốc độ cho việc liên lạc giữa các services với nhau.
HTTP/2 sử dụng một kiểu dữ liệu do Google phát minh đó là Protobuf - Protocol Buffer, tốc độ encoding/ decoding của nó khi so với text hay JSON sẽ như sau:
Protobuf - Protocol Buffer

Một chú ý khác đó là gRPC nên dùng cho giao tiếp giữa backend - backend thay vì backend-frontend do việc giao tiếp stateful giữa backend-frontend sẽ gây ra các vấn đề ""scale tải"" hoặc ""HOL""
backend - backend
backend-frontend
backend-frontend
Kết phần 1
Trong phần này tôi đã trình bày với bạn đọc:
Khái niệm về RPC
Sự khác biệt giữa RPC và REST
Tổng quan về gRPC framework
Áp dụng gRPC trong thực tế
Hi vọng bạn đọc đã hiểu được phần nào về gRPC để có thể tiếp tục theo dõi các phần tiếp theo trong series lần này. Hẹn gặp lại.
Tham khảo
https://200lab.io/blog/grpc-la-gi/
https://medium.com/javarevisited/what-is-grpc-is-it-better-than-rest-api-58a3b7aff13a",https://viblo.asia/p/xay-dung-micro-service-voi-grpc-E1XVOZvXLMz
90,5 LOẠI TÀI LIỆU MÀ NHÀ PHÂN TÍCH NGHIỆP VỤ TẠO RA,"Bí quyết thành công là nhận ra tiềm năng của tài liệu không chỉ là công cụ tạo hồ sơ mà còn là phương tiện phân tích, suy nghĩ và hỗ trợ ra quyết định cho công ty. Cùng điểm qua năm loại tài liệu yêu cầu khác nhau mà mọi nhà phân tích nghiệp vụ nên làm quen và bắt đầu sử dụng trên phần lớn các loại tài liệu này trong các dự án.
1. Bản đặc tả phạm vi dự án
Bản đặc tả phạm vi dự án là loại tài liệu đầu tiên bạn nên soạn thảo đối với hầu hết mọi loại dự án. Nó thiết lập các thông số của dự án.
Bản đặc tả phạm vi dự án phản hồi một cách hiệu quả các câu hỏi sau trong đặc tả yêu cầu này:
Chúng ta đang cố gắng giải quyết vấn đề gì?
Công ty cần gì?
Mức độ của giải pháp của vấn đề là gì?
Chúng ta giải quyết vấn đề đó từ góc độ cấp cao như thế nào?
Câu trả lời xuất hiện như thế nào?
Câu hỏi cuối cùng là: Việc đầu tư để giải quyết vấn đề đó có đáng không? Dự án này có mang lại lợi tức đầu tư tích cực không?
Tất nhiên, những loại câu hỏi này có thể có câu trả lời theo định dạng hoành tráng trong môi trường linh hoạt. Tuy nhiên, bất kể định dạng, tài liệu hoặc phương pháp nào bạn chọn, bạn phải hiểu rõ vấn đề bạn đang cố gắng giải quyết và giải pháp cấp cao mà bạn nghĩ đến để xác định phạm vi dự án một cách hợp lý và đảm bảo sự hỗ trợ của các bên liên quan cấp cao.
2. Kế hoạch phân tích kinh doanh
Kế hoạch phân tích kinh doanh là loại tài liệu yêu cầu tiếp theo sau khi bạn có phạm vi. Thông thường, một nhà phân tích nghiệp vụ sẽ soạn thảo một kế hoạch nêu chi tiết các quy trình phân tích, xác nhận và xác minh yêu cầu, cũng như ai chịu trách nhiệm về những gì trong khuôn khổ dự án phân tích kinh doanh. Phương pháp phân tích kinh doanh hoặc phát triển phần mềm được công ty sử dụng sẽ thường xuyên đóng vai trò là nền tảng cho kế hoạch phân tích kinh doanh.
3. Tài liệu quy trình kinh doanh
Đã đến lúc bắt đầu xem xét các chi tiết cụ thể khi phạm vi đã được thiết lập, kế hoạch của bạn đã sẵn sàng và bạn đã biết đích đến của mình. Việc phân tích quy trình kinh doanh gần như luôn có ý nghĩa trước khi đi sâu vào giải pháp phần mềm hoặc các yêu cầu chức năng, bất chấp sự thôi thúc liên tục phải làm khác đi.
Sơ đồ dòng quy trình, đôi khi được gọi là sơ đồ quy trình công việc hoặc bản đồ quy trình, được sử dụng để phân tích quy trình kinh doanh. Từ quan điểm của bên liên quan hoặc người dùng cuối, nó hiển thị các nét khái quát của toàn bộ quy trình. Với sự trợ giúp của loại mô hình trực quan này, bạn có thể thu thập nhiều thông tin và thiết lập sự hiểu biết chung về quy trình của mình cho cả trạng thái hiện tại và tương lai một cách nhanh chóng.
Quan điểm kinh doanh của cả quy trình kinh doanh ở trạng thái hiện tại và quy trình kinh doanh ở trạng thái dự định trong tương lai của bạn là rất quan trọng. Khi nói đến loại tài liệu thứ ba này - tài liệu về quy trình kinh doanh. Điều này sẽ giúp hỗ trợ làm sáng tỏ thêm các vấn đề cụ thể mà bạn phải sử dụng công nghệ để giải quyết.
4. Tài liệu yêu cầu chức năng
Không phải tất cả các giải pháp đều yêu cầu phần mềm; trong một số trường hợp, vấn đề kinh doanh có thể được giải quyết bằng cách cập nhật quy trình kinh doanh. Mặc dù không phải lúc nào cũng cần thiết nhưng chúng ta thường xuyên sử dụng công nghệ vì đó là một phần của câu trả lời. Nếu vậy, các yêu cầu chức năng cho dự án sẽ được nhà phân tích nghiệp vụ chỉ định. Đây cũng có thể được gọi là yêu cầu hệ thống, yêu cầu phần mềm, yêu cầu giải pháp hoặc thậm chí là yêu cầu kỹ thuật.
Yêu cầu chức năng xác định khả năng và hoạt động của hệ thống. Chúng thường được thể hiện ở cấp độ mà một người dùng cụ thể, chẳng hạn như người dùng cuối hoặc con người, có thể thực hiện được với hệ thống.Những gì họ thấy trong sự tương tác của họ với hệ thống là một yêu cầu về mặt chức năng.
Có thể còn rất nhiều điều đang diễn ra ở phía sau. Những yêu cầu như vậy sẽ phù hợp hơn với thiết kế hệ thống kỹ thuật. Điều gì đó mà người dùng cuối có thể gặp phải khi xử lý hệ thống phần mềm đó sẽ được coi là yêu cầu chức năng. Chúng có thể được bao gồm trong một loạt các đầu ra riêng biệt cho nhu cầu.
5. Tài liệu yêu cầu thông tin hoặc dữ liệu
Nắm bắt các yêu cầu thông tin hoặc dữ liệu là loại tài liệu yêu cầu cuối cùng. Các thành phần của mô hình thông tin cũng có thể được nhà phân tích nghiệp vụ nhận ra bên cạnh các tính năng hướng tới người dùng của phần mềm. Tài liệu về nhu cầu dữ liệu có một số dạng phổ biến.
Bảng thuật ngữ này là một trong những bảng thuật ngữ đầu tiên. Điều này phục vụ như một định nghĩa về thuật ngữ và từ vựng phổ biến. Để đảm bảo rằng tất cả các bên liên quan của bạn hiểu cùng một định nghĩa về các thuật ngữ như ""khách hàng"", ""tài khoản"" và ""đơn đặt hàng"" và rằng bạn sử dụng chúng một cách nhất quán trong các quy trình kinh doanh và trường hợp sử dụng của mình, bạn sẽ sử dụng điều này để trở nên cực kỳ rõ ràng về thuật ngữ mà công ty bạn đang sử dụng.
Sơ đồ mối quan hệ thực thể là mô hình nhu cầu dữ liệu được sử dụng rộng rãi tiếp theo. Điều này minh họa các ý chính có thể xuất hiện trong từ vựng của bạn, mối quan hệ của chúng và các chi tiết cụ thể được ghi lại cho từng ý tưởng trong hệ thống thông tin.
Ví dụ: bạn có nhận được ngày đặt hàng nếu bạn đã đặt hàng không? Bạn có nhận được liên kết đến các sản phẩm cụ thể không? Đó sẽ là sự kết nối với một thứ khác được gọi là sản phẩm. Thời gian đặt hàng, địa chỉ giao hàng và các chi tiết khác sẽ có sẵn cho bạn. Và những bức ảnh được chụp ở vị trí nào? Sơ đồ mối quan hệ thực thể của bạn đang hiển thị điều đó. Những điều này có thể mang tính kỹ thuật cao nhưng chúng cũng có thể được hoàn thành từ góc độ thương mại trừu tượng hơn, thể hiện ý tưởng và hoạt động kinh doanh của công ty.
Từ điển dữ liệu là loại tài liệu yêu cầu dữ liệu cuối cùng. Sau đó, nó sẽ đi vào chi tiết về các trường hoặc tính năng cụ thể, chẳng hạn như trường có thể dài bao nhiêu. Nó chứa loại dữ liệu nào? Có tồn tại bất kỳ quy tắc kinh doanh hoặc thủ tục xác nhận nào không? Đối với dự án tích hợp hệ thống hoặc di chuyển hệ thống một lần, bao gồm việc di chuyển dữ liệu từ nguồn này sang nguồn khác, bạn có thể xây dựng và kết hợp các từ điển dữ liệu từ các hệ thống khác nhau để chứng minh cách các trường từ hệ thống này sẽ ánh xạ sang hệ thống khác.
6. Chọn tài liệu yêu cầu của bạn một cách có chủ ý
Vừa rồi là phác thảo năm danh mục tài liệu và các loại thông số kỹ thuật khác nhau thuộc từng tiêu đề đó. Một nhà phân tích nghiệp vụ không viết tất cả các tiêu chí này cho mọi dự án. Dựa trên các yêu cầu của dự án của họ, phần lớn các nhà phân tích nghiệp vụ sẽ chọn các thông số kỹ thuật tốt nhất và sửa đổi các mẫu đó theo yêu cầu của các bên liên quan và các cân nhắc của dự án.
Để quyết định phải làm gì tiếp theo, bạn nên đánh giá trạng thái hiện tại của dự án và những gì thực sự cần thiết. Điều quan trọng hơn nữa là bạn phải có một kỹ thuật phân tích kinh doanh được xác định rõ ràng và có mục tiêu, giúp bạn có thể giải quyết dự án với sáng kiến ​​và chiến lược tốt hơn. Hy vọng rằng những chia sẻ của BAC sẽ giúp ích cho bạn. Đừng quên đón đọc các bài viết mới nhất được cập nhật tại BAC's Blog bạn nhé.
https://www.bridging-the-gap.com/
",https://viblo.asia/p/5-loai-tai-lieu-ma-nha-phan-tich-nghiep-vu-tao-ra-yZjJYgxOVOE
91,General knowledge in Cryptography - kiến thức tổng quan trong mật mã học (phần 1),"1. Lịch sử hình thành
Mật mã học (Cryptography) là một lĩnh vực nghiên cứu khoa học đóng vai trò quan trọng và hấp dẫn trong ngành khoa học máy tính (Computer Science) và an toàn thông tin (Infomation Security).
Các vấn đề liên quan đến mật mã đã xuất hiện từ hàng ngàn, chục ngàn năm trước, trước khi khái niệm của nó được định nghĩa. Sự hiện hữu đầu tiên của mật mã học được thể hiện dưới nhu cầu giao tiếp và chia sẻ thông tin của con người thời cổ đại: Các ký hiệu tượng trưng cho sự vật, sự việc, ... có thể coi là một dạng mã hóa, và chỉ những người sử dụng, được tiếp cận chúng mới có thể giải mã và hiểu được ý nghĩa.
(Chữ viết tượng hình của Hy Lạp cổ đại là một dạng mã hóa thông tin)
Thuật ngữ Cryptography có nguồn gốc từ tiếng Hy Lạp, được tạo ra bằng cách kết hợp hai từ kryptós (ẩn) và gráphein (viết ra). Theo thời gian, mật mã học ngày càng đóng vai trò quan trọng trong cuộc sống và xã hội loài người: Từ tác dụng trao đổi thông tin thường ngày cho tới các kỹ thuật mã hóa thông điệp, thư mật, đến những hệ thống bảo mật phức tạp, mang tính lịch sử trong chiến tranh, và cho tới ngày nay, đã phát triển thành một bộ môn khoa học nghiên cứu nhằm đảm bảo an toàn, bảo mật trong quá trình truyền tải thông tin.
(Máy Enigma, được người Đức sử dụng trong Đại chiến thế giới II)
2. Tổng quan chuỗi bài viết Cryptography - Mật mã học
Trong chuỗi bài viết Cryptography - Mật mã học, chúng ta sẽ cùng tìm hiểu về các loại mật mã cổ điển cho tới hiện đại, phân tích các kỹ thuật mã hóa chuyên sâu, tìm hiểu các phương pháp tấn công, và khám phá các ứng dụng thực tế của mật mã học trong đời sống hàng ngày cũng như lĩnh vực công nghiệp. Từ đó hiểu được nguyên lý hoạt động, bản chất của các kỹ thuật mã hóa, giải mã, hướng tới xây dựng và phát triển chuyên sâu trong bộ môn khoa học này.
Với công nghệ hiện tại, quá trình mã hóa và giải mã sẽ được thực hiện dưới dạng lập trình. Với các bạn bắt đầu tiếp xúc tới bộ môn này thường thắc mắc nên sử dụng loại ngôn ngữ lập trình nào. Xuyên suốt trong chuỗi bài viết, chúng ta sẽ sử dụng Python (Phiên bản mới nhất hiện tại là 3.11) để xây dựng các chương trình. Đây cũng là ngôn ngữ được ưa chuộng nhất trong giới mật mã bởi tính đơn giản, dễ lập trình, dễ đọc hiểu và có khối lượng thư viện hỗ trợ phong phú, đa dạng (Tất nhiên không phải là bắt buộc).
Nhìn vào bộ môn toán học hiện nay, những công thức phức tạp về tích phân, nguyên hàm chắc hẳn đã làm cho nhiều bạn trong số chúng ta từng một thời ""đau đầu"". Đứng ở góc nhìn hiện tại trở về thuở sơ khai của môn toán, toán học chỉ đơn giản là những con số cực kỳ đơn giản, cùng với hai phép tính ""cộng"", ""trừ"" cơ bản. Có thể thấy, mọi kiến thức chuyên sâu, phức luôn bắt đầu từ những yếu tố cơ bản nhất. Đối với lĩnh vực mật mã học cũng vậy, chúng ta cần nắm chắc các kỹ thuật mã hóa cơ bản, sơ khai nhất.
1. Bảng mã ASCII
ASCII (American Standard Code for Information Interchange) sử dụng 777 bit nhị phân để mã hóa các ký tự dựa trên bảng chữ cái La Tinh (Các máy tính hiện nay sử dụng 888 bit, bit thứ tám gọi là bit parity dùng để kiểm tra lỗi).
Trong bảng mã ASCII chuẩn có 128128128 ký tự, trong đó: 959595 ký tự in ra được, được đánh số từ 323232 đến 126126126, còn lại là các ký tự đặc biệt cùng với lệnh thực hiện. Với bảng mở rộng hiện nay đã bổ sung thêm và bao gồm 256256256 ký tự.

Chúng ta có thể mã hóa các ký tự bằng bảng ASCII trở thành các số nguyên trong hệ thập phân. Ví dụ, sử dụng bảng trên chúng ta có thể mã hóa từ Viblo thành:
86 105 98 108 111
86 105 98 108 111
Đối với Python, chúng ta có thể sử dụng hàm ord() mã hóa từng ký tự sang mã ASCII dạng thập phân, hàm chr() thực hiện giải mã ngược lại.
ord()
chr()

2. Hexadecimal - Hệ thập lục phân
Với con người, hệ thập phân đã trở nên quen thuộc và rất dễ tiếp cận. Đó là bởi vì từ xa xưa, con người đã sử dụng 101010 ngón tay để đếm số lượng thức ăn. Chúng ta cũng đã vô cùng quen thuộc với 101010 chữ số từ 000 đến 999. Đối với máy tính, do chỉ có thể biểu diễn được hai trạng thái duy nhất (on - off) nên con người đã phát minh ra hệ nhị phân để máy tính có thể ""hiểu"". Vậy thì, vì sao lại tồn tại hệ thập lục phân, nó có gì đặc biệt?
Hexadecimal - Hệ thập lục phân được công ty IBM giới thiệu với thế giới điện toán năm 196319631963. Tên gọi là sự kết hợp từ chữ hexi (tiếng Hy Lạp có nghĩa là sáu) và decimal (tiếng Latin có nghĩa là mười) được biểu diễn bằng 161616 ký tự gồm 101010 ký tự số từ 000 đến 999 và 666 ký tự chữ từ AAA đến FFF.

Hexadecimal giúp biểu diễn hệ nhị phân một cách ngắn gọn và trực quan hơn, giúp giảm tải lượng tài nguyên cần sử dụng khi cần biểu diễn các dữ liệu lớn. Việc chuyển đổi giữa Binary và Hexadecimal cũng vô cùng đơn giản. Ví dụ, để biểu diễn số nhị phân 100101101010010110101001011010 sang hệ thập phân, thực hiện các bước:
Bước 111: Bổ sung thêm các chữ số 000 vào đầu để tổng số chữ số là bội của 444. Thu được 001001011010001001011010001001011010.
Bước 222: Chia thành từng nhóm 444 chữ số, thu được 0010 0101 10100010\ 0101\ 10100010 0101 1010
Bước 333: Biểu diễn từng nhóm chữ số sang hệ 161616, cuối cùng ghép lại thu được kết quả ở hệ Hexadecimal: 25A.
25A
Thực hiện đảo ngược các bước trên để chuyển đổi ngược lại từ hệ 161616 sang hệ cơ số 222.
Trong Python, chúng ta có thể sử dụng hàm hex() để chuyển đổi các số sang hệ Hexadecimal. Giá trị đầu vào có thể là một số ở hệ cơ số bất kỳ, kết quả trả về ở dạng thập lục phân với ký tự thường và tiền tố 0x. Ví dụ:
hex()
0x
# decimal
print(hex(250))  # output: 0xfa
# binary
print(hex(0b1011))  # output: 0xb
# octal
print(hex(0o54))  # output: 0x2c
# hexadecimal
print(hex(0X4AF))  # output: 0x4af
Chúng ta cũng có thể sử dụng .hex() để chuyển một chuỗi sang biểu diễn dạng hex, nhưng trước hết cần chuyển chuỗi đó sang dạng byte (có thể sử dụng hàm encode()). Ví dụ:
.hex()
encode()
str = ""Hexadecimal_is_very_useful""
str_in_byte = str.encode()  # b'Hexadecimal_is_very_useful'
str_in_hex = str_in_byte.hex()
print(str_in_hex)

Ngược lại, khi được cung cấp một chuỗi hex, chúng ta có thể chuyển chuỗi đó sang dạng byte bằng bytes.fromhex(). Ví dụ:
bytes.fromhex()
str_in_hex = '48657861646563696d616c5f69735f766572795f75736566756c'
str_in_byte = bytes.fromhex(str_in_hex)

print(str_in_byte)

Một thử thách nhỏ dành cho bạn đọc: Bạn có thể chuyển chuỗi sau về dạng plain text không?
333333373333333633333336333333393333333633333332333333363336333333333336333633363333333733363332333333373333333733333336333333353333333633363333333333363333333333333336333633363333333633363334333333363333333533333335333633363333333733333334333333363336333633333335333633363333333633333333333333373333333233333337333333393333333733333330333333373333333433333336333633363333333633333337333333373333333233333336333333313333333733333330333333363333333833333337333333393333333733363334
Tài liệu tham khảo
https://vi.wikipedia.org/wiki/Mật_mã_học
https://vi.wikipedia.org/wiki/Lịch_sử_mật_mã_học
https://en.wikipedia.org/wiki/ASCII
https://en.wikipedia.org/wiki/Hexadecimal",https://viblo.asia/p/general-knowledge-in-cryptography-kien-thuc-tong-quan-trong-mat-ma-hoc-phan-1-Ny0VG79EVPA
92,Miichisoftが【Japan IT Week[春] 第33回ソフトウェアとアプリ開発展】に参加！最新AIソリューションを披露,"弊社Miichisoftは、2024年4月24日（水）～26日（金）の3日間、東京ビッグサイトで開催されるJapan IT Week【春】の「ソフトウェアとアプリ開発展」に出展いたします。
Japan IT Weekとは

Japan IT Weekは、日本最大の情報技術展であり、世界中からの参加者が集う大規模なイベントです。特に「ソフトウェアとアプリ開発展」では、最新のIT技術やソフトウェア開発に関する製品やサービスが展示され、革新性と多様性が評価されています。
12の専門展から構成されている日本最大*のIT・DX展示会です。
デジタル領域のテーマを幅広く網羅。IT課題を抱えるビジネスユーザーにとって、欠かせない展示会です。
ブースでは課題解決に向けた相談、見積り・導入時期の打合せなどが行われます。
特に「ソフトウェアとアプリ開発展」では、最新のIT技術やソフトウェア開発に関する製品やサービスが展示され、革新性と多様性が評価されています。
Miichisoftは、この壮大なイベントに参加し、革新的なAIソリューションをご紹介いたします。
2024年のJAPAN IT WEEKにMiichisoftが提示するソリューション

弊社ブースでは、ビジネスのスピードアップをAIで支援する「Total AI Business Assistant」のソリューションをご紹介いたします。特に、RAG CoreやLLMの専門知識、プロンプトエンジニアリングなどのキーワードにフォーカスして、多数の開発実績とデモをご紹介させていただきます。
Miichisoftの製品およびサービスに興味があるかもしれません
自社ソリューション
生成AI (AI Analysis / Insight, AI Chatbot, AI社内力向上サービス)
GPS ソリューション
AR/VR ソリューション
自社ソリューション
生成AI (AI Analysis / Insight, AI Chatbot, AI社内力向上サービス)
GPS ソリューション
AR/VR ソリューション
ITオフショアサービス
IT コンサルティング
ウェブサイト開発
モバイルアプリ開発
業務システム開発
LAAS (Labo as a service)
ITオフショアサービス
IT コンサルティング
ウェブサイト開発
モバイルアプリ開発
業務システム開発
LAAS (Labo as a service)
DX ソリューション
Cloud Computing
Kintone
Salesforce
DX ソリューション
Cloud Computing
Kintone
Salesforce
出展情報
日時：2024年4月24日(水)～４月２６日(金) 10:00-18:00(最終日のみ17:00まで)
場所：東京ビッグサイト 東ホール　E43-6
参加方法：こちらより事前登録が必要です。（無
2024年Japan IT Week【春】でMiichisoftのブースにお立ち寄りいただき、最新のAIソリューションをご覧ください。ご来場を心よりお待ちしております。
私たちは設立以来、6年連続でSODECイベントに参加しており、今年もあなたにお会いできることを楽しみにしています！",https://viblo.asia/p/miichisoftがjapan-it-week春-第33回ソフトウェアとアプリ開発展に参加最新aiソリューションを披露-gwd43go34X9
93,Xây dựng tính năng thanh toán với Braintree,"Chào các bạn 👋👋, sau một số bài viết liên quan đến DevOps, Go, gRPC thì hôm nay mình sẽ quay lại với xuất phát điểm ban đầu của mình đó là PHP. Trong bài viết này mình sẽ giới thiệu với các bạn về Braintree và cách tích hợp Braintree Sandbox để xây dựng tính năng thanh toán cho ứng dụng Laravel.
Braintree là một nền tảng thanh toán, cung cấp giải pháp thanh toán đa dạng cho các doanh nghiệp, người dùng trên khắp thế giới. Được phát triển bởi PayPal, Braintree không chỉ cung cấp các công cụ để tích hợp thanh toán một cách dễ dàng vào các ứng dụng web và di động, mà còn đảm bảo tính bảo mật cao và đem lại trải nghiệm tốt nhất cho người dùng.
Braintree cho phép doanh nghiệp hoặc nhà cung cấp chấp nhận thanh toán từ nhiều nguồn khác nhau bao gồm Debit Card và Credit Card, PayPal, Apple Pay, Android Pay và nhiều phương thức thanh toán khác. Điều này giúp tối ưu hóa quá trình thanh toán và tăng sự tiện lợi cho người dùng.
Một trong những điểm mạnh của Braintree là tính linh hoạt và dễ dàng tích hợp. Với các API và SDK, Braintree cho phép developer tùy chỉnh và tích hợp thanh toán vào các ứng dụng một cách linh hoạt và hiệu quả nhất. Bên cạnh đó,  Braintree cung cấp các công cụ quản lý giao dịch, phân tích và báo cáo, giúp doanh nghiệp/nhà cung cấp hiểu rõ hơn về hoạt động thanh toán của mình và tối ưu hóa chiến lược kinh doanh.
Chuẩn bị
Trước khi đi vào phần tích hợp braintree vào ứng dụng laravel các bạn cần chuẩn bị một số thứ như sau
Repository Laravel/ứng dụng Laravel hiện tại của bạn mà bạn muốn làm chức năng thanh toán
Tài khoản Braintree sandbox. Nếu các bạn gặp vấn đề về việc đăng ký tài khoản các bạn có thể theo dõi phần phía bên dưới.
Đăng ký tài khoản Braintree sanbox
Bước 1: Truy cập vào link https://www.braintreepayments.com/sandbox. Ta sẽ có giao diện như sau:

Bước 2: Điền các thông tin cần thiết vào form đăng ký

Name, compane name, region: các bạn có thể điền bất kỳ tên nào mà các bạn muốn
Email: phần email các bạn cần điền đúng email của mình vì Braintree sẽ gửi mail xác nhận để bạn tiếp tục đăng ký tài khoản
Name, compane name, region: các bạn có thể điền bất kỳ tên nào mà các bạn muốn
Email: phần email các bạn cần điền đúng email của mình vì Braintree sẽ gửi mail xác nhận để bạn tiếp tục đăng ký tài khoản
Bước 3: Sau khi điền xong các bạn chọn nút Try the sandbox. Sau đó Braintree sẽ gửi mail đến email bạn đã đăng ký. Bạn vào kiểm tra mail và tiếp tục làm các bước tiếp theo.
Bước 4: Sau khi hoàn tất đăng ký các bạn tiến hành đăng nhập bạn sẽ thấy luôn thông tin về MERCHANT_ID, PUBLIC_KEY, PRIVATE_KEY. Những thông tin này sẽ cần sử dụng khi tích hợp Braintree vào ứng dụng.
MERCHANT_ID
PUBLIC_KEY
PRIVATE_KEY
Tích hợp Braintree vào ứng dụng Laravel
Để có thể tích hợp Braintree vào ứng dụng khác như Ruby, Python, ... các bạn có thể tham khảo ở docs. Trong phần này mình sẽ hướng dẫn tích hợp với Laravel.
Phần này mình chỉ hướng dẫn mọi người cách tích hợp Braintree vào ứng dụng thôi. Còn core business logic như mọi người lưu transaction ra sao hay xử lý thanh toán thế nào thì tùy theo ứng dụng của mình mà mọi người có logic phù hợp.
Đầu tiên ta cần bốn biến môi trường đó là:
BT_ENVIRONMENT: sandbox
BT_MERCHANT_ID: ******
BT_PUBLIC_KEY: ******
BT_PRIVATE_KEY: ******
Đối với biến BT_ENVIRONMENT thì mọi người để giá trị là sandbox vì đây là môi trường dev. Còn ba giá trị là BT_MERCHANT_ID, BT_PUBLIC_KEY, BT_PRIVATE_KEY thì mọi người lấy value trên tràn của Braintree nhé vì ngay khi đăng nhập thành công nó đã show cho mọi người thông tin này rồi.
BT_ENVIRONMENT
BT_MERCHANT_ID
BT_PUBLIC_KEY
BT_PRIVATE_KEY
Sau khi đã thêm được bốn biến môi trường của Braintree thì ta đưa nó vào config để sử dụng. Trong file services.php ta thêm config như sau.
services.php
// services.php
<?php

return [

    ...

    'braintree' => [
        'environment' => env('BT_ENVIRONMENT', 'sandbox'),
        'merchantId' => env('BT_MERCHANT_ID'),
        'publicKey' => env('BT_PUBLIC_KEY'),
        'privateKey' => env('BT_PRIVATE_KEY'),
    ],

];
Vì nội dung chính trong bài viết này là hướng dẫn tích hợp nên mình sẽ dựng lên một form request đơn giản để mọi người thực hiện thanh toán. Trong thư mục views mọi người thêm file payment.blade.php có nội dung sau.
views
payment.blade.php
<!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"">
    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    
    <!-- custom css file link  -->
    <link href=""{{ asset('css/style.css') }}"" rel=""stylesheet"">

</head>
<body>

<div class=""container"">

    @if (session('success_message'))
        <div class=""alert alert-success"">
            {{ session('success_message') }}
        </div>
    @endif

    @if(count($errors) > 0)
        <div class=""alert alert-danger"">
            <ul>
                @foreach ($errors->all() as $error)
                    <li>{{ $error }}</li>
                @endforeach
            </ul>
        </div>
    @endif

    <form method=""post"" id=""payment-form"" action=""{{ url('/payment/checkout') }}"">
        @csrf
        <div class=""row"">

            <div class=""col"">

                <h3 class=""title"">billing address</h3>

                <div class=""inputBox"">
                    <span>full name :</span>
                    <input type=""text"" placeholder=""Simon"">
                </div>
                <div class=""inputBox"">
                    <span>email :</span>
                    <input type=""email"" placeholder=""Example@example.com"">
                </div>
                <div class=""inputBox"">
                    <span>address :</span>
                    <input type=""text"" placeholder=""Home Number - Street - Locality"">
                </div>
                <div class=""inputBox"">
                    <span>city :</span>
                    <input type=""text"" placeholder=""Hanoi"">
                </div>

            </div>

            <div class=""col"">

                <h3 class=""title"">payment</h3>

                <div class=""inputBox"">
                    <span>Total in $</span>
                    <input id=""amount"" name=""amount"" type=""tel"" min=""1"" placeholder=""Amount"" value=""10"">
                </div>

                <div id=""dropin-wrapper"">
                    <div id=""checkout-message""></div>
                    <div id=""dropin-container""></div>
                </div>

            </div>
    
        </div>

        <input id=""nonce"" name=""payment_method_nonce"" type=""hidden"" />
        <button id=""submit-button"" class=""submit-btn"" type=""submit"" >Submit payment</button>

    </form>

</div>
</body>
</html>
File style.css.
style.css
@import url('https://fonts.googleapis.com/css2?family=Poppins:wght@100;300;400;500;600&display=swap');

*{
  font-family: 'Poppins', sans-serif;
  margin:0; padding:0;
  box-sizing: border-box;
  outline: none; border:none;
  text-transform: capitalize;
  transition: all .2s linear;
}

.container{
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
  padding:25px;
  min-height: 100vh;
  background-image: linear-gradient(to bottom right, #FDFCFB, #E2D1C3);
}

.container .alert{
  padding: 20px;
  width: 700px;
  margin-bottom: 50px;
  box-shadow: 0 5px 10px rgba(0, 0, 0, .1);
  font-size: 17px;
  color: #333;
}

.container .alert.alert-success{
  background-image: linear-gradient(to bottom right, #72FFB6, #10D164);
}

.container .alert.alert-danger{
  background-image: linear-gradient(to bottom right, #FF512F, #DD2476);
}

.container form{
  padding: 20px;
  width: 700px;
  background: #fff;
  box-shadow: 0 5px 10px rgba(0, 0, 0, .1);
}

.container form .row{
  display: flex;
  flex-wrap: wrap;
  gap: 15px;
}

.container form .row .col{
  flex: 1 1 250px;
}

.container form .row .col .title{
  font-size: 20px;
  color: #333;
  padding-bottom: 5px;
  text-transform: uppercase;
}

.container form .row .col .inputBox{
  margin: 15px 0;
}

.container form .row .col .inputBox span{
  margin-bottom: 10px;
  display: block;
}

.container form .row .col .inputBox input{
  width: 100%;
  border: 1px solid #ccc;
  padding: 10px 15px;
  font-size: 15px;
  text-transform: none;
}

.container form .row .col .inputBox input:focus{
  border: 1px solid #000;
}

.container form .row .col .flex{
  display: flex;
  gap: 15px;
}

.container form .row .col .flex .inputBox{
  margin-top: 5px;
}

.container form .row .col .inputBox img{
  height: 34px;
  margin-top: 5px;
  filter: drop-shadow(0 0 1px #000);
}

.container form .submit-btn{
  width: 100%;
  padding: 12px;
  font-size: 17px;
  background: radial-gradient(circle at 4.3% 10.7%, rgb(138, 118, 249) 13.6%, rgb(75, 252, 235) 100.7%);
  color: #fff;
  margin-top: 5px;
  cursor: pointer;
}

.container form .submit-btn:hover{
  background: #2ecc71;
}
Sau khi thêm hai file này vào ứng dụng ta sẽ có giao diện.

Giao diện này sẽ bao gồm 1 form để người dùng có thể gửi thông tin thanh toán và hiển thị trạng thái của việc thanh toán là thành công hay thất bại.
Để có thể tương tác với API của Braintree ta sẽ sử dụng package braintree_php. Các bạn cài thêm package này bằng lệnh.
composer require braintree/braintree_php
Sau đó tiến hành tạo PaymentController.php và thêm route cho phần thanh toán như sau.
PaymentController.php
// routes.php

...

Route::get('/payment', 'PaymentController@index')->name('payment');
Route::post('/payment/checkout', 'PaymentController@checkout')->name('checkout');
// PaymentController.php

namespace App\Http\Controllers;

use Illuminate\Http\Request;

class PaymentController extends Controller
{
    public function index()
    {
        $gateway = new \Braintree\Gateway([
            'environment' => config('services.braintree.environment'),
            'merchantId' => config('services.braintree.merchantId'),
            'publicKey' => config('services.braintree.publicKey'),
            'privateKey' => config('services.braintree.privateKey')
        ]);
    
        $token = $gateway->ClientToken()->generate();
    
        return view('payment', compact('token'));
    }

    public function checkout(Request $request)
    {
        $gateway = new \Braintree\Gateway([
            'environment' => config('services.braintree.environment'),
            'merchantId' => config('services.braintree.merchantId'),
            'publicKey' => config('services.braintree.publicKey'),
            'privateKey' => config('services.braintree.privateKey')
        ]);

        $amount = $request->amount;
        $nonce = $request->payment_method_nonce;

        $result = $gateway->transaction()->sale([
            'amount' => $amount,
            'paymentMethodNonce' => $nonce,
            'customer' => [
                'firstName' => 'Simon',
                'lastName' => 'Nguyen',
                'email' => 'testing-user@gmail.com',
            ],
            'options' => [
                'submitForSettlement' => true
            ]
        ]);

        if ($result->success) {
            $transaction = $result->transaction;
    
            return back()->with('success_message', 'Transaction successful. The ID is: '. $transaction->id);
        } else {
            $errorString = """";
    
            foreach ($result->errors->deepAll() as $error) {
                $errorString .= 'Error: ' . $error->code . "": "" . $error->message . ""\n"";
            }
    
            return back()->withErrors('An error occurred with the message: '.$result->message);
        }
    }
}
Trong file routes.php mình thêm hai routes mới với một method là GET dùng để hiện thị form mà minh đã code ở trên và một method là POST dùng để xử lý logic khi users tiến hành thanh toán. Trong file PaymentController có hai hàm là index() và checkout(Request $request).
routes.php
GET
POST
PaymentController
index()
checkout(Request $request)
index(): dùng để hiển thị form thanh toán cho người dùng và tạo token của Braintree dùng để tạo instance Braintree phía client sau này
index()
checkout(Request $request): dùng để xử lý cho việc thanh toán với Braintree. Đầu tiên mình sẽ tạo gateway của Braintree, sau đó sẽ thêm transaction cho lần thanh toán đó của users. Ở đây mình sẽ hash code user như trong ví dụ là Simon. Và cuối cùng là hiển thị kết quả của lần thanh toán này là thành công hay thất bại.
checkout(Request $request)
Sau khi đã setup xong phía server có đầy đủ logic rồi thì ta sẽ tiến hành thêm Braintree instance cho phía client. Ở đây mình sẽ sử dụng drop-ui. Như file blade mình đã đề cập ở trên thì ta có một chỗ đang chưa được sử dụng đó là đoạn.
<div id=""dropin-wrapper"">
    <div id=""checkout-message""></div>
    <div id=""dropin-container""></div>
</div>
Thì chính phần container này sẽ được sử dụng cho phần drop-ui mà mình sẽ thêm ở đây. Để có thể sử dụng được SDK này đầu tiên các bạn thêm dòng script xuống cuối của phần body.
drop-ui
body
<script src=""https://js.braintreegateway.com/web/dropin/1.42.0/js/dropin.min.js""></script>
Sau khi đã thêm thành công SDK của Braintree ta sẽ tiến hành viết script để xử lý gửi request lên trên server. Ta thêm đoạn logic sau.
<script type=""text/javascript"">
    const form = document.querySelector('#payment-form');
    var client_token = ""{{ $token }}"";

    braintree.dropin.create({
        authorization: client_token,
        container: '#dropin-container'
    }, function (createErr, instance) {
        if (createErr) {
            console.log('Create Error', createErr);
            return;
        }

        form.addEventListener('submit', function (event) {
            event.preventDefault();

            instance.requestPaymentMethod(function (err, payload) {
              if (err) {
                console.log('Payment Method Error', err);
                return;
              }

              // Add the nonce to the form and submit
              document.getElementById('nonce').value = payload.nonce;
              form.submit();
            });
        });
    });
</script>
Đầu tiên ta sẽ lấy ra form submit bằng cách query ra id #payment-form, tạo biến để lưu trữ token của Braintree như mình đã đề cập trong hàm index() ở trên. Sau đó ta sẽ tiến hành tạo instance dropin với SDK. Hàm create sẽ nhận config của instance và một callback. Trong callback này mình sẽ kiểm tra xem instance đã được tạo thành công hay chưa và log ra lỗi ở console, thêm event submit cho form để khi user nhấn submit sẽ gửi request lên trên server. Trong hàm xử lý phần submit form mình sẽ sử dụng hàm requestPaymentMethod để tạo giá trị nonce sau đó điền giá trị này vào input đã được hidden ở trên form để gửi lên server.
#payment-form
index()
dropin
create
submit
requestPaymentMethod
nonce
Sau khi đã có đầy đủ logic như trên rồi thì giao diện ta sẽ có thêm phần thanh toán cho thẻ của Braintree.

Ta sẽ tiến hành điền một vài thông tin vào form để kiểm tra lại chức năng thanh toán xem nó đã hoạt động hay chưa.

Sau khi điền thông tin như hình và bấm submit ta sẽ có thông báo như sau.

Vậy là transaction đã thành công. Để kiểm tra lại phần này ta sẽ lên lại trang web sandbox của Braintree và vào phần Transactions (lưu ý các bạn cần đăng nhập đúng tài khoản có config với Laravel nhé). Trong trang Transactions sẽ cho phép ta tra cứu lại các giao dịch theo điều kiện để kiểm tra nhanh gọn các bạn chỉ cần kéo xuống dưới cùng và chọn Search, các bạn sẽ thấy lịch sử vừa thành công của mình.

Các bạn có thể nhấn vào ID của transaction đó để xem chi tiết hơn về giao dịch, ngoài ra trong màn chi tiết ta còn có thể in hóa đơn, refund.
Qua bài viết này mình đã giới thiệu Braintree và hướng dẫn tích hợp Braintree vào ứng dụng Laravel. Braintree còn rất nhiều tiệc ích khác (thanh toán Paypal, ApplePay, ...) mà trong bài viết mình không đề cập đến vì nội dụng của bài viết đã khá dài rồi. Hy vọng bài viết có thể giúp ích được các bạn phần nào. Hẹn gặp lại các bạn ở những bài viết khác và cảm ơn các bạn đã theo dõi đến hết bài viết ❤️.",https://viblo.asia/p/xay-dung-tinh-nang-thanh-toan-voi-braintree-bXP4WjgYV7G
94,Object và class trong Java (phần 2),"Biến tham chiếu và kiểu tham chiếu
Biến tham chiếu và kiểu tham chiếu là các khái niệm quan trọng trong lập trình Java. Chúng liên quan đến cách các object được tạo và quản lý thông qua việc tham chiếu đến chúng.
Biến tham chiếu (Reference Variables): Biến tham chiếu là các biến được sử dụng để lưu trữ tham chiếu đến object. Cụ thể, biến tham chiếu chứa địa chỉ hay vị trí trong bộ nhớ của object. Biến tham chiếu được khai báo bằng cách chỉ định kiểu của object mà chúng có thể tham chiếu.
Ví dụ:
Circle myCircle;
Biến tham chiếu (Reference Variables): Biến tham chiếu là các biến được sử dụng để lưu trữ tham chiếu đến object. Cụ thể, biến tham chiếu chứa địa chỉ hay vị trí trong bộ nhớ của object. Biến tham chiếu được khai báo bằng cách chỉ định kiểu của object mà chúng có thể tham chiếu.
Ví dụ:
Circle myCircle;
Kiểu tham chiếu (Reference Types): Kiểu tham chiếu là loại dữ liệu trong Java, dựa trên class hoặc interface, mà biến tham chiếu có thể tham chiếu đến. Kiểu tham chiếu định nghĩa kiểu dữ liệu cho biến tham chiếu và quy định cách các object của class hoặc interface đó được tạo và quản lý. Biến tham chiếu phải tham chiếu đến một object thuộc kiểu tham chiếu.
Ví dụ:
Circle myCircle = new Circle();
Kiểu tham chiếu (Reference Types): Kiểu tham chiếu là loại dữ liệu trong Java, dựa trên class hoặc interface, mà biến tham chiếu có thể tham chiếu đến. Kiểu tham chiếu định nghĩa kiểu dữ liệu cho biến tham chiếu và quy định cách các object của class hoặc interface đó được tạo và quản lý. Biến tham chiếu phải tham chiếu đến một object thuộc kiểu tham chiếu.
Ví dụ:
Circle myCircle = new Circle();
Trong ví dụ trên, myCircle là một biến tham chiếu kiểu Circle. Nó có thể tham chiếu đến một object Circle. Biến tham chiếu được khởi tạo bằng cách sử dụng toán tử new để tạo một object và gán tham chiếu của object đó cho biến tham chiếu.
myCircle
Circle
Circle
new
Truy cập vào dữ liệu và phương thức của Object
Trong Java, để truy cập vào dữ liệu và phương thức của một object, bạn cần sử dụng biến tham chiếu (reference variable) và dấu chấm (.). Dưới đây là cách truy cập dữ liệu và phương thức của object:
.
Truy cập thuộc tính (dữ liệu) của object:
Để truy cập giá trị của một thuộc tính trong object, ta sử dụng biến tham chiếu kết hợp với tên thuộc tính và dấu chấm ..
Ví dụ:
HocSinh student = new HocSinh(""John"", 1001);
String ten = student.ten; // Truy cập thuộc tính 'ten'
int maHocSinh = student.maHocSinh; // Truy cập thuộc tính 'maHocSinh'
Truy cập thuộc tính (dữ liệu) của object:
Để truy cập giá trị của một thuộc tính trong object, ta sử dụng biến tham chiếu kết hợp với tên thuộc tính và dấu chấm ..
.
Ví dụ:
HocSinh student = new HocSinh(""John"", 1001);
String ten = student.ten; // Truy cập thuộc tính 'ten'
int maHocSinh = student.maHocSinh; // Truy cập thuộc tính 'maHocSinh'
Gọi phương thức của object:
Để gọi một phương thức của object, bạn sử dụng biến tham chiếu kết hợp với tên phương thức và dấu chấm ..
Ví dụ:
HocSinh student = new HocSinh(""John"", 1001);
student.hienThiThongTin(); // Gọi phương thức 'hienThiThongTin'
Gọi phương thức của object:
Để gọi một phương thức của object, bạn sử dụng biến tham chiếu kết hợp với tên phương thức và dấu chấm ..
.
Ví dụ:
HocSinh student = new HocSinh(""John"", 1001);
student.hienThiThongTin(); // Gọi phương thức 'hienThiThongTin'
Truy cập phương thức của object với giá trị trả về:
Nếu phương thức trả về giá trị, bạn có thể sử dụng giá trị đó sau khi gọi phương thức.
Ví dụ:
HocSinh student = new HocSinh(""John"", 1001);
double diemTrungBinh = student.tinhDiemTrungBinh(); // Gọi phương thức và lấy giá trị trả về
System.out.println(""Điểm trung bình của học sinh là: "" + diemTrungBinh);
Truy cập phương thức của object với giá trị trả về:
Nếu phương thức trả về giá trị, bạn có thể sử dụng giá trị đó sau khi gọi phương thức.
Ví dụ:
HocSinh student = new HocSinh(""John"", 1001);
double diemTrungBinh = student.tinhDiemTrungBinh(); // Gọi phương thức và lấy giá trị trả về
System.out.println(""Điểm trung bình của học sinh là: "" + diemTrungBinh);
Hãy nhớ rằng bạn cần có một biến tham chiếu đến object để truy cập dữ liệu và phương thức của nó. Trong các ví dụ trên, biến student là biến tham chiếu đến object HocSinh, cho phép truy cập và sử dụng thông tin và hành vi của object đó.
student
HocSinh
Giới thiệu
Trong Java, từ khóa static được sử dụng để khai báo biến, hằng số và phương thức làm cho chúng thuộc về class chứ không phải thuộc một object cụ thể. Dưới đây là giới thiệu về biến, hằng số và phương thức sử dụng static:
static
Biến static:

Một biến được khai báo sử dụng static là biến của class chứ không phải của object.
Tất cả các object của class sử dụng cùng một bản sao của biến static.
Được khai báo bên ngoài phương thức, thường là ở cấp độ class.

public class Example {
    static int staticVariable = 10;

    public static void main(String[] args) {
        System.out.println(""Static Variable: "" + staticVariable);
    }
}
Biến static:
Một biến được khai báo sử dụng static là biến của class chứ không phải của object.
static
Tất cả các object của class sử dụng cùng một bản sao của biến static.
Được khai báo bên ngoài phương thức, thường là ở cấp độ class.
public class Example {
    static int staticVariable = 10;

    public static void main(String[] args) {
        System.out.println(""Static Variable: "" + staticVariable);
    }
}
Hằng số static:

Một hằng số static được khai báo bằng final và static.
Thường được đặt tên bằng chữ in hoa và sử dụng dấu gạch dưới (_) để ngăn cách các từ.

public class Example {
    static final int MAX_VALUE = 100;

    public static void main(String[] args) {
        System.out.println(""Static Constant: "" + MAX_VALUE);
    }
}
Hằng số static:
Một hằng số static được khai báo bằng final và static.
final
static
Thường được đặt tên bằng chữ in hoa và sử dụng dấu gạch dưới (_) để ngăn cách các từ.
public class Example {
    static final int MAX_VALUE = 100;

    public static void main(String[] args) {
        System.out.println(""Static Constant: "" + MAX_VALUE);
    }
}
Phương thức:

Một phương thức static thuộc về class chứ không phải object, nó có thể được gọi trực tiếp thông qua tên class mà không cần tạo object cụ thể.
Thường được sử dụng cho các hoạt động liên quan đến class chứ không phải object.

public class Example {
    static void staticMethod() {
        System.out.println(""This is a static method."");
    }

    public static void main(String[] args) {
        staticMethod(); // Gọi phương thức static trực tiếp
    }
}
Phương thức:
Một phương thức static thuộc về class chứ không phải object, nó có thể được gọi trực tiếp thông qua tên class mà không cần tạo object cụ thể.
Thường được sử dụng cho các hoạt động liên quan đến class chứ không phải object.
public class Example {
    static void staticMethod() {
        System.out.println(""This is a static method."");
    }

    public static void main(String[] args) {
        staticMethod(); // Gọi phương thức static trực tiếp
    }
}
Khi nào sử dụng static
Sử dụng static trong Java khi bạn muốn các thành phần của class (biến, hằng số, phương thức) thuộc về class chứ không phải object. Dưới đây là một số tình huống phổ biến khi sử dụng static:
static
static
Biến Static:

Sử dụng biến static khi bạn muốn chia sẻ một giá trị chung giữa tất cả các object của class.
Ví dụ: Đếm số lượng object đã được tạo của class.

public class Example {
    static int instanceCount = 0;

    public Example() {
        instanceCount++;
    }
}
Biến Static:
Sử dụng biến static khi bạn muốn chia sẻ một giá trị chung giữa tất cả các object của class.
Ví dụ: Đếm số lượng object đã được tạo của class.
public class Example {
    static int instanceCount = 0;

    public Example() {
        instanceCount++;
    }
}
Hằng Số Static:

Sử dụng hằng số static khi bạn muốn định nghĩa một giá trị hằng số mà không cần tạo object.
Ví dụ: Định nghĩa các hằng số như PI, MAX_SIZE, MIN_VALUE.

public class Constants {
    static final double PI = 3.14159;
    static final int MAX_SIZE = 100;
}
Hằng Số Static:
Sử dụng hằng số static khi bạn muốn định nghĩa một giá trị hằng số mà không cần tạo object.
Ví dụ: Định nghĩa các hằng số như PI, MAX_SIZE, MIN_VALUE.
public class Constants {
    static final double PI = 3.14159;
    static final int MAX_SIZE = 100;
}
Phương Thức Static:

Sử dụng phương thức static khi phương thức không phụ thuộc vào trạng thái của object và có thể được gọi trực tiếp từ tên class.
Ví dụ: Các tiện ích chung không liên quan trực tiếp đến object cụ thể.

public class MathUtils {
    static int add(int a, int b) {
        return a + b;
    }
}
Phương Thức Static:
Sử dụng phương thức static khi phương thức không phụ thuộc vào trạng thái của object và có thể được gọi trực tiếp từ tên class.
Ví dụ: Các tiện ích chung không liên quan trực tiếp đến object cụ thể.
public class MathUtils {
    static int add(int a, int b) {
        return a + b;
    }
}
Ví dụ thực tế
Hãy xem xét một ví dụ bài toán thực tế sử dụng static: quản lý số lượng object đã được tạo của một class.
static
public class Product {
    private static int totalProducts = 0;

    private int productId;
    private String productName;

    public Product(String name) {
        this.productName = name;
        this.productId = ++totalProducts;
    }

    public static int getTotalProducts() {
        return totalProducts;
    }

    public int getProductId() {
        return productId;
    }

    public String getProductName() {
        return productName;
    }

    public static void main(String[] args) {
        Product product1 = new Product(""Laptop"");
        Product product2 = new Product(""Smartphone"");

        System.out.println(""Total products created: "" + Product.getTotalProducts());

        System.out.println(""Product 1 ID: "" + product1.getProductId());
        System.out.println(""Product 2 ID: "" + product2.getProductId());
    }
}
Trong ví dụ này, totalProducts là một biến static theo dõi tổng số lượng object Product đã được tạo. Mỗi khi một object mới được tạo, totalProducts được tăng lên.
totalProducts
Product
totalProducts
Phương thức getTotalProducts() là một phương thức static cho phép truy cập vào tổng số lượng sản phẩm từ bên ngoài class mà không cần tạo object Product cụ thể.
getTotalProducts()
Product
Trong main method, chúng ta tạo hai object Product và sau đó in ra tổng số lượng sản phẩm và ID của mỗi sản phẩm. Sử dụng static giúp chúng ta theo dõi thông tin chung về tất cả các sản phẩm mà không cần tạo object Product.
main
Product
static
Product
Một số lưu ý
Khi sử dụng static trong Java, có một số lưu ý cần xem xét để tránh các vấn đề tiềm ẩn và tăng tính linh hoạt của code:
static
Không Sử Dụng Quá Nhiều Biến Static:

Việc sử dụng quá nhiều biến static có thể dẫn đến sự chia sẻ trạng thái toàn cục, làm cho code trở nên khó hiểu và khó kiểm soát.
Sử dụng static một cách cẩn thận để tránh hiệu ứng ""global state"" quá mức.
Không Sử Dụng Quá Nhiều Biến Static:
Việc sử dụng quá nhiều biến static có thể dẫn đến sự chia sẻ trạng thái toàn cục, làm cho code trở nên khó hiểu và khó kiểm soát.
Sử dụng static một cách cẩn thận để tránh hiệu ứng ""global state"" quá mức.
Biến Static Không Thay Đổi:

Một biến static thường không nên thay đổi giá trị sau khi đã được khởi tạo, để tránh những thay đổi không mong muốn và khó kiểm soát.
Nếu cần thay đổi giá trị, hãy cân nhắc sử dụng các phương thức static để kiểm soát giá trị bị thay đổi.
Biến Static Không Thay Đổi:
Một biến static thường không nên thay đổi giá trị sau khi đã được khởi tạo, để tránh những thay đổi không mong muốn và khó kiểm soát.
Nếu cần thay đổi giá trị, hãy cân nhắc sử dụng các phương thức static để kiểm soát giá trị bị thay đổi.
Static Constants:

Hãy sử dụng static constants cho các hằng số mà không thay đổi trong suốt vòng đời của chương trình.
Đặt tên hằng số bằng chữ in hoa và sử dụng dấu gạch dưới (_) để phân tách các từ.
Static Constants:
Hãy sử dụng static constants cho các hằng số mà không thay đổi trong suốt vòng đời của chương trình.
Đặt tên hằng số bằng chữ in hoa và sử dụng dấu gạch dưới (_) để phân tách các từ.
Thread Safety:

Tránh sử dụng biến static trong môi trường đa luồng mà không có biện pháp đồng bộ hóa. Nếu cần, sử dụng synchronized hoặc các cơ chế khác để đảm bảo thread safety.
Thread Safety:
Tránh sử dụng biến static trong môi trường đa luồng mà không có biện pháp đồng bộ hóa. Nếu cần, sử dụng synchronized hoặc các cơ chế khác để đảm bảo thread safety.
synchronized
Trong bài viết này, chúng ta đã thảo luận về các khái niệm quan trọng trong lập trình Java như từ khóa static, biến tham chiếu và kiểu tham chiếu. static giúp chia sẻ thông tin chung và duy trì trạng thái toàn cục, trong khi biến tham chiếu và kiểu tham chiếu mang lại tính linh hoạt khi làm việc với đối tượng. Với việc nắm rõ những khái niệm này, chùng ta có thể viết code Java hiệu quả và dễ dàng maintain.
static
static",https://viblo.asia/p/object-va-class-trong-java-phan-2-BQyJK2Q14Me
95,Tự code lại JavaScript Promise từ đầu,"Hôm nay mình đã thử tự code lại Promise trong JavaScript từ đầu theo chuẩn  Promises/A+ để kiểm tra khả năng của mình. Sẵn tiện mình viết bài này chia sẻ cách làm cho những bạn quan tâm. Bắt đầu luôn nha.
Hãy sẽ gọi Promise của chúng ta là NotNow. Những cái tên bắt tai như will, future, later, v.v. đã bị dùng bởi mấy package khác rồi.
Khởi đầu đơn giản thôi.
const PENDING = ""pending"";
const FULFILLED = ""fulfilled"";

class NotNow {
  #state = PENDING;
  #onFulfilleds = [];
  #value;

  constructor(fn) {
    fn(this.#fulfill.bind(this));
  }

  #fulfill(value) {
    this.#state = FULFILLED;
    this.#value = value;
    this.#onFulfilleds.forEach((fn) => fn(value));
  }

  then(onFulfilled) {
    this.#addOnFulfilled(onFulfilled);
  }

  #addOnFulfilled(onFulfilled) {
    if (this.#state === PENDING) {
      this.#onFulfilleds.push(onFulfilled);
    } else if (this.#state === FULFILLED) {
      onFulfilled(this.#value);
    }
  }
}
constructor sẽ thực thi function bạn truyền vào, với phương thức #fulfill là callback để bạn trả về kết quả.
constructor
#fulfill
Phương thức #fulfill lưu lại giá trị trả về và kích hoạt các callback với giá trị đó.
#fulfill
Phương thức then thêm callback để thực thi khi promise nhận được giá trị trả về. Bên trong phương thức này gọi phương thức #addOnFulfilled.
then
#addOnFulfilled
Phương thức #addOnFulfilled, nếu promise chưa nhận được giá trị sẽ thêm callback vào để thực thi khi promise nhận được giá trị, còn nếu promise đã có giá trị thì thực thi callback luôn.
#addOnFulfilled
Test thử nào.
const notNow = new NotNow((fulfill) => {
  setTimeout(() => fulfill(2), 2000);
});
notNow.then((value) => console.log(`callback 1, fulfilled with ${value}`));
notNow.then((value) => console.log(`callback 2, fulfilled with ${value}`));
Sau 2 giây, màn hình sẽ in ra:
callback 1, fulfilled with 2
callback 2, fulfilled with 2
Ok. Bước đầu tạm ổn.
Thực tế thì các callback không được thực thi một cách đồng bộ ngay khi có kết quả trả về, mà sẽ thực thi bất đồng bộ. Điều này để đảm bảo tính nhất quán: cho dù promise có nhận được giá trị một cách đồng bộ, thì các callback vẫn được thực thi một cách bất đồng bộ. Bạn có thể dùng setTimeout hay queueMicrotask đều được. Mình sẽ dùng queueMicrotask để các callback được thực thi sớm hơn.
setTimeout
queueMicrotask
queueMicrotask
Sửa lại phương thức #fulfill:
#fulfill
// this.#onFulfilleds.forEach((fn) => fn(value));
queueMicrotask(() => this.#onFulfilleds.forEach((fn) => fn(value)));
Sửa lại phương thức #addOnFulfilled:
#addOnFulfilled
// onFulfilled(this.#result);
queueMicrotask(() => onFulfilled(this.#value));
Mỗi khi thêm callback cho promise, thực ra chúng ta đang tạo ra một promise mới. Promise mới này chính là kết quả khi thực thi callback. Chúng ta sau đó có thể truyền promise mới này đến những nơi khác trong chương trình để có thể thêm các callback tiếp tục xử lý kết quả của callback hiện tại.
Vậy thì trong phương thức then, chúng ta sẽ tạo ra một promise mới. Khi promise hiện tại nhận được giá trị, chúng ta thực thi callback, rồi trả về cho promise mới kết quả của callback.
then
class NotNow {
  // ...
  then(onFulfilled) {
    return new Promise((nextFulfill) => {
      this.#addOnFulfilled((value) => {
        const nextValue = onFulfilled(value);
        nextFulfill(nextValue);
      });
    });
  }
  // ...
}
Test thử luôn.
const a = new NotNow((fulfill) => {
  setTimeout(() => fulfill(2), 2000);
});
const b = a.then((value) => value * 2);
b.then((value) => console.log(value));
a sẽ nhận được giá trị 2, rồi b sẽ nhận được giá trị 2 * 2  là 4. Vậy 4 sẽ được in ra màn hình.
a
b
Chúng ta có thể gọi nối tiếp then:
then
new NotNow((fulfill) => {
  setTimeout(() => fulfill(2), 2000);
})
  .then((value) => value * 2)
  .then((value) => console.log(value));
Nối thêm nhiều lần cũng được luôn:
new NotNow((fulfill) => {
  setTimeout(() => fulfill(2), 2000);
})
  .then((value) => value * 2)
  .then((value) => value * 2)
  .then((value) => value * 2);
  .then((value) => console.log(value));
Ahh, method chaining nhìn cứ thấy dễ chịu nhỉ.
Một đặc tính của promise là khi giá trị trả về cho promise cũng là một promise (là cũng có phương thức then), thì promise hiện tại sẽ cố nhận lấy giá trị trả về của promise đó luôn. Ví dụ:
then
const a = new Promise((fulfill) => fulfill(2));
const b = new Promise((fulfill) => fulfill(a));
b.then((value) => console.log(value));
Kết quả in ra sẽ là:
2
Mặc dù chúng ta đã trả cho b giá trị là a, nó vẫn cố nhận lấy luôn giá trị trả về của a là 2, bởi vì a là một promise.
b
a
a
a
Kết quả cũng vẫn vậy dù có có lồng qua nhiều lớp promise:
const a = new Promise((fulfill) => fulfill(2));
const b = new Promise((fulfill) => fulfill(a));
const c = new Promise((fulfill) => fulfill(b));
const d = new Promise((fulfill) => fulfill(c));
d.then((value) => console.log(value));
Kết quả:
2
Để làm được điều này, khi nhận được giá trị trong phương thức #fulfill, kiểm tra nếu giá trị đó là promise thì chúng ta không thực thi các callback ngay, mà đợi cho đến khi promise  đó cũng nhận được giá trị luôn, bằng cách thêm #fulfill vào promise đó như một callback.
#fulfill
#fulfill
class NotNow {
  //...
  #fulfill(value) {
    if (typeof value?.then === ""function"") {
      return value.then(this.#fulfill);
    }
    this.#state = FULFILLED;
    this.#value = value;
    queueMicrotask(() => this.#onFulfilleds.forEach((fn) => fn(value)));
  }
  //...
}
Lần tiếp theo khi #fulfill được gọi, nó cũng thực thi tương tự, nên nó hoạt động một cách đệ quy tới khi giá trị nhận được không phải là promise.
#fulfill
Kiểm tra luôn:
const a = new Promise((fulfill) => fulfill(2));
const b = new Promise((fulfill) => fulfill(a));
const c = new Promise((fulfill) => fulfill(b));
const d = new Promise((fulfill) => fulfill(c));
d.then((value) => console.log(value));
Kết quả:
2
Chính xác!
Có thể dùng async/await với NotNow không? Tất nhiên rồi! Giá trị được awaitchỉ cần hoạt động như một promise, class thực sự của nó không phải thành vấn đề.
async
await
NotNow
await
const a = await new NotNow((fulfill) => {
  setTimeout(() => fulfill(5), 2000);
});
console.log(a);
Đại khái là vậy. Trong bài viết này mình đã tạm bỏ qua xử lý lỗi cho đơn giản, để dễ tập trung vào các vấn đề cốt lõi. Hãy tham khảo phiên bản đầy đủ trong source code. Code đã pass hoàn toàn bộ test của Promises/A+. Ngoài ra trong đó còn có các phương thức bổ trợ như all, allSettled, any, race.
all
allSettled
any
race
Mong là bài viết hữu ích. Hãy thoải mái chia sẻ suy nghĩ, góp ý, hoặc cả cách làm của bạn, ở phần bình luận.",https://viblo.asia/p/tu-code-lai-javascript-promise-tu-dau-BQyJK2xW4Me
96,Công cụ phân tích dữ liệu hot nhất hiện nay,"Tableau
Tableau là một trong những công cụ phân tích dữ liệu và biểu đồ hàng đầu trên thị trường. Với giao diện người dùng thân thiện và khả năng kéo thả, Tableau giúp người dùng dễ dàng tạo ra các báo cáo và dashboard đẹp mắt. Công cụ này hỗ trợ một loạt các nguồn dữ liệu khác nhau và cho phép người dùng chia sẻ dữ liệu thông qua Tableau Server hoặc Tableau Online.
Tableau
Tableau là một trong những công cụ phân tích dữ liệu và biểu đồ hàng đầu trên thị trường. Với giao diện người dùng thân thiện và khả năng kéo thả, Tableau giúp người dùng dễ dàng tạo ra các báo cáo và dashboard đẹp mắt. Công cụ này hỗ trợ một loạt các nguồn dữ liệu khác nhau và cho phép người dùng chia sẻ dữ liệu thông qua Tableau Server hoặc Tableau Online.
Microsoft Power BI
Microsoft Power BI là một giải pháp phân tích kinh doanh, cho phép người dùng kết nối với dữ liệu, chế biến và tạo ra các báo cáo chi tiết. Power BI có khả năng tích hợp sâu với các sản phẩm khác của Microsoft, như Excel và Azure, làm cho nó trở thành một lựa chọn mạnh mẽ cho các tổ chức sử dụng hệ sinh thái Microsoft.
Microsoft Power BI
Microsoft Power BI là một giải pháp phân tích kinh doanh, cho phép người dùng kết nối với dữ liệu, chế biến và tạo ra các báo cáo chi tiết. Power BI có khả năng tích hợp sâu với các sản phẩm khác của Microsoft, như Excel và Azure, làm cho nó trở thành một lựa chọn mạnh mẽ cho các tổ chức sử dụng hệ sinh thái Microsoft.
Google Data Studio
Google Data Studio cung cấp một giải pháp phân tích và báo cáo dữ liệu miễn phí, cho phép người dùng tạo ra các báo cáo đẹp mắt và dễ hiểu từ một loạt các nguồn dữ liệu. Data Studio rất dễ sử dụng và có khả năng tích hợp mạnh mẽ với các dịch vụ khác của Google như Google Analytics, Google Ads, và Google Sheets.
Google Data Studio
Google Data Studio cung cấp một giải pháp phân tích và báo cáo dữ liệu miễn phí, cho phép người dùng tạo ra các báo cáo đẹp mắt và dễ hiểu từ một loạt các nguồn dữ liệu. Data Studio rất dễ sử dụng và có khả năng tích hợp mạnh mẽ với các dịch vụ khác của Google như Google Analytics, Google Ads, và Google Sheets.
Python và R
Đối với những người dùng có kỹ năng lập trình, Python và R là hai ngôn ngữ lập trình mạnh mẽ cho phân tích dữ liệu. Python được yêu thích bởi thư viện phong phú như Pandas, NumPy và Matplotlib, trong khi R được ưa chuộng trong thống kê và phân tích dữ liệu. Cả hai đều có một cộng đồng lớn và nhiều tài nguyên hỗ trợ.
Python và R
Đối với những người dùng có kỹ năng lập trình, Python và R là hai ngôn ngữ lập trình mạnh mẽ cho phân tích dữ liệu. Python được yêu thích bởi thư viện phong phú như Pandas, NumPy và Matplotlib, trong khi R được ưa chuộng trong thống kê và phân tích dữ liệu. Cả hai đều có một cộng đồng lớn và nhiều tài nguyên hỗ trợ.
Apache Spark
Apache Spark là một công cụ phân tích dữ liệu lớn, tập trung vào tốc độ và khả năng xử lý dữ liệu lớn. Spark có thể xử lý cả batch và real-time data processing, làm cho nó trở thành một lựa chọn phù hợp cho các ứng dụng yêu cầu xử lý dữ liệu lớn và phức tạp.
Apache Spark
Apache Spark là một công cụ phân tích dữ liệu lớn, tập trung vào tốc độ và khả năng xử lý dữ liệu lớn. Spark có thể xử lý cả batch và real-time data processing, làm cho nó trở thành một lựa chọn phù hợp cho các ứng dụng yêu cầu xử lý dữ liệu lớn và phức tạp.
Lựa Chọn Công Cụ Phân Tích Dữ Liệu
Khi lựa chọn công cụ phân tích dữ liệu, cần xem xét các yếu tố sau:
Nguồn dữ liệu và tích hợp: Chọn công cụ có khả năng tích hợp dễ dàng với các nguồn dữ liệu bạn sử dụng.
Nhu cầu phân tích: Đánh giá mức độ phức tạp của nhu cầu phân tích để chọn công cụ cung cấp đủ sức mạnh và linh hoạt.
Kỹ năng của đội ngũ: Lựa chọn công cụ phù hợp với kỹ năng hiện tại của đội ngũ hoặc xem xét khả năng đào tạo cần thiết.
Chi phí: Cân nhắc ngân sách dành cho công cụ và chi phí liên quan (bản quyền, đào tạo, v.v.).
Kết Luận
Công cụ phân tích dữ liệu chính xác có thể cung cấp sự hiểu biết sâu sắc và giá trị từ dữ liệu, giúp các tổ chức đưa ra quyết định thông minh dựa trên thông tin. Sự lựa chọn giữa các công cụ như Tableau, Microsoft Power BI, Google Data Studio, Python và R, cũng như Apache Spark, phụ thuộc vào nhu cầu cụ thể, kỹ năng của đội ngũ, và môi trường công nghệ của tổ chức. Bằng cách đánh giá cẩn thận và chọn lựa kỹ lưỡng, bạn có thể tối đa hóa hiệu quả của quá trình phân tích dữ liệu.",https://viblo.asia/p/cong-cu-phan-tich-du-lieu-hot-nhat-hien-nay-EbNVQ5QmVvR
97,Sự khác biệt giữa Count(*) và Count(1)? Cái nào hiệu quả hơn?,"Xin chào mọi người.
Khi chúng ta đếm các bản ghi trong bảng dữ liệu, chúng ta đã quen với việc sử dụng hàm count để đếm, nhưng có nhiều loại tham số có thể được truyền trong hàm count, chẳng hạn như count(1), count(), count(column), …
Vậy sử dụng cái nào là hiệu quả nhất? Có phải là count(*) sẽ kém hiệu quả nhất không?
Mọi người thường nhận định count(*) là kém hiệu quả nhất và cho rằng nó sẽ đọc tất cả các trường trong bảng, giống như câu query SELECT * FROM. Điều này có thực sự đúng không, chúng ta hãy tìm hiểu bên dưới nhé.
SELECT * FROM
Chúng ta sẽ bắt đầu với kết luận trước:
coun(*) = count(1) > count(primary key column) > count(column)
Để hiểu điều này, chúng ta cần đi sâu vào nguyên tắc hoạt động của hàm count() nhé.
Trước khi đi tiếp, chúng ta cùng thống nhất ngữ cảnh của bài viết:
Database: MySQL
Store Engine: InnoDB
1.1. Count() là gì?
Count(arg) là một hàm tổng hợp, tham số của hàm count() là một cột, một hằng số hoặc thậm chí là (*). Hàm được sử dụng để đếm số lượng bản ghi đáp ứng điều kiện truy vấn và tham số trong hàm count(arg) có giá trị khác null.
Giả sử đối số hàm count là một trường như sau:
null
select count(name) from member;
Câu lệnh này là để đếm số lượng bản ghi trong bảng member với trường name không phải là null. Nói cách khác, nếu giá trị của trường name trong bản ghi là null, nó sẽ không được tính.
Giả sử rằng tham số của hàm count() là hằng số 1, như sau:
null
null
select count(1) from member;
Tất nhiên 1 thì luôn khácnull rồi, vì vậy câu lệnh trên đếm có bao nhiêu bản ghi trong bảng member.
Để hiểu rõ hơn, sau đây chúng ta cùng tìm hiểu các cơ chế hoạt động và chiến lược thực thi của hàm count nhé.
null
1.2. Count(primary key) hoạt động như thế nào?
Khi ta đếm có bao nhiêu bản ghi thông qua hàm count(arg), lúc này MySQL duy trì một biến gọi là count và đọc các record trong một lần duyệt. Nếu giá trị arg trong hàm count khác null, nó sẽ cộng thêm 1 vào biến count cho đến khi tất cả các bản ghi được duyệt và sau đó thoát khỏi lượt duyệt đó. Cuối cùng, trả về giá trị của biến count cho client.
null
Như chúng ta biết rằng sẽ có hai loại index là clustered index và secondary index. Sự khác biệt giữa chúng là các nút lá của clustered index lưu trữ dữ liệu thực tế, trong khi các nút lá của secondary index chỉ lưu trữ giá trị khóa chính thay vì dữ liệu của bản ghi. Mặc định clustered index sẽ được tự động tạo khi chúng ta tạo primary key và InnoDB sẽ lưu các bản ghi ở các nút lá của cây B+ Tree.
Lấy câu lệnh sau đây làm ví dụ:
select count(CardNo) from member;
Nếu bảng chỉ có clustered index, không có second index, InnoDB sẽ duyệt trên clustered index. Mỗi bản ghi, InnoDB đọc giá trị primary key để so sánh với null. Nếu nó khác null, biến count sẽ được cộng thêm 1.
null
null
Ví dụ trường hợp khác, Phone_No là khoá chính và có tồn tại 1 second index trên (Phone_No, Name) trong bảng Member.
Phone_No
(Phone_No, Name)
Member

Tuy nhiên nếu có một secondary index trong bảng, InnoDB sẽ không duyệt clustered index, mà duyệt secondary index. Lý do là secondary index có thể chiếm ít dung lượng lưu trữ hơn so với clustered index => cây secondary index nhỏ hơn cây clustered index => chi phí I/O khi scan second index thấp hơn so với việc scan clustered index. Vì vậy optimizer ưu tiên dùng secondary index hơn.
1.3. Count(1) hoạt động như thế nào?
Tham số của hàm count là 1, rõ ràng không phải là cột, cũng không phải null. Vậy count(1) sẽ hoạt động thế nào?
null
count(1)
Ví dụ:
select count(1) from member;

Trong trường hợp chỉ có clustered index và không có secondary index, InnoDB duyệt clustered index để đếm bản ghi, nhưng nó chỉ duyệt bản ghi, chứ không cần đọc lấy giá trị của primary key. Vì việc bản ghi có tồn tại được xem là khác null.
null
Ta có thể thấy, Count(1) sẽ nhanh hơn Count(primary key column), vì nó không cần đọc giá trị bản ghi để so sánh với  null.
Tuy nhiên, nếu có một secondary index trong bảng, InnoDB sẽ duyệt secondary index trước.
Count(1)
Count(primary key column)
null
1.4. Count(*) hoạt động như thế nào?
Khi bạn nhìn thấy ký tự *, bạn có nghĩ rằng nó đang đọc tất cả các giá trị trường trong bản ghi không?
Nó sẽ đúng trong trường hợp select *, còn count(*) thì không nhé.
Khi chúng ta gọi count(*), MySQL sẽ chuyển đổi tham số thành count(0).
select *
count(*)
count(*)
count(0)

Do đó, quá trình thực thi count(*) giống hệt count(1) và không có sự khác biệt về hiệu suất.
1.5. Count(column) hoạt động như thế nào?
Ta có ví dụ sau:
select count(name) from member;
Đối với truy vấn này, MySQL quét toàn bộ bảng để đếm, vì vậy Count(column) có hiệu năng kém nhất so với count(1), count(*) và count(primary key column).

Trong trường hợp column có secondary index thì câu lệnh sẽ sử dụng index để duyệt, từ đó tốc độ sẽ được cải thiện.
1.6. Tóm tắt
coun(*) = count(1) > count(primary key column) > count(column)
Ưu tiên sử dụng coun(*) hoặc count(1).
coun(*)
count(1)
Nếu có secondary index trong bảng, InnoDB sẽ chọn secondary index để duyệt. Vì nó hiệu quả hơn so với duyệt primary index.
Nếu không có second index trên column X thì không nên sử dụng Count(column X) để đếm số lượng bản ghi, vì nó có thể scan toàn bộ bảng => kém hiệu quả nhất.
column X
Count(column X)
2.1. Storage Engine khác count(*) như nào?
Bạn có thể tự hỏi tại sao hàm count() cần phải duyệt qua các bản ghi?
Từ đầu bài, mình chỉ đề cập tới storage engine InnoDB, tuy nhiên các storage engine khác nhau có thể có cách thực thi hàm count khác nhau. Ví dụ như MyISAM, 1 storage engine khác của MySQL, phổ biến thứ 2 sau InnoDB.
Trong trường hợp sử dụng MyISAM, mỗi bảng sẽ có metadata chứa giá trị row_count.  Như vậy khi cần đếm tất cả bản ghi trong bảng (count() không có điều kiện lọc), MyISAM chỉ cần đọc giá trị row_count với độ phức tạp O(1).
row_count
row_count
Khi count() có điều kiện lọc thì MyISAM và InnoDB hoạt động không khác gì nhau. Cả hai đều cần scan bảng để đếm số bản ghi phù hợp.
*Lưu ý, khi đọc lấy row_count, MyISAM lock bảng để đảm bảo tính nhất quán của giá trị này.
row_count
2.2. Count trong transaction hoạt động như nào?
Storage engine InnoDB hỗ trợ transaction, nhiều transaction có thể thực cùng lúc. Cơ chế MVCC (multi-version concurrency control) và Isolation có thể ảnh hưởng tới kết quả count().
Ví dụ: bảng member có 100 bản ghi. Và có 2 sesssion thực hiện song song và các query được thực hiện theo thứ tự sau:

Ở cuối session A và B, ta kiểm tra tổng số bản ghi trong bảng member cùng một lúc nhưng bạn có thể thấy rằng kết quả hiển thị là khác nhau. Do isolation level mặc định của transaction A là repeatable nên count(*) thứ 2 sẽ lặp kết quả là 100.
count(*)
InnoDB cần duyệt dữ liệu trong undo logs để đảm bảo tính isolation của transaction. Các bạn có thể đọc thêm về MVCC và tính isolation nhé.
Nếu bạn thường xuyên sử dụng count(*) cho một table lớn thì đó không phải giải pháp tốt.
Ví dụ bảng member có tổng cộng 12+ triệu bản ghi, và mình cũng đã tạo một secondary index, nhưng phải mất khoảng 5 giây để thực thi một lần: select count(*) from member
select count(*) from member

Vậy có cách nào tốt hơn để làm điều này khi đối mặt với một bảng lớn không?
3.1. Lấy giá trị xấp xỉ
Nếu bạn không cần phải rất chính xác về số lượng thống kê, ví dụ: khi công cụ tìm kiếm tìm kiếm từ khóa, số lượng kết quả tìm kiếm được đưa ra là một giá trị gần đúng.

Trong trường hợp này, chúng ta có thể sử dụng lệnh explain để ước tính bảng. Lệnh EXPLAIN (không đi kèm tham số ANALYZE) rất hiệu quả vì nó không thực sự truy vấn.

3.2. Tạo một bảng để lưu giá trị biến count
Nếu muốn lấy tổng số bản ghi chính xác trong một bảng, ta có thể lưu giá trị đếm này vào một bảng đếm riêng biệt.
Khi thêm một bản ghi vào bảng dữ liệu, ta tăng trường count lên 1 và khi xóa một bản ghi thì giảm số lượng trường count xuống 1.
Cuối cùng, chúng ta cần nhớ vài điểm quan trọng sau đây:
Count(*) có hiệu năng tốt hơn so với Count(pk), count(column)
Hàm count ưu tiên sử dụng secondary index để thực hiện đếm.
Nếu trường hợp không cần số liệu chính xác, hãy lấy giá trị xấp xỉ.
Hẹn gặp lại các bạn trong các bài viết tiếp theo.
Nếu mọi người thấy hay thì cho mình xin 1 upvote 🔼 và share nhé. 
Cám ơn mọi người rất nhiều 🙏
🧑‍💻 70+ Ronin Engineers: https://ronin-engineer.github.io/c1/ 
📚️ System Design VN: https://fb.com/groups/systemdesign.vn",https://viblo.asia/p/su-khac-biet-giua-count-va-count1-cai-nao-hieu-qua-hon-vlZL9a2BLQK
98,OneSignal với Nextjs 14  website,"Cài đặt gói
yarn add react-onesignal
với version 3.0.1
Cài đặt gói
yarn add react-onesignal
với version 3.0.1
Mở quyền thông báo
Mở quyền thông báo
Mở quyền cho ứng dụng trong Cài đặt của máy tính
Cấp quyền thông báo cho website
xem thêm hướng dẫn ở đây : https://documentation.onesignal.com/docs/notifications-not-shown-web-push
Code OneSignal
kiểm tra cấp quyền chưa, chưa cấp thì mình enable init, có rồi thì thôi
login  tự mình cấp , nó có thêm tham số thứ 2 là token ,  có sài thì thêm vô, hông thì thôi , thêm vô thì nó ghi nhận lại trong Audience - Subscriptions trên dashboard.onesignal.com
await OneSignal.login('huyi_abi');
await OneSignal.User.addAlias('myAlias', 'huyi');
'use client';

import {
    PropsWithChildren,
    createContext,
    useContext,
    useEffect,
    useState,
} from 'react';
import OneSignal from 'react-onesignal';

const useLogic = () => {
    const appId = '9dcb4b67-6e24-4eb5-9be6-a8e19b71f63d'; // muốn có cái này thì đăng ký app là có 

    const [enable, setEnable] = useState(false);

    useEffect(() => {
        const init = async () => {
            try {
                if (!enable) {
                    await OneSignal.init({
                        appId,
                        notifyButton: {
                            enable: true,
                        },
                        autoResubscribe: true, // tự động theo đõi 
                        autoRegister: true,

                        serviceWorkerPath: '/OneSignalSDKWorker.js',
                        allowLocalhostAsSecureOrigin: true,
                    });
                }

                setEnable(OneSignal.Notifications.permission);

                await OneSignal.Notifications.requestPermission();
                await OneSignal.Slidedown.promptPush();

                await OneSignal.login('huyi_abi');
                await OneSignal.User.addAlias('myAlias', 'huyi');

                await OneSignal.Debug.setLogLevel('trace'); // này để log, hông muốn log thì để """" 
            } catch (error) {
                console.log('error :>> ', error);
            }
        };
        init();
    }, [enable]);

    return {};
};

type Extra = {};

type ValueCtx = ReturnType<typeof useLogic> & Extra;

export const AppCtx = createContext({} as ValueCtx);

export const AppProvider = ({ ...props }: PropsWithChildren<Extra>) => {
    const valueCtx = useLogic();
    return (
        <AppCtx.Provider value={{ ...valueCtx, ...props }}>
            <>{props.children}</>
        </AppCtx.Provider>
    );
};

export const useAppCtx = () => useContext(AppCtx);
mình tạo cái OneSignalSDKWorker trong public : /public/OneSignalSDKWorker.js
importScripts('https://onesignal.com/sdks/web/v16/OneSignalSDK.sw.js');
hướng dẫn đăng ký App


Cái này quan trọng :


Mình sài package nên khỏi dùng cái này
Chạy không ra thông báo thì cmt ở dưới nha!",https://viblo.asia/p/onesignal-voi-nextjs-14-website-qPoL78o1Vvk
99,Tìm hiểu về bộ công cụ quản lí log ELK (P1),"1. Tổng quan
Logging
Logging là một phần không thể thiếu trong bất kì hệ thống nào, nó giúp người dùng lưu lại dấu vết, hoạt động của ứng dụng, giúp phân tích, điều tra bugs,....
Việc quản lý log sao cho hiệu quả luôn là một bài toán cần phải giải khi mà trong một hệ thống lớn thì file log có thể lên tới vài chục GB, hay là trong mô hình microservices thì người dùng có rất nhiều servers tương ứng với rất nhiều file logs.
Đó là lý do mà ELK stack ra đời.
Lịch sử hình thành
Lịch sử hình thành của ELK Log bắt đầu vào năm 2010 khi Florian Leibert, Florian Dörfler và Simon Eisenmann, những nhà phát triển tại Elasticsearch, bắt đầu phát triển dự án mã nguồn mở ""Logstash"" để thu thập và phân tích nhật ký hệ thống. Dự án này nhanh chóng trở nên phổ biến trong cộng đồng phát triển phần mềm.

Năm 2012, Elasticsearch, Inc. được thành lập để hỗ trợ sự phát triển của Logstash và các dự án mã nguồn mở khác của Elasticsearch. Công ty phát hành phiên bản mới của Logstash vào năm 2013, bổ sung nhiều tính năng mới như khả năng thu thập nhật ký từ các nguồn khác nhau và phân tích nhật ký theo thời gian thực.
Năm 2015, Elasticsearch, Inc. phát hành Elasticsearch, công cụ tìm kiếm phân tán thiết kế để lưu trữ và phân tích dữ liệu nhật ký từ Logstash và các nguồn khác như ứng dụng web và cơ sở dữ liệu.
Năm 2016, Elasticsearch, Inc. phát hành Kibana, công cụ giao diện người dùng cho phép trực quan hóa và phân tích dữ liệu nhật ký. Kibana tạo biểu đồ, bảng và bản đồ từ dữ liệu nhật ký.
Vào đầu năm 2021, Elastic tuyên bố ELK Stack sẽ không còn là mã nguồn mở từ phiên bản 7.11. Công ty áp dụng hai giấy phép độc quyền SSPL và Elastic license cho các dự án liên quan đến ELK.
Kể từ đó, ELK Stack trở thành giải pháp phổ biến để thu thập, phân tích và trực quan hóa dữ liệu nhật ký, được sử dụng rộng rãi để giám sát hiệu suất hệ thống, phát hiện sự cố và cải thiện bảo mật.
2. ELK Stack là gì?
Rất đơn giản thôi là viết tắt của 3 từ Elasticsearch, Logstash, Kibana và gộp lại thành một bộ công cụ dùng để thu thập, xử lý, hiển thị dữ liệu trực quan.
ELK Stack, hay còn gọi là Elastic Stack, là một bộ sản phẩm mã nguồn mở bao gồm bốn thành phần chính: Elasticsearch, Logstash, Kibana và Beats, được phát triển và duy trì bởi Elastic.

Elasticsearch: Hệ thống tìm kiếm và phân tích văn bản đầy đủ, dựa trên công cụ tìm kiếm mã nguồn mở Apache Lucene.
Logstash: Trình thu thập log tổng hợp dữ liệu từ nhiều nguồn đầu vào khác nhau, thực hiện các biến đổi và chuyển dữ liệu đến các điểm đích đầu ra được hỗ trợ. Mặc dù nhiều triển khai hiện đại của ELK không sử dụng Logstash, thay vào đó chuyển sang các giải pháp như Fluentd.
Kibana: Lớp trình diện hóa làm việc trực tiếp trên Elasticsearch, cung cấp khả năng phân tích và hiển thị dữ liệu cho người dùng.
Beats: Các ứng dụng nhỏ và hiệu quả được cài đặt trực tiếp trên máy chủ hoặc thiết bị để thu thập và chuyển tiếp các loại dữ liệu khác nhau vào ELK Stack.
Những thành phần này thường được sử dụng để theo dõi, khắc phục sự cố và bảo mật môi trường IT. Beats và Logstash đảm nhận việc thu thập và xử lý dữ liệu, Elasticsearch lập chỉ mục và lưu trữ dữ liệu, và Kibana cung cấp giao diện người dùng để truy vấn và hiển thị dữ liệu.
Trong phần tiếp theo, ta sẽ tìm hiểu từng chức năng của ELK stack.
Chúc mừng năm mới đến toàn thể dân IT! Mong rằng năm mới sẽ đem lại cơ hội mới, thành công và niềm vui trong công việc. Hãy tiếp tục nỗ lực và sáng tạo để đạt được những mục tiêu mới. Chúc mọi người một năm mới tràn đầy hạnh phúc và thành công!``",https://viblo.asia/p/tim-hieu-ve-bo-cong-cu-quan-li-log-elk-p1-WR5JRB7AJGv
